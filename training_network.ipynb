{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Construct customized ResNet\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, pcpt_block, pcpt_layers, scoop_block, scoop_layers, h, w, pcpt_is_upsample=0, scoop_is_upsample=0):\n",
    "        self.inplanes = 64\n",
    "        self.pcpt_is_upsample = pcpt_is_upsample\n",
    "        super(ResNet, self).__init__()\n",
    "        self.pcpt_conv1 = nn.Conv2d(4, 64, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.pcpt_bn1 = nn.BatchNorm2d(64)\n",
    "        self.pcpt_relu = nn.ReLU(inplace=True)\n",
    "        self.pcpt_maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.pcpt_upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.pcpt_layer1 = self._make_layer(pcpt_block, 128, pcpt_layers[0])\n",
    "        self.pcpt_layer2 = self._make_layer(pcpt_block, 256, pcpt_layers[1])\n",
    "        self.pcpt_layer3 = self._make_layer(pcpt_block, 512, pcpt_layers[2])\n",
    "\n",
    "        self.inplanes = 512\n",
    "        self.scoop_is_upsample = scoop_is_upsample\n",
    "        self.scoop_upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.scoop_layer1 = self._make_layer(scoop_block, 256, scoop_layers[0])\n",
    "        self.scoop_layer2 = self._make_layer(scoop_block, 128, scoop_layers[1])\n",
    "        self.scoop_layer3 = self._make_layer(scoop_block, 64, scoop_layers[2])\n",
    "        self.scoop_conv1 = nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.scoop_bn1 = nn.BatchNorm2d(1)\n",
    "        self.scoop_conv2 = nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.scoop_bn2 = nn.BatchNorm2d(3)\n",
    "        self.scoop_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.scoop_head = nn.Linear(h*w, 1)\n",
    "\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pcpt_conv1(x)\n",
    "        x = self.pcpt_bn1(x)\n",
    "        x = self.pcpt_relu(x)\n",
    "        x = self.pcpt_maxpool(x)\n",
    "\n",
    "        x = self.pcpt_layer1(x)\n",
    "        x = self.pcpt_maxpool(x)\n",
    "        x = self.pcpt_layer2(x)\n",
    "        x = self.pcpt_layer3(x)\n",
    "\n",
    "        x = self.scoop_layer1(x)\n",
    "        x = self.scoop_layer2(x)\n",
    "        x = self.scoop_upsample(x)\n",
    "        x = self.scoop_layer3(x)\n",
    "        x = self.scoop_upsample(x)\n",
    "\n",
    "        x = self.scoop_conv2(x)\n",
    "        x = self.scoop_bn2(x)\n",
    "        x = self.scoop_relu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11016, 200, 200, 4)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.utils.data as Data\n",
    "import scipy.ndimage\n",
    "BATCH_SIZE =  32\n",
    "NUM_EPOCH = 50\n",
    "\n",
    "# make data\n",
    "\n",
    "data_input = np.load(\"data_20210503/input_data_array.npy\")/255.0\n",
    "#normalization=np.ones(data_input.shape)\n",
    "#normalization[:,:,:]=[255,255,255,0.08]\n",
    "#data_input = data_input/normalization\n",
    "print(data_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406230 2018235\n",
      "Class weights: tensor([0.0000, 5.9682, 1.2013], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "input_array_mean = np.mean(np.mean(np.mean(data_input, axis=0), axis=0), axis=0)\n",
    "input_array_std = np.mean(np.mean(np.std(data_input, axis=0), axis=0), axis=0)\n",
    "print('input_array_mean', input_array_mean)\n",
    "print('input_array_std', input_array_std)\n",
    "'''\n",
    "\n",
    "#input_array_mean=np.ones(data_input.shape)\n",
    "#input_array_mean[:,:,:]=[0.54569177,0.48216905,0.49853667,0.42829879]\n",
    "#input_array_std=np.ones(data_input.shape)\n",
    "#input_array_std[:,:,:]=[0.31950833,0.29141234,0.30389949,0.20277719]\n",
    "\n",
    "#data_input = (data_input-input_array_mean)/input_array_std\n",
    "\n",
    "\n",
    "data_input = torch.from_numpy(data_input).permute(0,3,1,2)\n",
    "train_data_input = data_input[0: data_input.shape[0]-30, :, :, :]\n",
    "val_data_input = data_input[data_input.shape[0]-30: data_input.shape[0], :, :, :]\n",
    "\n",
    "data_label = np.load(\"data_20210503/label_data_array.npy\")\n",
    "\n",
    "data_label[data_label==0]=2\n",
    "data_label[data_label==255]=0\n",
    "data_label[(data_label==128)]=1\n",
    "#print(data_label[np.logical_and(np.logical_and((data_label!=0),(data_label!=1)),(data_label!=2))])\n",
    "\n",
    "\n",
    "good_cnt = (data_label==1).sum()\n",
    "bad_cnt = (data_label==2).sum()\n",
    "print(good_cnt, bad_cnt)\n",
    "total = good_cnt + bad_cnt\n",
    "\n",
    "weights = [0, total/good_cnt, total/bad_cnt]\n",
    "class_weights = torch.FloatTensor(weights)\n",
    "if torch.cuda.is_available():\n",
    "    class_weights = class_weights.cuda()\n",
    "print('Class weights:', class_weights)\n",
    "\n",
    "data_label = torch.from_numpy(data_label).long()\n",
    "train_data_label = data_label[0: data_label.shape[0]-30, :, :]\n",
    "val_data_label = data_label[data_label.shape[0]-30: data_label.shape[0], :, :]\n",
    "\n",
    "train_torch_dataset = Data.TensorDataset(train_data_input, train_data_label)\n",
    "loader = Data.DataLoader(dataset=train_torch_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "htmap_h = data_input.shape[2]\n",
    "htmap_w = data_input.shape[3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (pcpt_conv1): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (pcpt_bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pcpt_relu): ReLU(inplace=True)\n",
      "  (pcpt_maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pcpt_upsample): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "  (pcpt_layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pcpt_layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pcpt_layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (scoop_upsample): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "  (scoop_layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (scoop_layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (scoop_layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (scoop_conv1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (scoop_bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (scoop_conv2): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (scoop_bn2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (scoop_relu): ReLU(inplace=True)\n",
      "  (scoop_head): Linear(in_features=40000, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = ResNet(pcpt_block=BasicBlock, pcpt_layers=[1,5,1], scoop_block=BasicBlock, scoop_layers=[1,5,1], h=htmap_h, w=htmap_w).cuda()     # define the network\n",
    "print(net)  # net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0   step:  0   train loss:  1.2335237264633179  val loss:  1.1043323278427124\n",
      "min_val_loss_print 1.1043323278427124\n",
      "epoch:  0   step:  1   train loss:  1.2284549474716187  val loss:  1.0534299612045288\n",
      "min_val_loss_print 1.0534299612045288\n",
      "epoch:  0   step:  2   train loss:  1.2841556072235107  val loss:  1.079496145248413\n",
      "epoch:  0   step:  3   train loss:  1.1853079795837402  val loss:  1.168154239654541\n",
      "epoch:  0   step:  4   train loss:  1.1608363389968872  val loss:  1.1972343921661377\n",
      "epoch:  0   step:  5   train loss:  1.1357402801513672  val loss:  1.2147738933563232\n",
      "epoch:  0   step:  6   train loss:  1.144417405128479  val loss:  1.2305231094360352\n",
      "epoch:  0   step:  7   train loss:  1.168941855430603  val loss:  1.2437999248504639\n",
      "epoch:  0   step:  8   train loss:  1.1132926940917969  val loss:  1.3085803985595703\n",
      "epoch:  0   step:  9   train loss:  1.13316011428833  val loss:  1.3532899618148804\n",
      "epoch:  0   step:  10   train loss:  1.1312527656555176  val loss:  1.3467546701431274\n",
      "epoch:  0   step:  11   train loss:  1.0834766626358032  val loss:  1.3014161586761475\n",
      "epoch:  0   step:  12   train loss:  1.056172251701355  val loss:  1.2866946458816528\n",
      "epoch:  0   step:  13   train loss:  1.1191143989562988  val loss:  1.2956465482711792\n",
      "epoch:  0   step:  14   train loss:  1.0226974487304688  val loss:  1.3047267198562622\n",
      "epoch:  0   step:  15   train loss:  1.0741207599639893  val loss:  1.3137670755386353\n",
      "epoch:  0   step:  16   train loss:  1.0493215322494507  val loss:  1.3303016424179077\n",
      "epoch:  0   step:  17   train loss:  1.0674678087234497  val loss:  1.3423755168914795\n",
      "epoch:  0   step:  18   train loss:  0.9724046587944031  val loss:  1.3231821060180664\n",
      "epoch:  0   step:  19   train loss:  1.034645438194275  val loss:  1.3040908575057983\n",
      "epoch:  0   step:  20   train loss:  0.9926443099975586  val loss:  1.2939856052398682\n",
      "epoch:  0   step:  21   train loss:  0.9846383333206177  val loss:  1.279870867729187\n",
      "epoch:  0   step:  22   train loss:  1.0201586484909058  val loss:  1.26430344581604\n",
      "epoch:  0   step:  23   train loss:  0.9598529934883118  val loss:  1.2522265911102295\n",
      "epoch:  0   step:  24   train loss:  0.9894317984580994  val loss:  1.2360721826553345\n",
      "epoch:  0   step:  25   train loss:  0.987369954586029  val loss:  1.2053656578063965\n",
      "epoch:  0   step:  26   train loss:  0.9795969128608704  val loss:  1.1787630319595337\n",
      "epoch:  0   step:  27   train loss:  0.9783834218978882  val loss:  1.1945099830627441\n",
      "epoch:  0   step:  28   train loss:  0.9831171631813049  val loss:  1.1936225891113281\n",
      "epoch:  0   step:  29   train loss:  0.9847882986068726  val loss:  1.167769432067871\n",
      "epoch:  0   step:  30   train loss:  0.8976811766624451  val loss:  1.1771913766860962\n",
      "epoch:  0   step:  31   train loss:  0.9487297534942627  val loss:  1.1942188739776611\n",
      "epoch:  0   step:  32   train loss:  0.9485120177268982  val loss:  1.173169493675232\n",
      "epoch:  0   step:  33   train loss:  0.9076449275016785  val loss:  1.1589906215667725\n",
      "epoch:  0   step:  34   train loss:  0.9146559238433838  val loss:  1.1596572399139404\n",
      "epoch:  0   step:  35   train loss:  0.9213162064552307  val loss:  1.175453543663025\n",
      "epoch:  0   step:  36   train loss:  0.8611888289451599  val loss:  1.1907308101654053\n",
      "epoch:  0   step:  37   train loss:  0.9064744114875793  val loss:  1.1629308462142944\n",
      "epoch:  0   step:  38   train loss:  0.9179134964942932  val loss:  1.180191993713379\n",
      "epoch:  0   step:  39   train loss:  0.8906779885292053  val loss:  1.1727936267852783\n",
      "epoch:  0   step:  40   train loss:  0.9012190699577332  val loss:  1.1536179780960083\n",
      "epoch:  0   step:  41   train loss:  0.920133650302887  val loss:  1.144773006439209\n",
      "epoch:  0   step:  42   train loss:  0.8955605030059814  val loss:  1.1531155109405518\n",
      "epoch:  0   step:  43   train loss:  1.0146197080612183  val loss:  1.1583631038665771\n",
      "epoch:  0   step:  44   train loss:  0.930078387260437  val loss:  1.1724361181259155\n",
      "epoch:  0   step:  45   train loss:  0.933525562286377  val loss:  1.152855396270752\n",
      "epoch:  0   step:  46   train loss:  0.953274130821228  val loss:  1.1657522916793823\n",
      "epoch:  0   step:  47   train loss:  0.8755018711090088  val loss:  1.1543947458267212\n",
      "epoch:  0   step:  48   train loss:  0.8774980902671814  val loss:  1.1270339488983154\n",
      "epoch:  0   step:  49   train loss:  0.892062783241272  val loss:  1.1216872930526733\n",
      "epoch:  0   step:  50   train loss:  0.9054214954376221  val loss:  1.1253573894500732\n",
      "epoch:  0   step:  51   train loss:  0.8253664970397949  val loss:  1.1399997472763062\n",
      "epoch:  0   step:  52   train loss:  0.8479718565940857  val loss:  1.1451412439346313\n",
      "epoch:  0   step:  53   train loss:  0.8822392821311951  val loss:  1.1479400396347046\n",
      "epoch:  0   step:  54   train loss:  0.861990749835968  val loss:  1.1341791152954102\n",
      "epoch:  0   step:  55   train loss:  0.8364267349243164  val loss:  1.130121111869812\n",
      "epoch:  0   step:  56   train loss:  0.8296411633491516  val loss:  1.1127612590789795\n",
      "epoch:  0   step:  57   train loss:  0.8134043216705322  val loss:  1.111349105834961\n",
      "epoch:  0   step:  58   train loss:  0.810261607170105  val loss:  1.1114357709884644\n",
      "epoch:  0   step:  59   train loss:  0.837997317314148  val loss:  1.1134490966796875\n",
      "epoch:  0   step:  60   train loss:  0.9064191579818726  val loss:  1.1148756742477417\n",
      "epoch:  0   step:  61   train loss:  0.8743950128555298  val loss:  1.1264259815216064\n",
      "epoch:  0   step:  62   train loss:  0.746715784072876  val loss:  1.1319684982299805\n",
      "epoch:  0   step:  63   train loss:  0.8220593333244324  val loss:  1.1437766551971436\n",
      "epoch:  0   step:  64   train loss:  0.8557539582252502  val loss:  1.153870940208435\n",
      "epoch:  0   step:  65   train loss:  0.7829204797744751  val loss:  1.153279185295105\n",
      "epoch:  0   step:  66   train loss:  0.885650098323822  val loss:  1.1351279020309448\n",
      "epoch:  0   step:  67   train loss:  0.8699537515640259  val loss:  1.1202670335769653\n",
      "epoch:  0   step:  68   train loss:  0.8669514656066895  val loss:  1.118424892425537\n",
      "epoch:  0   step:  69   train loss:  0.8918715119361877  val loss:  1.1161435842514038\n",
      "epoch:  0   step:  70   train loss:  0.8470778465270996  val loss:  1.1161434650421143\n",
      "epoch:  0   step:  71   train loss:  0.8437612652778625  val loss:  1.1103373765945435\n",
      "epoch:  0   step:  72   train loss:  0.8835409879684448  val loss:  1.1054580211639404\n",
      "epoch:  0   step:  73   train loss:  0.8102704286575317  val loss:  1.103594422340393\n",
      "epoch:  0   step:  74   train loss:  0.7873929738998413  val loss:  1.109838843345642\n",
      "epoch:  0   step:  75   train loss:  0.8221733570098877  val loss:  1.10809326171875\n",
      "epoch:  0   step:  76   train loss:  0.7592811584472656  val loss:  1.11098313331604\n",
      "epoch:  0   step:  77   train loss:  0.7769147753715515  val loss:  1.1138923168182373\n",
      "epoch:  0   step:  78   train loss:  0.70055091381073  val loss:  1.114200472831726\n",
      "epoch:  0   step:  79   train loss:  0.8312588334083557  val loss:  1.1171624660491943\n",
      "epoch:  0   step:  80   train loss:  0.7896555662155151  val loss:  1.112337350845337\n",
      "epoch:  0   step:  81   train loss:  0.7838308215141296  val loss:  1.1151803731918335\n",
      "epoch:  0   step:  82   train loss:  0.8602563142776489  val loss:  1.1136181354522705\n",
      "epoch:  0   step:  83   train loss:  0.760563313961029  val loss:  1.1130379438400269\n",
      "epoch:  0   step:  84   train loss:  0.7512403130531311  val loss:  1.1166824102401733\n",
      "epoch:  0   step:  85   train loss:  0.7593693733215332  val loss:  1.121301531791687\n",
      "epoch:  0   step:  86   train loss:  0.798775851726532  val loss:  1.1137632131576538\n",
      "epoch:  0   step:  87   train loss:  0.8816705942153931  val loss:  1.113006830215454\n",
      "epoch:  0   step:  88   train loss:  0.7846464514732361  val loss:  1.1109205484390259\n",
      "epoch:  0   step:  89   train loss:  0.7828595042228699  val loss:  1.1042547225952148\n",
      "epoch:  0   step:  90   train loss:  0.7574318647384644  val loss:  1.099474549293518\n",
      "epoch:  0   step:  91   train loss:  0.7440458536148071  val loss:  1.1020970344543457\n",
      "epoch:  0   step:  92   train loss:  0.8340131044387817  val loss:  1.1027681827545166\n",
      "epoch:  0   step:  93   train loss:  0.7072051763534546  val loss:  1.100052833557129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0   step:  94   train loss:  0.7787453532218933  val loss:  1.0946744680404663\n",
      "epoch:  0   step:  95   train loss:  0.7232161164283752  val loss:  1.0896122455596924\n",
      "epoch:  0   step:  96   train loss:  0.8108125925064087  val loss:  1.0879814624786377\n",
      "epoch:  0   step:  97   train loss:  0.7937119007110596  val loss:  1.0844051837921143\n",
      "epoch:  0   step:  98   train loss:  0.7641717195510864  val loss:  1.0863267183303833\n",
      "epoch:  0   step:  99   train loss:  0.7214170098304749  val loss:  1.0881752967834473\n",
      "epoch:  0   step:  100   train loss:  0.6882071495056152  val loss:  1.0836213827133179\n",
      "epoch:  0   step:  101   train loss:  0.7453848719596863  val loss:  1.0783249139785767\n",
      "epoch:  0   step:  102   train loss:  0.751952588558197  val loss:  1.0776264667510986\n",
      "epoch:  0   step:  103   train loss:  0.8310127854347229  val loss:  1.074295163154602\n",
      "epoch:  0   step:  104   train loss:  0.8630310893058777  val loss:  1.0732964277267456\n",
      "epoch:  0   step:  105   train loss:  0.7173608541488647  val loss:  1.072729468345642\n",
      "epoch:  0   step:  106   train loss:  0.7423384785652161  val loss:  1.0735540390014648\n",
      "epoch:  0   step:  107   train loss:  0.7150919437408447  val loss:  1.0824646949768066\n",
      "epoch:  0   step:  108   train loss:  0.7590318322181702  val loss:  1.0867387056350708\n",
      "epoch:  0   step:  109   train loss:  0.7037115097045898  val loss:  1.0805293321609497\n",
      "epoch:  0   step:  110   train loss:  0.6925559639930725  val loss:  1.081526756286621\n",
      "epoch:  0   step:  111   train loss:  0.7900670766830444  val loss:  1.0822099447250366\n",
      "epoch:  0   step:  112   train loss:  0.6838445663452148  val loss:  1.0802276134490967\n",
      "epoch:  0   step:  113   train loss:  0.7341870069503784  val loss:  1.0809632539749146\n",
      "epoch:  0   step:  114   train loss:  0.7189326286315918  val loss:  1.0771186351776123\n",
      "epoch:  0   step:  115   train loss:  0.6862035989761353  val loss:  1.0752155780792236\n",
      "epoch:  0   step:  116   train loss:  0.6847798824310303  val loss:  1.0775820016860962\n",
      "epoch:  0   step:  117   train loss:  0.84507817029953  val loss:  1.0747451782226562\n",
      "epoch:  0   step:  118   train loss:  0.6910073161125183  val loss:  1.0674279928207397\n",
      "epoch:  0   step:  119   train loss:  0.7177940607070923  val loss:  1.06291663646698\n",
      "epoch:  0   step:  120   train loss:  0.635612428188324  val loss:  1.0583410263061523\n",
      "epoch:  0   step:  121   train loss:  0.7592493295669556  val loss:  1.0609468221664429\n",
      "epoch:  0   step:  122   train loss:  0.6807792782783508  val loss:  1.0625641345977783\n",
      "epoch:  0   step:  123   train loss:  0.6544526815414429  val loss:  1.0665063858032227\n",
      "epoch:  0   step:  124   train loss:  0.6959148645401001  val loss:  1.0617321729660034\n",
      "epoch:  0   step:  125   train loss:  0.7049832344055176  val loss:  1.0561063289642334\n",
      "epoch:  0   step:  126   train loss:  0.7770373821258545  val loss:  1.057664155960083\n",
      "epoch:  0   step:  127   train loss:  0.7565736174583435  val loss:  1.0587308406829834\n",
      "epoch:  0   step:  128   train loss:  0.7694450616836548  val loss:  1.0675519704818726\n",
      "epoch:  0   step:  129   train loss:  0.7238034605979919  val loss:  1.0680735111236572\n",
      "epoch:  0   step:  130   train loss:  0.7101384997367859  val loss:  1.061956763267517\n",
      "epoch:  0   step:  131   train loss:  0.6178982257843018  val loss:  1.0604898929595947\n",
      "epoch:  0   step:  132   train loss:  0.6644695401191711  val loss:  1.0582916736602783\n",
      "epoch:  0   step:  133   train loss:  0.6608226299285889  val loss:  1.054144263267517\n",
      "epoch:  0   step:  134   train loss:  0.6638855338096619  val loss:  1.0525773763656616\n",
      "min_val_loss_print 1.0525773763656616\n",
      "epoch:  0   step:  135   train loss:  0.6186628341674805  val loss:  1.0453872680664062\n",
      "min_val_loss_print 1.0453872680664062\n",
      "epoch:  0   step:  136   train loss:  0.6867973208427429  val loss:  1.043668270111084\n",
      "min_val_loss_print 1.043668270111084\n",
      "epoch:  0   step:  137   train loss:  0.6373337507247925  val loss:  1.0383763313293457\n",
      "min_val_loss_print 1.0383763313293457\n",
      "epoch:  0   step:  138   train loss:  0.6399025917053223  val loss:  1.0351632833480835\n",
      "min_val_loss_print 1.0351632833480835\n",
      "epoch:  0   step:  139   train loss:  0.6262862682342529  val loss:  1.0346426963806152\n",
      "min_val_loss_print 1.0346426963806152\n",
      "epoch:  0   step:  140   train loss:  0.6453510522842407  val loss:  1.030722975730896\n",
      "min_val_loss_print 1.030722975730896\n",
      "epoch:  0   step:  141   train loss:  0.6507644057273865  val loss:  1.0270992517471313\n",
      "min_val_loss_print 1.0270992517471313\n",
      "epoch:  0   step:  142   train loss:  0.580590546131134  val loss:  1.0271576642990112\n",
      "epoch:  0   step:  143   train loss:  0.7814212441444397  val loss:  1.0231350660324097\n",
      "min_val_loss_print 1.0231350660324097\n",
      "epoch:  0   step:  144   train loss:  0.6526831388473511  val loss:  1.0273754596710205\n",
      "epoch:  0   step:  145   train loss:  0.6838664412498474  val loss:  1.0332574844360352\n",
      "epoch:  0   step:  146   train loss:  0.7334098219871521  val loss:  1.04253089427948\n",
      "epoch:  0   step:  147   train loss:  0.6676183342933655  val loss:  1.0495145320892334\n",
      "epoch:  0   step:  148   train loss:  0.6755819916725159  val loss:  1.0473641157150269\n",
      "epoch:  0   step:  149   train loss:  0.7514488697052002  val loss:  1.050970435142517\n",
      "epoch:  0   step:  150   train loss:  0.7293037176132202  val loss:  1.050648808479309\n",
      "epoch:  0   step:  151   train loss:  0.635753333568573  val loss:  1.0384584665298462\n",
      "epoch:  0   step:  152   train loss:  0.6346390843391418  val loss:  1.0357986688613892\n",
      "epoch:  0   step:  153   train loss:  0.6659196615219116  val loss:  1.0350090265274048\n",
      "epoch:  0   step:  154   train loss:  0.6329614520072937  val loss:  1.036900281906128\n",
      "epoch:  0   step:  155   train loss:  0.6674356460571289  val loss:  1.0378081798553467\n",
      "epoch:  0   step:  156   train loss:  0.6313264966011047  val loss:  1.030222773551941\n",
      "epoch:  0   step:  157   train loss:  0.5559133887290955  val loss:  1.02482008934021\n",
      "epoch:  0   step:  158   train loss:  0.6175562143325806  val loss:  1.0212360620498657\n",
      "min_val_loss_print 1.0212360620498657\n",
      "epoch:  0   step:  159   train loss:  0.6410903334617615  val loss:  1.0202373266220093\n",
      "min_val_loss_print 1.0202373266220093\n",
      "epoch:  0   step:  160   train loss:  0.6322988867759705  val loss:  1.0212864875793457\n",
      "epoch:  0   step:  161   train loss:  0.6689531803131104  val loss:  1.0231636762619019\n",
      "epoch:  0   step:  162   train loss:  0.6740078926086426  val loss:  1.0275553464889526\n",
      "epoch:  0   step:  163   train loss:  0.5870637893676758  val loss:  1.0301374197006226\n",
      "epoch:  0   step:  164   train loss:  0.5885498523712158  val loss:  1.0327162742614746\n",
      "epoch:  0   step:  165   train loss:  0.5909841060638428  val loss:  1.033120036125183\n",
      "epoch:  0   step:  166   train loss:  0.6093619465827942  val loss:  1.043260097503662\n",
      "epoch:  0   step:  167   train loss:  0.5740565657615662  val loss:  1.0450847148895264\n",
      "epoch:  0   step:  168   train loss:  0.5641553997993469  val loss:  1.0426567792892456\n",
      "epoch:  0   step:  169   train loss:  0.7315760254859924  val loss:  1.0466750860214233\n",
      "epoch:  0   step:  170   train loss:  0.7302912473678589  val loss:  1.050296425819397\n",
      "epoch:  0   step:  171   train loss:  0.6685355305671692  val loss:  1.047646164894104\n",
      "epoch:  0   step:  172   train loss:  0.6317332983016968  val loss:  1.0439132452011108\n",
      "epoch:  0   step:  173   train loss:  0.6274071931838989  val loss:  1.0391738414764404\n",
      "epoch:  0   step:  174   train loss:  0.6050264835357666  val loss:  1.035253643989563\n",
      "epoch:  0   step:  175   train loss:  0.6167877912521362  val loss:  1.0345913171768188\n",
      "epoch:  0   step:  176   train loss:  0.49859219789505005  val loss:  1.0368973016738892\n",
      "epoch:  0   step:  177   train loss:  0.6799888610839844  val loss:  1.0354150533676147\n",
      "epoch:  0   step:  178   train loss:  0.7084513902664185  val loss:  1.0351239442825317\n",
      "epoch:  0   step:  179   train loss:  0.6353773474693298  val loss:  1.033823013305664\n",
      "epoch:  0   step:  180   train loss:  0.700471818447113  val loss:  1.0406134128570557\n",
      "epoch:  0   step:  181   train loss:  0.6026134490966797  val loss:  1.0417604446411133\n",
      "epoch:  0   step:  182   train loss:  0.5393566489219666  val loss:  1.045546293258667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0   step:  183   train loss:  0.5607079267501831  val loss:  1.0433772802352905\n",
      "epoch:  0   step:  184   train loss:  0.5704908967018127  val loss:  1.0391621589660645\n",
      "epoch:  0   step:  185   train loss:  0.5660005211830139  val loss:  1.0391632318496704\n",
      "epoch:  0   step:  186   train loss:  0.6284170746803284  val loss:  1.0337727069854736\n",
      "epoch:  0   step:  187   train loss:  0.5351576805114746  val loss:  1.0353777408599854\n",
      "epoch:  0   step:  188   train loss:  0.5767691135406494  val loss:  1.036331295967102\n",
      "epoch:  0   step:  189   train loss:  0.6438424587249756  val loss:  1.0349843502044678\n",
      "epoch:  0   step:  190   train loss:  0.5485089421272278  val loss:  1.0361069440841675\n",
      "epoch:  0   step:  191   train loss:  0.5575436353683472  val loss:  1.0390796661376953\n",
      "epoch:  0   step:  192   train loss:  0.5337405204772949  val loss:  1.0393612384796143\n",
      "epoch:  0   step:  193   train loss:  0.5300801396369934  val loss:  1.0385831594467163\n",
      "epoch:  0   step:  194   train loss:  0.5275602340698242  val loss:  1.0332565307617188\n",
      "epoch:  0   step:  195   train loss:  0.6020979881286621  val loss:  1.0280739068984985\n",
      "epoch:  0   step:  196   train loss:  0.5912467837333679  val loss:  1.026714563369751\n",
      "epoch:  0   step:  197   train loss:  0.6411319375038147  val loss:  1.0282799005508423\n",
      "epoch:  0   step:  198   train loss:  0.5037980079650879  val loss:  1.0288469791412354\n",
      "epoch:  0   step:  199   train loss:  0.6090568900108337  val loss:  1.021243691444397\n",
      "epoch:  0   step:  200   train loss:  0.576071560382843  val loss:  1.019034743309021\n",
      "min_val_loss_print 1.019034743309021\n",
      "epoch:  0   step:  201   train loss:  0.5773320198059082  val loss:  1.0131953954696655\n",
      "min_val_loss_print 1.0131953954696655\n",
      "epoch:  0   step:  202   train loss:  0.5416547656059265  val loss:  1.007391095161438\n",
      "min_val_loss_print 1.007391095161438\n",
      "epoch:  0   step:  203   train loss:  0.5636801719665527  val loss:  1.0089315176010132\n",
      "epoch:  0   step:  204   train loss:  0.5561892986297607  val loss:  1.011610746383667\n",
      "epoch:  0   step:  205   train loss:  0.6547573208808899  val loss:  1.0173546075820923\n",
      "epoch:  0   step:  206   train loss:  0.6293772459030151  val loss:  1.0182468891143799\n",
      "epoch:  0   step:  207   train loss:  0.5264849066734314  val loss:  1.0116572380065918\n",
      "epoch:  0   step:  208   train loss:  0.6103630065917969  val loss:  1.0062013864517212\n",
      "min_val_loss_print 1.0062013864517212\n",
      "epoch:  0   step:  209   train loss:  0.5228857398033142  val loss:  1.0093756914138794\n",
      "epoch:  0   step:  210   train loss:  0.6261190176010132  val loss:  1.005232572555542\n",
      "min_val_loss_print 1.005232572555542\n",
      "epoch:  0   step:  211   train loss:  0.5465441942214966  val loss:  1.0037444829940796\n",
      "min_val_loss_print 1.0037444829940796\n",
      "epoch:  0   step:  212   train loss:  0.45997557044029236  val loss:  1.0036861896514893\n",
      "min_val_loss_print 1.0036861896514893\n",
      "epoch:  0   step:  213   train loss:  0.642594575881958  val loss:  1.0025010108947754\n",
      "min_val_loss_print 1.0025010108947754\n",
      "epoch:  0   step:  214   train loss:  0.5829384922981262  val loss:  1.0060813426971436\n",
      "epoch:  0   step:  215   train loss:  0.5202305316925049  val loss:  1.008114218711853\n",
      "epoch:  0   step:  216   train loss:  0.7038822770118713  val loss:  1.0076689720153809\n",
      "epoch:  0   step:  217   train loss:  0.5586659908294678  val loss:  1.0124043226242065\n",
      "epoch:  0   step:  218   train loss:  0.55837482213974  val loss:  1.0171245336532593\n",
      "epoch:  0   step:  219   train loss:  0.5688695907592773  val loss:  1.0177969932556152\n",
      "epoch:  0   step:  220   train loss:  0.6393181085586548  val loss:  1.0144716501235962\n",
      "epoch:  0   step:  221   train loss:  0.6522569060325623  val loss:  1.0128228664398193\n",
      "epoch:  0   step:  222   train loss:  0.5113732218742371  val loss:  1.0118606090545654\n",
      "epoch:  0   step:  223   train loss:  0.5649799108505249  val loss:  1.017561912536621\n",
      "epoch:  0   step:  224   train loss:  0.6204015612602234  val loss:  1.0232012271881104\n",
      "epoch:  0   step:  225   train loss:  0.41039571166038513  val loss:  1.0284250974655151\n",
      "epoch:  0   step:  226   train loss:  0.6570441722869873  val loss:  1.024263620376587\n",
      "epoch:  0   step:  227   train loss:  0.5176264047622681  val loss:  1.019496202468872\n",
      "epoch:  0   step:  228   train loss:  0.5276162624359131  val loss:  1.0124350786209106\n",
      "epoch:  0   step:  229   train loss:  0.502514660358429  val loss:  1.0079874992370605\n",
      "epoch:  0   step:  230   train loss:  0.5227674245834351  val loss:  1.0059499740600586\n",
      "epoch:  0   step:  231   train loss:  0.5016703605651855  val loss:  1.0037531852722168\n",
      "epoch:  0   step:  232   train loss:  0.5817424058914185  val loss:  1.0011709928512573\n",
      "min_val_loss_print 1.0011709928512573\n",
      "epoch:  0   step:  233   train loss:  0.5223138332366943  val loss:  0.9938024878501892\n",
      "min_val_loss_print 0.9938024878501892\n",
      "epoch:  0   step:  234   train loss:  0.567200243473053  val loss:  0.9861778616905212\n",
      "min_val_loss_print 0.9861778616905212\n",
      "epoch:  0   step:  235   train loss:  0.543978214263916  val loss:  0.984740674495697\n",
      "min_val_loss_print 0.984740674495697\n",
      "epoch:  0   step:  236   train loss:  0.5516787171363831  val loss:  0.9765583276748657\n",
      "min_val_loss_print 0.9765583276748657\n",
      "epoch:  0   step:  237   train loss:  0.5185707211494446  val loss:  0.9735758304595947\n",
      "min_val_loss_print 0.9735758304595947\n",
      "epoch:  0   step:  238   train loss:  0.5539162755012512  val loss:  0.964145302772522\n",
      "min_val_loss_print 0.964145302772522\n",
      "epoch:  0   step:  239   train loss:  0.4881196618080139  val loss:  0.9646726250648499\n",
      "epoch:  0   step:  240   train loss:  0.5088468790054321  val loss:  0.9647377729415894\n",
      "epoch:  0   step:  241   train loss:  0.46532750129699707  val loss:  0.9669046401977539\n",
      "epoch:  0   step:  242   train loss:  0.46793147921562195  val loss:  0.9648228287696838\n",
      "epoch:  0   step:  243   train loss:  0.5648290514945984  val loss:  0.9593537449836731\n",
      "min_val_loss_print 0.9593537449836731\n",
      "epoch:  0   step:  244   train loss:  0.5254446864128113  val loss:  0.9617010951042175\n",
      "epoch:  0   step:  245   train loss:  0.49976658821105957  val loss:  0.9538346529006958\n",
      "min_val_loss_print 0.9538346529006958\n",
      "epoch:  0   step:  246   train loss:  0.45251262187957764  val loss:  0.9497737884521484\n",
      "min_val_loss_print 0.9497737884521484\n",
      "epoch:  0   step:  247   train loss:  0.4942875802516937  val loss:  0.9430565237998962\n",
      "min_val_loss_print 0.9430565237998962\n",
      "epoch:  0   step:  248   train loss:  0.5276964902877808  val loss:  0.9428876638412476\n",
      "min_val_loss_print 0.9428876638412476\n",
      "epoch:  0   step:  249   train loss:  0.5999972820281982  val loss:  0.9497153759002686\n",
      "epoch:  0   step:  250   train loss:  0.5884755849838257  val loss:  0.9500030875205994\n",
      "epoch:  0   step:  251   train loss:  0.5255072116851807  val loss:  0.9518062472343445\n",
      "epoch:  0   step:  252   train loss:  0.3963538706302643  val loss:  0.949571430683136\n",
      "epoch:  0   step:  253   train loss:  0.5456222295761108  val loss:  0.9454890489578247\n",
      "epoch:  0   step:  254   train loss:  0.5072513222694397  val loss:  0.9387309551239014\n",
      "min_val_loss_print 0.9387309551239014\n",
      "epoch:  0   step:  255   train loss:  0.4703899621963501  val loss:  0.9338362812995911\n",
      "min_val_loss_print 0.9338362812995911\n",
      "epoch:  0   step:  256   train loss:  0.519601583480835  val loss:  0.9272373914718628\n",
      "min_val_loss_print 0.9272373914718628\n",
      "epoch:  0   step:  257   train loss:  0.4767240881919861  val loss:  0.9245115518569946\n",
      "min_val_loss_print 0.9245115518569946\n",
      "epoch:  0   step:  258   train loss:  0.5445517301559448  val loss:  0.9280846118927002\n",
      "epoch:  0   step:  259   train loss:  0.5990543961524963  val loss:  0.9274431467056274\n",
      "epoch:  0   step:  260   train loss:  0.5129991769790649  val loss:  0.9281983375549316\n",
      "epoch:  0   step:  261   train loss:  0.525788426399231  val loss:  0.9265677332878113\n",
      "epoch:  0   step:  262   train loss:  0.48112744092941284  val loss:  0.9240379333496094\n",
      "min_val_loss_print 0.9240379333496094\n",
      "epoch:  0   step:  263   train loss:  0.47177037596702576  val loss:  0.9297244548797607\n",
      "epoch:  0   step:  264   train loss:  0.512218713760376  val loss:  0.9374842047691345\n",
      "epoch:  0   step:  265   train loss:  0.4760804772377014  val loss:  0.940007209777832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0   step:  266   train loss:  0.5514103770256042  val loss:  0.939519464969635\n",
      "epoch:  0   step:  267   train loss:  0.5124726891517639  val loss:  0.9358761310577393\n",
      "epoch:  0   step:  268   train loss:  0.4779519736766815  val loss:  0.9362013339996338\n",
      "epoch:  0   step:  269   train loss:  0.4966558516025543  val loss:  0.9371048212051392\n",
      "epoch:  0   step:  270   train loss:  0.5358237028121948  val loss:  0.9351515769958496\n",
      "epoch:  0   step:  271   train loss:  0.46594396233558655  val loss:  0.9308488368988037\n",
      "epoch:  0   step:  272   train loss:  0.586431622505188  val loss:  0.9219648838043213\n",
      "min_val_loss_print 0.9219648838043213\n",
      "epoch:  0   step:  273   train loss:  0.49211710691452026  val loss:  0.9211357831954956\n",
      "min_val_loss_print 0.9211357831954956\n",
      "epoch:  0   step:  274   train loss:  0.49378183484077454  val loss:  0.9165507555007935\n",
      "min_val_loss_print 0.9165507555007935\n",
      "epoch:  0   step:  275   train loss:  0.4240872859954834  val loss:  0.9126670956611633\n",
      "min_val_loss_print 0.9126670956611633\n",
      "epoch:  0   step:  276   train loss:  0.49475374817848206  val loss:  0.9106149673461914\n",
      "min_val_loss_print 0.9106149673461914\n",
      "epoch:  0   step:  277   train loss:  0.46938246488571167  val loss:  0.911217212677002\n",
      "epoch:  0   step:  278   train loss:  0.5138214826583862  val loss:  0.9157269597053528\n",
      "epoch:  0   step:  279   train loss:  0.5297816395759583  val loss:  0.9127606749534607\n",
      "epoch:  0   step:  280   train loss:  0.5223437547683716  val loss:  0.9157180190086365\n",
      "epoch:  0   step:  281   train loss:  0.449954628944397  val loss:  0.9195890426635742\n",
      "epoch:  0   step:  282   train loss:  0.5597074031829834  val loss:  0.9158429503440857\n",
      "epoch:  0   step:  283   train loss:  0.4589979350566864  val loss:  0.9180012345314026\n",
      "epoch:  0   step:  284   train loss:  0.43269190192222595  val loss:  0.9159002304077148\n",
      "epoch:  0   step:  285   train loss:  0.4816439747810364  val loss:  0.9246429204940796\n",
      "epoch:  0   step:  286   train loss:  0.4006267189979553  val loss:  0.923524796962738\n",
      "epoch:  0   step:  287   train loss:  0.526574969291687  val loss:  0.9234288334846497\n",
      "epoch:  0   step:  288   train loss:  0.4866001307964325  val loss:  0.9191094040870667\n",
      "epoch:  0   step:  289   train loss:  0.4499826729297638  val loss:  0.9158249497413635\n",
      "epoch:  0   step:  290   train loss:  0.4841679334640503  val loss:  0.9095565676689148\n",
      "min_val_loss_print 0.9095565676689148\n",
      "epoch:  0   step:  291   train loss:  0.5353269577026367  val loss:  0.9093997478485107\n",
      "min_val_loss_print 0.9093997478485107\n",
      "epoch:  0   step:  292   train loss:  0.42834144830703735  val loss:  0.9082465767860413\n",
      "min_val_loss_print 0.9082465767860413\n",
      "epoch:  0   step:  293   train loss:  0.44837072491645813  val loss:  0.9059875011444092\n",
      "min_val_loss_print 0.9059875011444092\n",
      "epoch:  0   step:  294   train loss:  0.5183703303337097  val loss:  0.9060033559799194\n",
      "epoch:  0   step:  295   train loss:  0.43461593985557556  val loss:  0.9000335931777954\n",
      "min_val_loss_print 0.9000335931777954\n",
      "epoch:  0   step:  296   train loss:  0.4812021553516388  val loss:  0.896530270576477\n",
      "min_val_loss_print 0.896530270576477\n",
      "epoch:  0   step:  297   train loss:  0.46146905422210693  val loss:  0.8917948603630066\n",
      "min_val_loss_print 0.8917948603630066\n",
      "epoch:  0   step:  298   train loss:  0.3869664669036865  val loss:  0.8839450478553772\n",
      "min_val_loss_print 0.8839450478553772\n",
      "epoch:  0   step:  299   train loss:  0.4621615707874298  val loss:  0.8779709935188293\n",
      "min_val_loss_print 0.8779709935188293\n",
      "epoch:  0   step:  300   train loss:  0.36639630794525146  val loss:  0.8787811398506165\n",
      "epoch:  0   step:  301   train loss:  0.4815778136253357  val loss:  0.8801906704902649\n",
      "epoch:  0   step:  302   train loss:  0.46780258417129517  val loss:  0.8808225393295288\n",
      "epoch:  0   step:  303   train loss:  0.4235863983631134  val loss:  0.8813835382461548\n",
      "epoch:  0   step:  304   train loss:  0.43627721071243286  val loss:  0.8761429190635681\n",
      "min_val_loss_print 0.8761429190635681\n",
      "epoch:  0   step:  305   train loss:  0.4784940481185913  val loss:  0.8690234422683716\n",
      "min_val_loss_print 0.8690234422683716\n",
      "epoch:  0   step:  306   train loss:  0.46436893939971924  val loss:  0.8651615977287292\n",
      "min_val_loss_print 0.8651615977287292\n",
      "epoch:  0   step:  307   train loss:  0.40204858779907227  val loss:  0.8623977303504944\n",
      "min_val_loss_print 0.8623977303504944\n",
      "epoch:  0   step:  308   train loss:  0.4233984649181366  val loss:  0.8603241443634033\n",
      "min_val_loss_print 0.8603241443634033\n",
      "epoch:  0   step:  309   train loss:  0.481157511472702  val loss:  0.8619651198387146\n",
      "epoch:  0   step:  310   train loss:  0.4731779992580414  val loss:  0.8559867739677429\n",
      "min_val_loss_print 0.8559867739677429\n",
      "epoch:  0   step:  311   train loss:  0.4707629382610321  val loss:  0.8494217395782471\n",
      "min_val_loss_print 0.8494217395782471\n",
      "epoch:  0   step:  312   train loss:  0.42690610885620117  val loss:  0.8445190787315369\n",
      "min_val_loss_print 0.8445190787315369\n",
      "epoch:  0   step:  313   train loss:  0.4659169316291809  val loss:  0.8389307260513306\n",
      "min_val_loss_print 0.8389307260513306\n",
      "epoch:  0   step:  314   train loss:  0.4561946392059326  val loss:  0.8388174176216125\n",
      "min_val_loss_print 0.8388174176216125\n",
      "epoch:  0   step:  315   train loss:  0.4328746497631073  val loss:  0.8385864496231079\n",
      "min_val_loss_print 0.8385864496231079\n",
      "epoch:  0   step:  316   train loss:  0.3388654291629791  val loss:  0.841547429561615\n",
      "epoch:  0   step:  317   train loss:  0.44135433435440063  val loss:  0.8417248725891113\n",
      "epoch:  0   step:  318   train loss:  0.44881701469421387  val loss:  0.8407918214797974\n",
      "epoch:  0   step:  319   train loss:  0.4569613039493561  val loss:  0.8394887447357178\n",
      "epoch:  0   step:  320   train loss:  0.4510252773761749  val loss:  0.8410732746124268\n",
      "epoch:  0   step:  321   train loss:  0.4755776822566986  val loss:  0.8439887762069702\n",
      "epoch:  0   step:  322   train loss:  0.35620516538619995  val loss:  0.8471448421478271\n",
      "epoch:  0   step:  323   train loss:  0.4592331349849701  val loss:  0.8500284552574158\n",
      "epoch:  0   step:  324   train loss:  0.4087534248828888  val loss:  0.8552494049072266\n",
      "epoch:  0   step:  325   train loss:  0.42150428891181946  val loss:  0.8597515821456909\n",
      "epoch:  0   step:  326   train loss:  0.506208062171936  val loss:  0.8597627282142639\n",
      "epoch:  0   step:  327   train loss:  0.37833625078201294  val loss:  0.861372172832489\n",
      "epoch:  0   step:  328   train loss:  0.44042837619781494  val loss:  0.8609337210655212\n",
      "epoch:  0   step:  329   train loss:  0.38276317715644836  val loss:  0.8624520897865295\n",
      "epoch:  0   step:  330   train loss:  0.35298067331314087  val loss:  0.8634840250015259\n",
      "epoch:  0   step:  331   train loss:  0.4520830810070038  val loss:  0.861721932888031\n",
      "epoch:  0   step:  332   train loss:  0.46555301547050476  val loss:  0.8594908714294434\n",
      "epoch:  0   step:  333   train loss:  0.43020883202552795  val loss:  0.8556429743766785\n",
      "epoch:  0   step:  334   train loss:  0.5009817481040955  val loss:  0.8546285033226013\n",
      "epoch:  0   step:  335   train loss:  0.44451379776000977  val loss:  0.8409875631332397\n",
      "epoch:  0   step:  336   train loss:  0.36966440081596375  val loss:  0.8275496959686279\n",
      "min_val_loss_print 0.8275496959686279\n",
      "epoch:  0   step:  337   train loss:  0.3278914988040924  val loss:  0.8250062465667725\n",
      "min_val_loss_print 0.8250062465667725\n",
      "epoch:  0   step:  338   train loss:  0.41609010100364685  val loss:  0.8131048679351807\n",
      "min_val_loss_print 0.8131048679351807\n",
      "epoch:  0   step:  339   train loss:  0.4087119698524475  val loss:  0.8049957156181335\n",
      "min_val_loss_print 0.8049957156181335\n",
      "epoch:  0   step:  340   train loss:  0.445804238319397  val loss:  0.8019802570343018\n",
      "min_val_loss_print 0.8019802570343018\n",
      "epoch:  0   step:  341   train loss:  0.36756017804145813  val loss:  0.7954103946685791\n",
      "min_val_loss_print 0.7954103946685791\n",
      "epoch:  0   step:  342   train loss:  0.39207878708839417  val loss:  0.7904849052429199\n",
      "min_val_loss_print 0.7904849052429199\n",
      "epoch:  0   step:  343   train loss:  0.48835766315460205  val loss:  0.7861866354942322\n",
      "min_val_loss_print 0.7861866354942322\n",
      "epoch:  1   step:  0   train loss:  0.3741757869720459  val loss:  0.7812741994857788\n",
      "min_val_loss_print 0.7812741994857788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1   step:  1   train loss:  0.34351441264152527  val loss:  0.7791616916656494\n",
      "min_val_loss_print 0.7791616916656494\n",
      "epoch:  1   step:  2   train loss:  0.35779452323913574  val loss:  0.7816834449768066\n",
      "epoch:  1   step:  3   train loss:  0.4037347733974457  val loss:  0.7843561172485352\n",
      "epoch:  1   step:  4   train loss:  0.41448554396629333  val loss:  0.7860268950462341\n",
      "epoch:  1   step:  5   train loss:  0.40600019693374634  val loss:  0.7907997369766235\n",
      "epoch:  1   step:  6   train loss:  0.3702530860900879  val loss:  0.7888341546058655\n",
      "epoch:  1   step:  7   train loss:  0.3423885107040405  val loss:  0.7900857329368591\n",
      "epoch:  1   step:  8   train loss:  0.34402966499328613  val loss:  0.789971113204956\n",
      "epoch:  1   step:  9   train loss:  0.3425004482269287  val loss:  0.7899829149246216\n",
      "epoch:  1   step:  10   train loss:  0.4193994104862213  val loss:  0.7942352890968323\n",
      "epoch:  1   step:  11   train loss:  0.33250826597213745  val loss:  0.7916088104248047\n",
      "epoch:  1   step:  12   train loss:  0.38803696632385254  val loss:  0.7939176559448242\n",
      "epoch:  1   step:  13   train loss:  0.3903360068798065  val loss:  0.7967936992645264\n",
      "epoch:  1   step:  14   train loss:  0.37333160638809204  val loss:  0.7958002090454102\n",
      "epoch:  1   step:  15   train loss:  0.369098424911499  val loss:  0.8000625967979431\n",
      "epoch:  1   step:  16   train loss:  0.3828304708003998  val loss:  0.7954613566398621\n",
      "epoch:  1   step:  17   train loss:  0.37774765491485596  val loss:  0.7968446016311646\n",
      "epoch:  1   step:  18   train loss:  0.34842395782470703  val loss:  0.7985670566558838\n",
      "epoch:  1   step:  19   train loss:  0.39158767461776733  val loss:  0.7967370748519897\n",
      "epoch:  1   step:  20   train loss:  0.3533571660518646  val loss:  0.7956899404525757\n",
      "epoch:  1   step:  21   train loss:  0.370150625705719  val loss:  0.7928228974342346\n",
      "epoch:  1   step:  22   train loss:  0.3991568088531494  val loss:  0.7866829633712769\n",
      "epoch:  1   step:  23   train loss:  0.3376756012439728  val loss:  0.7842908501625061\n",
      "epoch:  1   step:  24   train loss:  0.33955278992652893  val loss:  0.7849591374397278\n",
      "epoch:  1   step:  25   train loss:  0.3035907745361328  val loss:  0.7811635136604309\n",
      "epoch:  1   step:  26   train loss:  0.3209361135959625  val loss:  0.777505099773407\n",
      "min_val_loss_print 0.777505099773407\n",
      "epoch:  1   step:  27   train loss:  0.34176039695739746  val loss:  0.7751055955886841\n",
      "min_val_loss_print 0.7751055955886841\n",
      "epoch:  1   step:  28   train loss:  0.4447343051433563  val loss:  0.7740212678909302\n",
      "min_val_loss_print 0.7740212678909302\n",
      "epoch:  1   step:  29   train loss:  0.3398151099681854  val loss:  0.767911970615387\n",
      "min_val_loss_print 0.767911970615387\n",
      "epoch:  1   step:  30   train loss:  0.3419281542301178  val loss:  0.7622624635696411\n",
      "min_val_loss_print 0.7622624635696411\n",
      "epoch:  1   step:  31   train loss:  0.3631652295589447  val loss:  0.7589669227600098\n",
      "min_val_loss_print 0.7589669227600098\n",
      "epoch:  1   step:  32   train loss:  0.32404667139053345  val loss:  0.7562741041183472\n",
      "min_val_loss_print 0.7562741041183472\n",
      "epoch:  1   step:  33   train loss:  0.45922204852104187  val loss:  0.7547683715820312\n",
      "min_val_loss_print 0.7547683715820312\n",
      "epoch:  1   step:  34   train loss:  0.3467409610748291  val loss:  0.752495527267456\n",
      "min_val_loss_print 0.752495527267456\n",
      "epoch:  1   step:  35   train loss:  0.3554949462413788  val loss:  0.7472593784332275\n",
      "min_val_loss_print 0.7472593784332275\n",
      "epoch:  1   step:  36   train loss:  0.4010360538959503  val loss:  0.7437608242034912\n",
      "min_val_loss_print 0.7437608242034912\n",
      "epoch:  1   step:  37   train loss:  0.3865034282207489  val loss:  0.7365538477897644\n",
      "min_val_loss_print 0.7365538477897644\n",
      "epoch:  1   step:  38   train loss:  0.2900371849536896  val loss:  0.732232928276062\n",
      "min_val_loss_print 0.732232928276062\n",
      "epoch:  1   step:  39   train loss:  0.36393269896507263  val loss:  0.7263553142547607\n",
      "min_val_loss_print 0.7263553142547607\n",
      "epoch:  1   step:  40   train loss:  0.4159449636936188  val loss:  0.7122997045516968\n",
      "min_val_loss_print 0.7122997045516968\n",
      "epoch:  1   step:  41   train loss:  0.41505396366119385  val loss:  0.7045578360557556\n",
      "min_val_loss_print 0.7045578360557556\n",
      "epoch:  1   step:  42   train loss:  0.33114737272262573  val loss:  0.7010306119918823\n",
      "min_val_loss_print 0.7010306119918823\n",
      "epoch:  1   step:  43   train loss:  0.45896047353744507  val loss:  0.700872004032135\n",
      "min_val_loss_print 0.700872004032135\n",
      "epoch:  1   step:  44   train loss:  0.41450318694114685  val loss:  0.6973897814750671\n",
      "min_val_loss_print 0.6973897814750671\n",
      "epoch:  1   step:  45   train loss:  0.3929312229156494  val loss:  0.6983296871185303\n",
      "epoch:  1   step:  46   train loss:  0.38359612226486206  val loss:  0.7005173563957214\n",
      "epoch:  1   step:  47   train loss:  0.3624923825263977  val loss:  0.6995094418525696\n",
      "epoch:  1   step:  48   train loss:  0.28730687499046326  val loss:  0.7029622793197632\n",
      "epoch:  1   step:  49   train loss:  0.3270568549633026  val loss:  0.7006053328514099\n",
      "epoch:  1   step:  50   train loss:  0.35935112833976746  val loss:  0.7050563097000122\n",
      "epoch:  1   step:  51   train loss:  0.3549192547798157  val loss:  0.7052019834518433\n",
      "epoch:  1   step:  52   train loss:  0.3437824547290802  val loss:  0.700329601764679\n",
      "epoch:  1   step:  53   train loss:  0.309075266122818  val loss:  0.698444128036499\n",
      "epoch:  1   step:  54   train loss:  0.37929099798202515  val loss:  0.6961367726325989\n",
      "min_val_loss_print 0.6961367726325989\n",
      "epoch:  1   step:  55   train loss:  0.35185378789901733  val loss:  0.6931602358818054\n",
      "min_val_loss_print 0.6931602358818054\n",
      "epoch:  1   step:  56   train loss:  0.3693690896034241  val loss:  0.6903092861175537\n",
      "min_val_loss_print 0.6903092861175537\n",
      "epoch:  1   step:  57   train loss:  0.36299601197242737  val loss:  0.6911727786064148\n",
      "epoch:  1   step:  58   train loss:  0.4374828338623047  val loss:  0.6889329552650452\n",
      "min_val_loss_print 0.6889329552650452\n",
      "epoch:  1   step:  59   train loss:  0.310690701007843  val loss:  0.6878923773765564\n",
      "min_val_loss_print 0.6878923773765564\n",
      "epoch:  1   step:  60   train loss:  0.3631366491317749  val loss:  0.6828615069389343\n",
      "min_val_loss_print 0.6828615069389343\n",
      "epoch:  1   step:  61   train loss:  0.29612091183662415  val loss:  0.6824395656585693\n",
      "min_val_loss_print 0.6824395656585693\n",
      "epoch:  1   step:  62   train loss:  0.3922511637210846  val loss:  0.6808711886405945\n",
      "min_val_loss_print 0.6808711886405945\n",
      "epoch:  1   step:  63   train loss:  0.3802963197231293  val loss:  0.6822231411933899\n",
      "epoch:  1   step:  64   train loss:  0.3911725580692291  val loss:  0.6822667717933655\n",
      "epoch:  1   step:  65   train loss:  0.33483219146728516  val loss:  0.6804719567298889\n",
      "min_val_loss_print 0.6804719567298889\n",
      "epoch:  1   step:  66   train loss:  0.2724647521972656  val loss:  0.6773056983947754\n",
      "min_val_loss_print 0.6773056983947754\n",
      "epoch:  1   step:  67   train loss:  0.3203291594982147  val loss:  0.6772475838661194\n",
      "min_val_loss_print 0.6772475838661194\n",
      "epoch:  1   step:  68   train loss:  0.31287682056427  val loss:  0.6799927353858948\n",
      "epoch:  1   step:  69   train loss:  0.28977423906326294  val loss:  0.6781309247016907\n",
      "epoch:  1   step:  70   train loss:  0.3334455192089081  val loss:  0.6797071695327759\n",
      "epoch:  1   step:  71   train loss:  0.36360371112823486  val loss:  0.6769725680351257\n",
      "min_val_loss_print 0.6769725680351257\n",
      "epoch:  1   step:  72   train loss:  0.3037356436252594  val loss:  0.6801398992538452\n",
      "epoch:  1   step:  73   train loss:  0.37309563159942627  val loss:  0.6822874546051025\n",
      "epoch:  1   step:  74   train loss:  0.37270814180374146  val loss:  0.6814193725585938\n",
      "epoch:  1   step:  75   train loss:  0.3275037407875061  val loss:  0.6809923648834229\n",
      "epoch:  1   step:  76   train loss:  0.38906145095825195  val loss:  0.678596556186676\n",
      "epoch:  1   step:  77   train loss:  0.33308693766593933  val loss:  0.6812378168106079\n",
      "epoch:  1   step:  78   train loss:  0.2967112064361572  val loss:  0.6806049942970276\n",
      "epoch:  1   step:  79   train loss:  0.3634185194969177  val loss:  0.6685891151428223\n",
      "min_val_loss_print 0.6685891151428223\n",
      "epoch:  1   step:  80   train loss:  0.3633475601673126  val loss:  0.6721402406692505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1   step:  81   train loss:  0.24426452815532684  val loss:  0.6718798279762268\n",
      "epoch:  1   step:  82   train loss:  0.36761581897735596  val loss:  0.6694478988647461\n",
      "epoch:  1   step:  83   train loss:  0.39067500829696655  val loss:  0.6664273738861084\n",
      "min_val_loss_print 0.6664273738861084\n",
      "epoch:  1   step:  84   train loss:  0.314943790435791  val loss:  0.6642811894416809\n",
      "min_val_loss_print 0.6642811894416809\n",
      "epoch:  1   step:  85   train loss:  0.31009289622306824  val loss:  0.6653901934623718\n",
      "epoch:  1   step:  86   train loss:  0.40160268545150757  val loss:  0.6636375188827515\n",
      "min_val_loss_print 0.6636375188827515\n",
      "epoch:  1   step:  87   train loss:  0.3679225444793701  val loss:  0.6642228364944458\n",
      "epoch:  1   step:  88   train loss:  0.2772510349750519  val loss:  0.6673759818077087\n",
      "epoch:  1   step:  89   train loss:  0.2774984538555145  val loss:  0.6681885123252869\n",
      "epoch:  1   step:  90   train loss:  0.3071867525577545  val loss:  0.6706189513206482\n",
      "epoch:  1   step:  91   train loss:  0.3180176317691803  val loss:  0.6704407930374146\n",
      "epoch:  1   step:  92   train loss:  0.31687477231025696  val loss:  0.6639878153800964\n",
      "epoch:  1   step:  93   train loss:  0.33165326714515686  val loss:  0.6606622338294983\n",
      "min_val_loss_print 0.6606622338294983\n",
      "epoch:  1   step:  94   train loss:  0.31093984842300415  val loss:  0.6584882736206055\n",
      "min_val_loss_print 0.6584882736206055\n",
      "epoch:  1   step:  95   train loss:  0.30468764901161194  val loss:  0.6549502015113831\n",
      "min_val_loss_print 0.6549502015113831\n",
      "epoch:  1   step:  96   train loss:  0.31059524416923523  val loss:  0.653877854347229\n",
      "min_val_loss_print 0.653877854347229\n",
      "epoch:  1   step:  97   train loss:  0.3084641695022583  val loss:  0.6534977555274963\n",
      "min_val_loss_print 0.6534977555274963\n",
      "epoch:  1   step:  98   train loss:  0.314298540353775  val loss:  0.6572133302688599\n",
      "epoch:  1   step:  99   train loss:  0.3439287543296814  val loss:  0.6557989716529846\n",
      "epoch:  1   step:  100   train loss:  0.2974725365638733  val loss:  0.6504387259483337\n",
      "min_val_loss_print 0.6504387259483337\n",
      "epoch:  1   step:  101   train loss:  0.34154003858566284  val loss:  0.6505216956138611\n",
      "epoch:  1   step:  102   train loss:  0.2817072868347168  val loss:  0.6456503868103027\n",
      "min_val_loss_print 0.6456503868103027\n",
      "epoch:  1   step:  103   train loss:  0.3498828411102295  val loss:  0.6420755982398987\n",
      "min_val_loss_print 0.6420755982398987\n",
      "epoch:  1   step:  104   train loss:  0.26024869084358215  val loss:  0.6348519325256348\n",
      "min_val_loss_print 0.6348519325256348\n",
      "epoch:  1   step:  105   train loss:  0.3815622627735138  val loss:  0.6291934251785278\n",
      "min_val_loss_print 0.6291934251785278\n",
      "epoch:  1   step:  106   train loss:  0.3187403082847595  val loss:  0.632631242275238\n",
      "epoch:  1   step:  107   train loss:  0.30188336968421936  val loss:  0.6300577521324158\n",
      "epoch:  1   step:  108   train loss:  0.31060564517974854  val loss:  0.6328059434890747\n",
      "epoch:  1   step:  109   train loss:  0.36344775557518005  val loss:  0.6194164752960205\n",
      "min_val_loss_print 0.6194164752960205\n",
      "epoch:  1   step:  110   train loss:  0.3201156258583069  val loss:  0.6108972430229187\n",
      "min_val_loss_print 0.6108972430229187\n",
      "epoch:  1   step:  111   train loss:  0.29370132088661194  val loss:  0.6059402227401733\n",
      "min_val_loss_print 0.6059402227401733\n",
      "epoch:  1   step:  112   train loss:  0.2780863344669342  val loss:  0.6050539612770081\n",
      "min_val_loss_print 0.6050539612770081\n",
      "epoch:  1   step:  113   train loss:  0.2561003863811493  val loss:  0.6016649007797241\n",
      "min_val_loss_print 0.6016649007797241\n",
      "epoch:  1   step:  114   train loss:  0.31643760204315186  val loss:  0.6072177886962891\n",
      "epoch:  1   step:  115   train loss:  0.2796194851398468  val loss:  0.6021672487258911\n",
      "epoch:  1   step:  116   train loss:  0.37700197100639343  val loss:  0.5964277386665344\n",
      "min_val_loss_print 0.5964277386665344\n",
      "epoch:  1   step:  117   train loss:  0.2967688739299774  val loss:  0.58977872133255\n",
      "min_val_loss_print 0.58977872133255\n",
      "epoch:  1   step:  118   train loss:  0.29811835289001465  val loss:  0.588836669921875\n",
      "min_val_loss_print 0.588836669921875\n",
      "epoch:  1   step:  119   train loss:  0.32132381200790405  val loss:  0.5905011892318726\n",
      "epoch:  1   step:  120   train loss:  0.3241918087005615  val loss:  0.5962895154953003\n",
      "epoch:  1   step:  121   train loss:  0.21383515000343323  val loss:  0.594940721988678\n",
      "epoch:  1   step:  122   train loss:  0.32351213693618774  val loss:  0.5889703631401062\n",
      "epoch:  1   step:  123   train loss:  0.31231236457824707  val loss:  0.5866026282310486\n",
      "min_val_loss_print 0.5866026282310486\n",
      "epoch:  1   step:  124   train loss:  0.2956790328025818  val loss:  0.5838561058044434\n",
      "min_val_loss_print 0.5838561058044434\n",
      "epoch:  1   step:  125   train loss:  0.299943745136261  val loss:  0.5852946639060974\n",
      "epoch:  1   step:  126   train loss:  0.2767762541770935  val loss:  0.5849087238311768\n",
      "epoch:  1   step:  127   train loss:  0.3101557791233063  val loss:  0.5834559202194214\n",
      "min_val_loss_print 0.5834559202194214\n",
      "epoch:  1   step:  128   train loss:  0.2988322377204895  val loss:  0.585232675075531\n",
      "epoch:  1   step:  129   train loss:  0.33777111768722534  val loss:  0.5915992856025696\n",
      "epoch:  1   step:  130   train loss:  0.3327197730541229  val loss:  0.591705858707428\n",
      "epoch:  1   step:  131   train loss:  0.288058876991272  val loss:  0.5900560021400452\n",
      "epoch:  1   step:  132   train loss:  0.29696252942085266  val loss:  0.5928612947463989\n",
      "epoch:  1   step:  133   train loss:  0.3257014751434326  val loss:  0.5882815718650818\n",
      "epoch:  1   step:  134   train loss:  0.288576602935791  val loss:  0.5955483913421631\n",
      "epoch:  1   step:  135   train loss:  0.2673588693141937  val loss:  0.5963388085365295\n",
      "epoch:  1   step:  136   train loss:  0.21423809230327606  val loss:  0.598761260509491\n",
      "epoch:  1   step:  137   train loss:  0.2772335112094879  val loss:  0.5959582328796387\n",
      "epoch:  1   step:  138   train loss:  0.3482872247695923  val loss:  0.5879871249198914\n",
      "epoch:  1   step:  139   train loss:  0.25846627354621887  val loss:  0.585783839225769\n",
      "epoch:  1   step:  140   train loss:  0.31433799862861633  val loss:  0.5927717685699463\n",
      "epoch:  1   step:  141   train loss:  0.34428396821022034  val loss:  0.5906664729118347\n",
      "epoch:  1   step:  142   train loss:  0.3120986521244049  val loss:  0.5913561582565308\n",
      "epoch:  1   step:  143   train loss:  0.22415286302566528  val loss:  0.5951730608940125\n",
      "epoch:  1   step:  144   train loss:  0.30911844968795776  val loss:  0.5892114043235779\n",
      "epoch:  1   step:  145   train loss:  0.21244457364082336  val loss:  0.588320255279541\n",
      "epoch:  1   step:  146   train loss:  0.27686357498168945  val loss:  0.5843325853347778\n",
      "epoch:  1   step:  147   train loss:  0.24013470113277435  val loss:  0.5819299817085266\n",
      "min_val_loss_print 0.5819299817085266\n",
      "epoch:  1   step:  148   train loss:  0.2881593704223633  val loss:  0.5810152888298035\n",
      "min_val_loss_print 0.5810152888298035\n",
      "epoch:  1   step:  149   train loss:  0.2824726998806  val loss:  0.5879546403884888\n",
      "epoch:  1   step:  150   train loss:  0.2540816068649292  val loss:  0.5884755849838257\n",
      "epoch:  1   step:  151   train loss:  0.3832647502422333  val loss:  0.5843662023544312\n",
      "epoch:  1   step:  152   train loss:  0.3238840103149414  val loss:  0.5777040719985962\n",
      "min_val_loss_print 0.5777040719985962\n",
      "epoch:  1   step:  153   train loss:  0.28109297156333923  val loss:  0.5808902978897095\n",
      "epoch:  1   step:  154   train loss:  0.31094643473625183  val loss:  0.5791189074516296\n",
      "epoch:  1   step:  155   train loss:  0.2539404332637787  val loss:  0.5769129991531372\n",
      "min_val_loss_print 0.5769129991531372\n",
      "epoch:  1   step:  156   train loss:  0.29198402166366577  val loss:  0.5741931200027466\n",
      "min_val_loss_print 0.5741931200027466\n",
      "epoch:  1   step:  157   train loss:  0.29301831126213074  val loss:  0.568964958190918\n",
      "min_val_loss_print 0.568964958190918\n",
      "epoch:  1   step:  158   train loss:  0.30985915660858154  val loss:  0.5615981221199036\n",
      "min_val_loss_print 0.5615981221199036\n",
      "epoch:  1   step:  159   train loss:  0.2652041018009186  val loss:  0.5585586428642273\n",
      "min_val_loss_print 0.5585586428642273\n",
      "epoch:  1   step:  160   train loss:  0.24684026837348938  val loss:  0.560915470123291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1   step:  161   train loss:  0.34509778022766113  val loss:  0.5527682900428772\n",
      "min_val_loss_print 0.5527682900428772\n",
      "epoch:  1   step:  162   train loss:  0.2790462076663971  val loss:  0.546625018119812\n",
      "min_val_loss_print 0.546625018119812\n",
      "epoch:  1   step:  163   train loss:  0.2627657651901245  val loss:  0.5488250255584717\n",
      "epoch:  1   step:  164   train loss:  0.2987588346004486  val loss:  0.5456478595733643\n",
      "min_val_loss_print 0.5456478595733643\n",
      "epoch:  1   step:  165   train loss:  0.3050822615623474  val loss:  0.5443939566612244\n",
      "min_val_loss_print 0.5443939566612244\n",
      "epoch:  1   step:  166   train loss:  0.35300225019454956  val loss:  0.5409097075462341\n",
      "min_val_loss_print 0.5409097075462341\n",
      "epoch:  1   step:  167   train loss:  0.27520740032196045  val loss:  0.5409504175186157\n",
      "epoch:  1   step:  168   train loss:  0.34362924098968506  val loss:  0.5348260998725891\n",
      "min_val_loss_print 0.5348260998725891\n",
      "epoch:  1   step:  169   train loss:  0.2238435596227646  val loss:  0.5303229093551636\n",
      "min_val_loss_print 0.5303229093551636\n",
      "epoch:  1   step:  170   train loss:  0.2524442970752716  val loss:  0.5279340147972107\n",
      "min_val_loss_print 0.5279340147972107\n",
      "epoch:  1   step:  171   train loss:  0.2763431668281555  val loss:  0.5258702635765076\n",
      "min_val_loss_print 0.5258702635765076\n",
      "epoch:  1   step:  172   train loss:  0.33894363045692444  val loss:  0.5264017581939697\n",
      "epoch:  1   step:  173   train loss:  0.21732623875141144  val loss:  0.5271955728530884\n",
      "epoch:  1   step:  174   train loss:  0.3592524826526642  val loss:  0.5243604183197021\n",
      "min_val_loss_print 0.5243604183197021\n",
      "epoch:  1   step:  175   train loss:  0.3117917776107788  val loss:  0.518410861492157\n",
      "min_val_loss_print 0.518410861492157\n",
      "epoch:  1   step:  176   train loss:  0.24677205085754395  val loss:  0.5189403891563416\n",
      "epoch:  1   step:  177   train loss:  0.24896401166915894  val loss:  0.5182565450668335\n",
      "min_val_loss_print 0.5182565450668335\n",
      "epoch:  1   step:  178   train loss:  0.312428742647171  val loss:  0.5141354203224182\n",
      "min_val_loss_print 0.5141354203224182\n",
      "epoch:  1   step:  179   train loss:  0.3149533271789551  val loss:  0.5079683065414429\n",
      "min_val_loss_print 0.5079683065414429\n",
      "epoch:  1   step:  180   train loss:  0.27836698293685913  val loss:  0.5015427470207214\n",
      "min_val_loss_print 0.5015427470207214\n",
      "epoch:  1   step:  181   train loss:  0.2420063018798828  val loss:  0.5021373629570007\n",
      "epoch:  1   step:  182   train loss:  0.3055151402950287  val loss:  0.49948737025260925\n",
      "min_val_loss_print 0.49948737025260925\n",
      "epoch:  1   step:  183   train loss:  0.300444632768631  val loss:  0.4972837567329407\n",
      "min_val_loss_print 0.4972837567329407\n",
      "epoch:  1   step:  184   train loss:  0.2247195541858673  val loss:  0.4981172978878021\n",
      "epoch:  1   step:  185   train loss:  0.2313111275434494  val loss:  0.49673375487327576\n",
      "min_val_loss_print 0.49673375487327576\n",
      "epoch:  1   step:  186   train loss:  0.3130050301551819  val loss:  0.49560317397117615\n",
      "min_val_loss_print 0.49560317397117615\n",
      "epoch:  1   step:  187   train loss:  0.3233586549758911  val loss:  0.49218496680259705\n",
      "min_val_loss_print 0.49218496680259705\n",
      "epoch:  1   step:  188   train loss:  0.21232707798480988  val loss:  0.49263134598731995\n",
      "epoch:  1   step:  189   train loss:  0.28294917941093445  val loss:  0.4955410659313202\n",
      "epoch:  1   step:  190   train loss:  0.34624239802360535  val loss:  0.4902515113353729\n",
      "min_val_loss_print 0.4902515113353729\n",
      "epoch:  1   step:  191   train loss:  0.20837342739105225  val loss:  0.4935987889766693\n",
      "epoch:  1   step:  192   train loss:  0.2556609809398651  val loss:  0.49464696645736694\n",
      "epoch:  1   step:  193   train loss:  0.18911981582641602  val loss:  0.49177277088165283\n",
      "epoch:  1   step:  194   train loss:  0.21320493519306183  val loss:  0.49464115500450134\n",
      "epoch:  1   step:  195   train loss:  0.26013869047164917  val loss:  0.493975430727005\n",
      "epoch:  1   step:  196   train loss:  0.19891862571239471  val loss:  0.48967933654785156\n",
      "min_val_loss_print 0.48967933654785156\n",
      "epoch:  1   step:  197   train loss:  0.26452481746673584  val loss:  0.4879893362522125\n",
      "min_val_loss_print 0.4879893362522125\n",
      "epoch:  1   step:  198   train loss:  0.27522698044776917  val loss:  0.48341116309165955\n",
      "min_val_loss_print 0.48341116309165955\n",
      "epoch:  1   step:  199   train loss:  0.3364923298358917  val loss:  0.48366105556488037\n",
      "epoch:  1   step:  200   train loss:  0.24184338748455048  val loss:  0.4908111095428467\n",
      "epoch:  1   step:  201   train loss:  0.25841107964515686  val loss:  0.48551326990127563\n",
      "epoch:  1   step:  202   train loss:  0.40398091077804565  val loss:  0.4808291494846344\n",
      "min_val_loss_print 0.4808291494846344\n",
      "epoch:  1   step:  203   train loss:  0.2646172046661377  val loss:  0.4824836254119873\n",
      "epoch:  1   step:  204   train loss:  0.275303453207016  val loss:  0.4846893548965454\n",
      "epoch:  1   step:  205   train loss:  0.2763451933860779  val loss:  0.4831879138946533\n",
      "epoch:  1   step:  206   train loss:  0.31419089436531067  val loss:  0.4751946032047272\n",
      "min_val_loss_print 0.4751946032047272\n",
      "epoch:  1   step:  207   train loss:  0.23207445442676544  val loss:  0.47364601492881775\n",
      "min_val_loss_print 0.47364601492881775\n",
      "epoch:  1   step:  208   train loss:  0.25010374188423157  val loss:  0.4739270508289337\n",
      "epoch:  1   step:  209   train loss:  0.1836181879043579  val loss:  0.47549813985824585\n",
      "epoch:  1   step:  210   train loss:  0.3117266297340393  val loss:  0.47077441215515137\n",
      "min_val_loss_print 0.47077441215515137\n",
      "epoch:  1   step:  211   train loss:  0.3133091330528259  val loss:  0.46853503584861755\n",
      "min_val_loss_print 0.46853503584861755\n",
      "epoch:  1   step:  212   train loss:  0.2511518597602844  val loss:  0.4681413471698761\n",
      "min_val_loss_print 0.4681413471698761\n",
      "epoch:  1   step:  213   train loss:  0.23699963092803955  val loss:  0.4627614915370941\n",
      "min_val_loss_print 0.4627614915370941\n",
      "epoch:  1   step:  214   train loss:  0.23603269457817078  val loss:  0.4621086120605469\n",
      "min_val_loss_print 0.4621086120605469\n",
      "epoch:  1   step:  215   train loss:  0.3049250543117523  val loss:  0.4637978971004486\n",
      "epoch:  1   step:  216   train loss:  0.2990399897098541  val loss:  0.4602274000644684\n",
      "min_val_loss_print 0.4602274000644684\n",
      "epoch:  1   step:  217   train loss:  0.261056512594223  val loss:  0.45757925510406494\n",
      "min_val_loss_print 0.45757925510406494\n",
      "epoch:  1   step:  218   train loss:  0.2527783513069153  val loss:  0.4595170021057129\n",
      "epoch:  1   step:  219   train loss:  0.19220006465911865  val loss:  0.46046608686447144\n",
      "epoch:  1   step:  220   train loss:  0.21971815824508667  val loss:  0.4644336700439453\n",
      "epoch:  1   step:  221   train loss:  0.23087731003761292  val loss:  0.459896981716156\n",
      "epoch:  1   step:  222   train loss:  0.2754689157009125  val loss:  0.4621523320674896\n",
      "epoch:  1   step:  223   train loss:  0.26774975657463074  val loss:  0.46780064702033997\n",
      "epoch:  1   step:  224   train loss:  0.2616877555847168  val loss:  0.47405365109443665\n",
      "epoch:  1   step:  225   train loss:  0.3502618670463562  val loss:  0.47440433502197266\n",
      "epoch:  1   step:  226   train loss:  0.32359716296195984  val loss:  0.47444504499435425\n",
      "epoch:  1   step:  227   train loss:  0.21978187561035156  val loss:  0.4678962528705597\n",
      "epoch:  1   step:  228   train loss:  0.2652197480201721  val loss:  0.46179044246673584\n",
      "epoch:  1   step:  229   train loss:  0.2506519556045532  val loss:  0.4623071253299713\n",
      "epoch:  1   step:  230   train loss:  0.31097280979156494  val loss:  0.4584941267967224\n",
      "epoch:  1   step:  231   train loss:  0.27063047885894775  val loss:  0.4592530429363251\n",
      "epoch:  1   step:  232   train loss:  0.3367716670036316  val loss:  0.4584752917289734\n",
      "epoch:  1   step:  233   train loss:  0.29266729950904846  val loss:  0.45765045285224915\n",
      "epoch:  1   step:  234   train loss:  0.35647276043891907  val loss:  0.4555678367614746\n",
      "min_val_loss_print 0.4555678367614746\n",
      "epoch:  1   step:  235   train loss:  0.31956830620765686  val loss:  0.4456292986869812\n",
      "min_val_loss_print 0.4456292986869812\n",
      "epoch:  1   step:  236   train loss:  0.1869930773973465  val loss:  0.4466269314289093\n",
      "epoch:  1   step:  237   train loss:  0.28567516803741455  val loss:  0.4427453577518463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_val_loss_print 0.4427453577518463\n",
      "epoch:  1   step:  238   train loss:  0.24813628196716309  val loss:  0.44343212246894836\n",
      "epoch:  1   step:  239   train loss:  0.223945751786232  val loss:  0.4439530670642853\n",
      "epoch:  1   step:  240   train loss:  0.27666082978248596  val loss:  0.4416496157646179\n",
      "min_val_loss_print 0.4416496157646179\n",
      "epoch:  1   step:  241   train loss:  0.24304433166980743  val loss:  0.4431542754173279\n",
      "epoch:  1   step:  242   train loss:  0.3008538782596588  val loss:  0.4396483898162842\n",
      "min_val_loss_print 0.4396483898162842\n",
      "epoch:  1   step:  243   train loss:  0.2410823404788971  val loss:  0.4404579699039459\n",
      "epoch:  1   step:  244   train loss:  0.2144622802734375  val loss:  0.4455110728740692\n",
      "epoch:  1   step:  245   train loss:  0.21190834045410156  val loss:  0.4467002749443054\n",
      "epoch:  1   step:  246   train loss:  0.22984375059604645  val loss:  0.4495128393173218\n",
      "epoch:  1   step:  247   train loss:  0.2454458624124527  val loss:  0.44796568155288696\n",
      "epoch:  1   step:  248   train loss:  0.24470730125904083  val loss:  0.45126745104789734\n",
      "epoch:  1   step:  249   train loss:  0.18871837854385376  val loss:  0.45444920659065247\n",
      "epoch:  1   step:  250   train loss:  0.2025730311870575  val loss:  0.45097896456718445\n",
      "epoch:  1   step:  251   train loss:  0.3477970361709595  val loss:  0.44912096858024597\n",
      "epoch:  1   step:  252   train loss:  0.23432213068008423  val loss:  0.45190364122390747\n",
      "epoch:  1   step:  253   train loss:  0.2507743835449219  val loss:  0.45215436816215515\n",
      "epoch:  1   step:  254   train loss:  0.23404864966869354  val loss:  0.4557086229324341\n",
      "epoch:  1   step:  255   train loss:  0.26213231682777405  val loss:  0.4535200297832489\n",
      "epoch:  1   step:  256   train loss:  0.2526489496231079  val loss:  0.45280754566192627\n",
      "epoch:  1   step:  257   train loss:  0.26819881796836853  val loss:  0.4493069350719452\n",
      "epoch:  1   step:  258   train loss:  0.1830032467842102  val loss:  0.45062077045440674\n",
      "epoch:  1   step:  259   train loss:  0.22282774746418  val loss:  0.4511564373970032\n",
      "epoch:  1   step:  260   train loss:  0.24702277779579163  val loss:  0.45078960061073303\n",
      "epoch:  1   step:  261   train loss:  0.2165052592754364  val loss:  0.4546259641647339\n",
      "epoch:  1   step:  262   train loss:  0.23598776757717133  val loss:  0.45049989223480225\n",
      "epoch:  1   step:  263   train loss:  0.25995907187461853  val loss:  0.4533739984035492\n",
      "epoch:  1   step:  264   train loss:  0.17749767005443573  val loss:  0.44879844784736633\n",
      "epoch:  1   step:  265   train loss:  0.2918541431427002  val loss:  0.4467999339103699\n",
      "epoch:  1   step:  266   train loss:  0.21680334210395813  val loss:  0.44320645928382874\n",
      "epoch:  1   step:  267   train loss:  0.2783108949661255  val loss:  0.4439976215362549\n",
      "epoch:  1   step:  268   train loss:  0.24718020856380463  val loss:  0.44715821743011475\n",
      "epoch:  1   step:  269   train loss:  0.17526745796203613  val loss:  0.4459982216358185\n",
      "epoch:  1   step:  270   train loss:  0.23862707614898682  val loss:  0.4429028034210205\n",
      "epoch:  1   step:  271   train loss:  0.2219828963279724  val loss:  0.437166303396225\n",
      "min_val_loss_print 0.437166303396225\n",
      "epoch:  1   step:  272   train loss:  0.25236618518829346  val loss:  0.436944842338562\n",
      "min_val_loss_print 0.436944842338562\n",
      "epoch:  1   step:  273   train loss:  0.24241922795772552  val loss:  0.43457382917404175\n",
      "min_val_loss_print 0.43457382917404175\n",
      "epoch:  1   step:  274   train loss:  0.23928767442703247  val loss:  0.4329628348350525\n",
      "min_val_loss_print 0.4329628348350525\n",
      "epoch:  1   step:  275   train loss:  0.23456469178199768  val loss:  0.4328203797340393\n",
      "min_val_loss_print 0.4328203797340393\n",
      "epoch:  1   step:  276   train loss:  0.19065022468566895  val loss:  0.4262649416923523\n",
      "min_val_loss_print 0.4262649416923523\n",
      "epoch:  1   step:  277   train loss:  0.24704070389270782  val loss:  0.42753756046295166\n",
      "epoch:  1   step:  278   train loss:  0.24652732908725739  val loss:  0.4207149147987366\n",
      "min_val_loss_print 0.4207149147987366\n",
      "epoch:  1   step:  279   train loss:  0.1850644052028656  val loss:  0.42089948058128357\n",
      "epoch:  1   step:  280   train loss:  0.24169453978538513  val loss:  0.41206011176109314\n",
      "min_val_loss_print 0.41206011176109314\n",
      "epoch:  1   step:  281   train loss:  0.2700079083442688  val loss:  0.4151642322540283\n",
      "epoch:  1   step:  282   train loss:  0.26280513405799866  val loss:  0.4113881587982178\n",
      "min_val_loss_print 0.4113881587982178\n",
      "epoch:  1   step:  283   train loss:  0.25693729519844055  val loss:  0.407645583152771\n",
      "min_val_loss_print 0.407645583152771\n",
      "epoch:  1   step:  284   train loss:  0.29598209261894226  val loss:  0.40889811515808105\n",
      "epoch:  1   step:  285   train loss:  0.18946433067321777  val loss:  0.4090999662876129\n",
      "epoch:  1   step:  286   train loss:  0.3501582145690918  val loss:  0.4055958688259125\n",
      "min_val_loss_print 0.4055958688259125\n",
      "epoch:  1   step:  287   train loss:  0.1633947342634201  val loss:  0.40588244795799255\n",
      "epoch:  1   step:  288   train loss:  0.2307385504245758  val loss:  0.41223010420799255\n",
      "epoch:  1   step:  289   train loss:  0.227390855550766  val loss:  0.4109629690647125\n",
      "epoch:  1   step:  290   train loss:  0.24482139945030212  val loss:  0.4072233736515045\n",
      "epoch:  1   step:  291   train loss:  0.22218509018421173  val loss:  0.4083500802516937\n",
      "epoch:  1   step:  292   train loss:  0.2994665205478668  val loss:  0.40617913007736206\n",
      "epoch:  1   step:  293   train loss:  0.22901377081871033  val loss:  0.4069111943244934\n",
      "epoch:  1   step:  294   train loss:  0.22102601826190948  val loss:  0.4039366841316223\n",
      "min_val_loss_print 0.4039366841316223\n",
      "epoch:  1   step:  295   train loss:  0.24859200417995453  val loss:  0.3950440287590027\n",
      "min_val_loss_print 0.3950440287590027\n",
      "epoch:  1   step:  296   train loss:  0.27873972058296204  val loss:  0.3918386697769165\n",
      "min_val_loss_print 0.3918386697769165\n",
      "epoch:  1   step:  297   train loss:  0.16264879703521729  val loss:  0.39003509283065796\n",
      "min_val_loss_print 0.39003509283065796\n",
      "epoch:  1   step:  298   train loss:  0.2408309131860733  val loss:  0.3895823359489441\n",
      "min_val_loss_print 0.3895823359489441\n",
      "epoch:  1   step:  299   train loss:  0.2334364950656891  val loss:  0.3861778974533081\n",
      "min_val_loss_print 0.3861778974533081\n",
      "epoch:  1   step:  300   train loss:  0.1586032509803772  val loss:  0.3887059986591339\n",
      "epoch:  1   step:  301   train loss:  0.34985610842704773  val loss:  0.38334396481513977\n",
      "min_val_loss_print 0.38334396481513977\n",
      "epoch:  1   step:  302   train loss:  0.22424469888210297  val loss:  0.381863534450531\n",
      "min_val_loss_print 0.381863534450531\n",
      "epoch:  1   step:  303   train loss:  0.25779563188552856  val loss:  0.3774968087673187\n",
      "min_val_loss_print 0.3774968087673187\n",
      "epoch:  1   step:  304   train loss:  0.25944074988365173  val loss:  0.3715018033981323\n",
      "min_val_loss_print 0.3715018033981323\n",
      "epoch:  1   step:  305   train loss:  0.2515553832054138  val loss:  0.3691810369491577\n",
      "min_val_loss_print 0.3691810369491577\n",
      "epoch:  1   step:  306   train loss:  0.2727785110473633  val loss:  0.3749055862426758\n",
      "epoch:  1   step:  307   train loss:  0.21400363743305206  val loss:  0.3714849054813385\n",
      "epoch:  1   step:  308   train loss:  0.18044298887252808  val loss:  0.3716588616371155\n",
      "epoch:  1   step:  309   train loss:  0.3202919065952301  val loss:  0.3731641173362732\n",
      "epoch:  1   step:  310   train loss:  0.2373528629541397  val loss:  0.37488850951194763\n",
      "epoch:  1   step:  311   train loss:  0.2489340454339981  val loss:  0.367053747177124\n",
      "min_val_loss_print 0.367053747177124\n",
      "epoch:  1   step:  312   train loss:  0.2591668367385864  val loss:  0.37328270077705383\n",
      "epoch:  1   step:  313   train loss:  0.24920569360256195  val loss:  0.3627423942089081\n",
      "min_val_loss_print 0.3627423942089081\n",
      "epoch:  1   step:  314   train loss:  0.2752090096473694  val loss:  0.36138221621513367\n",
      "min_val_loss_print 0.36138221621513367\n",
      "epoch:  1   step:  315   train loss:  0.2256806641817093  val loss:  0.36760610342025757\n",
      "epoch:  1   step:  316   train loss:  0.19029174745082855  val loss:  0.36517319083213806\n",
      "epoch:  1   step:  317   train loss:  0.22309933602809906  val loss:  0.3640773594379425\n",
      "epoch:  1   step:  318   train loss:  0.22297930717468262  val loss:  0.36647334694862366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1   step:  319   train loss:  0.23266980051994324  val loss:  0.36394545435905457\n",
      "epoch:  1   step:  320   train loss:  0.21873657405376434  val loss:  0.3677822947502136\n",
      "epoch:  1   step:  321   train loss:  0.30476659536361694  val loss:  0.36550667881965637\n",
      "epoch:  1   step:  322   train loss:  0.1945284903049469  val loss:  0.36782726645469666\n",
      "epoch:  1   step:  323   train loss:  0.17387813329696655  val loss:  0.36921244859695435\n",
      "epoch:  1   step:  324   train loss:  0.25862306356430054  val loss:  0.36083608865737915\n",
      "min_val_loss_print 0.36083608865737915\n",
      "epoch:  1   step:  325   train loss:  0.22377263009548187  val loss:  0.3588825464248657\n",
      "min_val_loss_print 0.3588825464248657\n",
      "epoch:  1   step:  326   train loss:  0.20627738535404205  val loss:  0.3589653968811035\n",
      "epoch:  1   step:  327   train loss:  0.258652001619339  val loss:  0.3523827791213989\n",
      "min_val_loss_print 0.3523827791213989\n",
      "epoch:  1   step:  328   train loss:  0.2866222560405731  val loss:  0.3505447208881378\n",
      "min_val_loss_print 0.3505447208881378\n",
      "epoch:  1   step:  329   train loss:  0.16806133091449738  val loss:  0.3576835095882416\n",
      "epoch:  1   step:  330   train loss:  0.24977600574493408  val loss:  0.3568306267261505\n",
      "epoch:  1   step:  331   train loss:  0.16450731456279755  val loss:  0.35644614696502686\n",
      "epoch:  1   step:  332   train loss:  0.24850142002105713  val loss:  0.3570765554904938\n",
      "epoch:  1   step:  333   train loss:  0.18309390544891357  val loss:  0.35512423515319824\n",
      "epoch:  1   step:  334   train loss:  0.22061966359615326  val loss:  0.3515497148036957\n",
      "epoch:  1   step:  335   train loss:  0.24800072610378265  val loss:  0.3530859649181366\n",
      "epoch:  1   step:  336   train loss:  0.27567818760871887  val loss:  0.3572978675365448\n",
      "epoch:  1   step:  337   train loss:  0.18870167434215546  val loss:  0.3559815287590027\n",
      "epoch:  1   step:  338   train loss:  0.27120155096054077  val loss:  0.35775938630104065\n",
      "epoch:  1   step:  339   train loss:  0.31650248169898987  val loss:  0.35584235191345215\n",
      "epoch:  1   step:  340   train loss:  0.23010800778865814  val loss:  0.3457775413990021\n",
      "min_val_loss_print 0.3457775413990021\n",
      "epoch:  1   step:  341   train loss:  0.2031400352716446  val loss:  0.3411010503768921\n",
      "min_val_loss_print 0.3411010503768921\n",
      "epoch:  1   step:  342   train loss:  0.28711560368537903  val loss:  0.33948153257369995\n",
      "min_val_loss_print 0.33948153257369995\n",
      "epoch:  1   step:  343   train loss:  0.18569345772266388  val loss:  0.33089593052864075\n",
      "min_val_loss_print 0.33089593052864075\n",
      "epoch:  2   step:  0   train loss:  0.18696467578411102  val loss:  0.3280831277370453\n",
      "min_val_loss_print 0.3280831277370453\n",
      "epoch:  2   step:  1   train loss:  0.26140451431274414  val loss:  0.32053491473197937\n",
      "min_val_loss_print 0.32053491473197937\n",
      "epoch:  2   step:  2   train loss:  0.18173392117023468  val loss:  0.32187432050704956\n",
      "epoch:  2   step:  3   train loss:  0.19253264367580414  val loss:  0.3303867280483246\n",
      "epoch:  2   step:  4   train loss:  0.2324393391609192  val loss:  0.32653841376304626\n",
      "epoch:  2   step:  5   train loss:  0.14408105611801147  val loss:  0.32374808192253113\n",
      "epoch:  2   step:  6   train loss:  0.1853705793619156  val loss:  0.32422807812690735\n",
      "epoch:  2   step:  7   train loss:  0.20106171071529388  val loss:  0.32434922456741333\n",
      "epoch:  2   step:  8   train loss:  0.23517464101314545  val loss:  0.32511839270591736\n",
      "epoch:  2   step:  9   train loss:  0.1963203102350235  val loss:  0.3281979262828827\n",
      "epoch:  2   step:  10   train loss:  0.21802298724651337  val loss:  0.31926748156547546\n",
      "min_val_loss_print 0.31926748156547546\n",
      "epoch:  2   step:  11   train loss:  0.2594318389892578  val loss:  0.31997692584991455\n",
      "epoch:  2   step:  12   train loss:  0.2506456673145294  val loss:  0.3220926523208618\n",
      "epoch:  2   step:  13   train loss:  0.23358218371868134  val loss:  0.3181651830673218\n",
      "min_val_loss_print 0.3181651830673218\n",
      "epoch:  2   step:  14   train loss:  0.16480880975723267  val loss:  0.30996179580688477\n",
      "min_val_loss_print 0.30996179580688477\n",
      "epoch:  2   step:  15   train loss:  0.2064260095357895  val loss:  0.3048703670501709\n",
      "min_val_loss_print 0.3048703670501709\n",
      "epoch:  2   step:  16   train loss:  0.19820472598075867  val loss:  0.31406229734420776\n",
      "epoch:  2   step:  17   train loss:  0.20017056167125702  val loss:  0.3225374221801758\n",
      "epoch:  2   step:  18   train loss:  0.2800978124141693  val loss:  0.3226153254508972\n",
      "epoch:  2   step:  19   train loss:  0.18199561536312103  val loss:  0.3288930654525757\n",
      "epoch:  2   step:  20   train loss:  0.2176673710346222  val loss:  0.32771536707878113\n",
      "epoch:  2   step:  21   train loss:  0.25060680508613586  val loss:  0.3170809745788574\n",
      "epoch:  2   step:  22   train loss:  0.29630327224731445  val loss:  0.3214765787124634\n",
      "epoch:  2   step:  23   train loss:  0.14896456897258759  val loss:  0.3122396767139435\n",
      "epoch:  2   step:  24   train loss:  0.18489409983158112  val loss:  0.3136487901210785\n",
      "epoch:  2   step:  25   train loss:  0.17265142500400543  val loss:  0.3171291947364807\n",
      "epoch:  2   step:  26   train loss:  0.21471713483333588  val loss:  0.31517931818962097\n",
      "epoch:  2   step:  27   train loss:  0.22923487424850464  val loss:  0.3112265169620514\n",
      "epoch:  2   step:  28   train loss:  0.1900990903377533  val loss:  0.31099534034729004\n",
      "epoch:  2   step:  29   train loss:  0.15543022751808167  val loss:  0.3194848895072937\n",
      "epoch:  2   step:  30   train loss:  0.23703545331954956  val loss:  0.3173651099205017\n",
      "epoch:  2   step:  31   train loss:  0.29094693064689636  val loss:  0.315089613199234\n",
      "epoch:  2   step:  32   train loss:  0.22239263355731964  val loss:  0.3065795302391052\n",
      "epoch:  2   step:  33   train loss:  0.2392245978116989  val loss:  0.3110749125480652\n",
      "epoch:  2   step:  34   train loss:  0.2139442414045334  val loss:  0.30311572551727295\n",
      "min_val_loss_print 0.30311572551727295\n",
      "epoch:  2   step:  35   train loss:  0.21452312171459198  val loss:  0.30521509051322937\n",
      "epoch:  2   step:  36   train loss:  0.1755709946155548  val loss:  0.30616775155067444\n",
      "epoch:  2   step:  37   train loss:  0.2473844289779663  val loss:  0.3085939586162567\n",
      "epoch:  2   step:  38   train loss:  0.15418359637260437  val loss:  0.30505040287971497\n",
      "epoch:  2   step:  39   train loss:  0.23715592920780182  val loss:  0.3004744052886963\n",
      "min_val_loss_print 0.3004744052886963\n",
      "epoch:  2   step:  40   train loss:  0.21456308662891388  val loss:  0.3008096516132355\n",
      "epoch:  2   step:  41   train loss:  0.14562827348709106  val loss:  0.3036494851112366\n",
      "epoch:  2   step:  42   train loss:  0.20668917894363403  val loss:  0.30234870314598083\n",
      "epoch:  2   step:  43   train loss:  0.20807744562625885  val loss:  0.30272939801216125\n",
      "epoch:  2   step:  44   train loss:  0.17070454359054565  val loss:  0.3048292100429535\n",
      "epoch:  2   step:  45   train loss:  0.23494301736354828  val loss:  0.3040040135383606\n",
      "epoch:  2   step:  46   train loss:  0.17594896256923676  val loss:  0.3078310191631317\n",
      "epoch:  2   step:  47   train loss:  0.1823706179857254  val loss:  0.3105507791042328\n",
      "epoch:  2   step:  48   train loss:  0.21348123252391815  val loss:  0.3044739067554474\n",
      "epoch:  2   step:  49   train loss:  0.2077781856060028  val loss:  0.30463674664497375\n",
      "epoch:  2   step:  50   train loss:  0.2396952509880066  val loss:  0.30473652482032776\n",
      "epoch:  2   step:  51   train loss:  0.25431621074676514  val loss:  0.3046131730079651\n",
      "epoch:  2   step:  52   train loss:  0.15667691826820374  val loss:  0.3056904971599579\n",
      "epoch:  2   step:  53   train loss:  0.19939561188220978  val loss:  0.3047542870044708\n",
      "epoch:  2   step:  54   train loss:  0.2560846209526062  val loss:  0.30284151434898376\n",
      "epoch:  2   step:  55   train loss:  0.19928844273090363  val loss:  0.2979916036128998\n",
      "min_val_loss_print 0.2979916036128998\n",
      "epoch:  2   step:  56   train loss:  0.16052022576332092  val loss:  0.2976744472980499\n",
      "min_val_loss_print 0.2976744472980499\n",
      "epoch:  2   step:  57   train loss:  0.18572965264320374  val loss:  0.29436102509498596\n",
      "min_val_loss_print 0.29436102509498596\n",
      "epoch:  2   step:  58   train loss:  0.1860610544681549  val loss:  0.29773667454719543\n",
      "epoch:  2   step:  59   train loss:  0.20603440701961517  val loss:  0.29707813262939453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2   step:  60   train loss:  0.18140676617622375  val loss:  0.2925608158111572\n",
      "min_val_loss_print 0.2925608158111572\n",
      "epoch:  2   step:  61   train loss:  0.17351363599300385  val loss:  0.2904210388660431\n",
      "min_val_loss_print 0.2904210388660431\n",
      "epoch:  2   step:  62   train loss:  0.15864713490009308  val loss:  0.29088982939720154\n",
      "epoch:  2   step:  63   train loss:  0.14473435282707214  val loss:  0.2849572002887726\n",
      "min_val_loss_print 0.2849572002887726\n",
      "epoch:  2   step:  64   train loss:  0.21785682439804077  val loss:  0.2827550768852234\n",
      "min_val_loss_print 0.2827550768852234\n",
      "epoch:  2   step:  65   train loss:  0.2169451266527176  val loss:  0.2806222140789032\n",
      "min_val_loss_print 0.2806222140789032\n",
      "epoch:  2   step:  66   train loss:  0.17693310976028442  val loss:  0.2789582312107086\n",
      "min_val_loss_print 0.2789582312107086\n",
      "epoch:  2   step:  67   train loss:  0.21132956445217133  val loss:  0.27976804971694946\n",
      "epoch:  2   step:  68   train loss:  0.23196780681610107  val loss:  0.2777085602283478\n",
      "min_val_loss_print 0.2777085602283478\n",
      "epoch:  2   step:  69   train loss:  0.13624954223632812  val loss:  0.27450674772262573\n",
      "min_val_loss_print 0.27450674772262573\n",
      "epoch:  2   step:  70   train loss:  0.208520770072937  val loss:  0.27920398116111755\n",
      "epoch:  2   step:  71   train loss:  0.1448202282190323  val loss:  0.2799648940563202\n",
      "epoch:  2   step:  72   train loss:  0.18858684599399567  val loss:  0.2828562259674072\n",
      "epoch:  2   step:  73   train loss:  0.1688080132007599  val loss:  0.28193238377571106\n",
      "epoch:  2   step:  74   train loss:  0.240781769156456  val loss:  0.2826547920703888\n",
      "epoch:  2   step:  75   train loss:  0.19691215455532074  val loss:  0.2824595272541046\n",
      "epoch:  2   step:  76   train loss:  0.2661338746547699  val loss:  0.27996012568473816\n",
      "epoch:  2   step:  77   train loss:  0.1651446521282196  val loss:  0.28190726041793823\n",
      "epoch:  2   step:  78   train loss:  0.16953034698963165  val loss:  0.27936360239982605\n",
      "epoch:  2   step:  79   train loss:  0.16763444244861603  val loss:  0.2801007926464081\n",
      "epoch:  2   step:  80   train loss:  0.12001434713602066  val loss:  0.2836810052394867\n",
      "epoch:  2   step:  81   train loss:  0.22946114838123322  val loss:  0.28546279668807983\n",
      "epoch:  2   step:  82   train loss:  0.19298319518566132  val loss:  0.28673091530799866\n",
      "epoch:  2   step:  83   train loss:  0.16646964848041534  val loss:  0.284636527299881\n",
      "epoch:  2   step:  84   train loss:  0.18318875133991241  val loss:  0.28210189938545227\n",
      "epoch:  2   step:  85   train loss:  0.2009245753288269  val loss:  0.2819069027900696\n",
      "epoch:  2   step:  86   train loss:  0.23944920301437378  val loss:  0.28397300839424133\n",
      "epoch:  2   step:  87   train loss:  0.17715740203857422  val loss:  0.28079643845558167\n",
      "epoch:  2   step:  88   train loss:  0.23909924924373627  val loss:  0.27861663699150085\n",
      "epoch:  2   step:  89   train loss:  0.16370317339897156  val loss:  0.28322577476501465\n",
      "epoch:  2   step:  90   train loss:  0.22447609901428223  val loss:  0.28447017073631287\n",
      "epoch:  2   step:  91   train loss:  0.18124182522296906  val loss:  0.28063201904296875\n",
      "epoch:  2   step:  92   train loss:  0.1819697618484497  val loss:  0.27976763248443604\n",
      "epoch:  2   step:  93   train loss:  0.20139294862747192  val loss:  0.2847880721092224\n",
      "epoch:  2   step:  94   train loss:  0.1831119805574417  val loss:  0.28434646129608154\n",
      "epoch:  2   step:  95   train loss:  0.13414570689201355  val loss:  0.28431960940361023\n",
      "epoch:  2   step:  96   train loss:  0.18496546149253845  val loss:  0.28047865629196167\n",
      "epoch:  2   step:  97   train loss:  0.20246021449565887  val loss:  0.27838000655174255\n",
      "epoch:  2   step:  98   train loss:  0.16608014702796936  val loss:  0.2791781425476074\n",
      "epoch:  2   step:  99   train loss:  0.18034157156944275  val loss:  0.27396562695503235\n",
      "min_val_loss_print 0.27396562695503235\n",
      "epoch:  2   step:  100   train loss:  0.19287870824337006  val loss:  0.2688603699207306\n",
      "min_val_loss_print 0.2688603699207306\n",
      "epoch:  2   step:  101   train loss:  0.17818138003349304  val loss:  0.2664027512073517\n",
      "min_val_loss_print 0.2664027512073517\n",
      "epoch:  2   step:  102   train loss:  0.1466231793165207  val loss:  0.2679929733276367\n",
      "epoch:  2   step:  103   train loss:  0.18637776374816895  val loss:  0.2650498151779175\n",
      "min_val_loss_print 0.2650498151779175\n",
      "epoch:  2   step:  104   train loss:  0.15862302482128143  val loss:  0.26133236289024353\n",
      "min_val_loss_print 0.26133236289024353\n",
      "epoch:  2   step:  105   train loss:  0.21568149328231812  val loss:  0.2633657157421112\n",
      "epoch:  2   step:  106   train loss:  0.18469737470149994  val loss:  0.2631465494632721\n",
      "epoch:  2   step:  107   train loss:  0.23845423758029938  val loss:  0.2649461328983307\n",
      "epoch:  2   step:  108   train loss:  0.1813260316848755  val loss:  0.2688826620578766\n",
      "epoch:  2   step:  109   train loss:  0.16926845908164978  val loss:  0.2740362286567688\n",
      "epoch:  2   step:  110   train loss:  0.20342913269996643  val loss:  0.2722727060317993\n",
      "epoch:  2   step:  111   train loss:  0.23756584525108337  val loss:  0.26944002509117126\n",
      "epoch:  2   step:  112   train loss:  0.22905349731445312  val loss:  0.27207353711128235\n",
      "epoch:  2   step:  113   train loss:  0.19316869974136353  val loss:  0.267208069562912\n",
      "epoch:  2   step:  114   train loss:  0.19814838469028473  val loss:  0.26606816053390503\n",
      "epoch:  2   step:  115   train loss:  0.22869616746902466  val loss:  0.2691197395324707\n",
      "epoch:  2   step:  116   train loss:  0.21572846174240112  val loss:  0.26937299966812134\n",
      "epoch:  2   step:  117   train loss:  0.21058538556098938  val loss:  0.26758867502212524\n",
      "epoch:  2   step:  118   train loss:  0.14288301765918732  val loss:  0.26660552620887756\n",
      "epoch:  2   step:  119   train loss:  0.1928240805864334  val loss:  0.2665371298789978\n",
      "epoch:  2   step:  120   train loss:  0.21460385620594025  val loss:  0.2734927237033844\n",
      "epoch:  2   step:  121   train loss:  0.1443595290184021  val loss:  0.2750178575515747\n",
      "epoch:  2   step:  122   train loss:  0.2491064965724945  val loss:  0.2699384391307831\n",
      "epoch:  2   step:  123   train loss:  0.21586860716342926  val loss:  0.26869523525238037\n",
      "epoch:  2   step:  124   train loss:  0.15895234048366547  val loss:  0.26808297634124756\n",
      "epoch:  2   step:  125   train loss:  0.17193612456321716  val loss:  0.26971518993377686\n",
      "epoch:  2   step:  126   train loss:  0.19134142994880676  val loss:  0.26861289143562317\n",
      "epoch:  2   step:  127   train loss:  0.19282166659832  val loss:  0.26755228638648987\n",
      "epoch:  2   step:  128   train loss:  0.20103760063648224  val loss:  0.26752007007598877\n",
      "epoch:  2   step:  129   train loss:  0.18502716720104218  val loss:  0.26357126235961914\n",
      "epoch:  2   step:  130   train loss:  0.2030152529478073  val loss:  0.26440712809562683\n",
      "epoch:  2   step:  131   train loss:  0.2038472443819046  val loss:  0.26615941524505615\n",
      "epoch:  2   step:  132   train loss:  0.1559988558292389  val loss:  0.26993706822395325\n",
      "epoch:  2   step:  133   train loss:  0.21746157109737396  val loss:  0.27113738656044006\n",
      "epoch:  2   step:  134   train loss:  0.16075977683067322  val loss:  0.2677258551120758\n",
      "epoch:  2   step:  135   train loss:  0.14704424142837524  val loss:  0.27124887704849243\n",
      "epoch:  2   step:  136   train loss:  0.24967072904109955  val loss:  0.26897096633911133\n",
      "epoch:  2   step:  137   train loss:  0.15731529891490936  val loss:  0.2623888850212097\n",
      "epoch:  2   step:  138   train loss:  0.20441985130310059  val loss:  0.26382601261138916\n",
      "epoch:  2   step:  139   train loss:  0.17907942831516266  val loss:  0.2637469172477722\n",
      "epoch:  2   step:  140   train loss:  0.2668190002441406  val loss:  0.26095131039619446\n",
      "min_val_loss_print 0.26095131039619446\n",
      "epoch:  2   step:  141   train loss:  0.1594429910182953  val loss:  0.2608036994934082\n",
      "min_val_loss_print 0.2608036994934082\n",
      "epoch:  2   step:  142   train loss:  0.20914244651794434  val loss:  0.25981345772743225\n",
      "min_val_loss_print 0.25981345772743225\n",
      "epoch:  2   step:  143   train loss:  0.244362935423851  val loss:  0.2630980908870697\n",
      "epoch:  2   step:  144   train loss:  0.1888677328824997  val loss:  0.2538148760795593\n",
      "min_val_loss_print 0.2538148760795593\n",
      "epoch:  2   step:  145   train loss:  0.18970376253128052  val loss:  0.2560526132583618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2   step:  146   train loss:  0.17397655546665192  val loss:  0.25142014026641846\n",
      "min_val_loss_print 0.25142014026641846\n",
      "epoch:  2   step:  147   train loss:  0.24098807573318481  val loss:  0.2514055371284485\n",
      "min_val_loss_print 0.2514055371284485\n",
      "epoch:  2   step:  148   train loss:  0.15587404370307922  val loss:  0.24962018430233002\n",
      "min_val_loss_print 0.24962018430233002\n",
      "epoch:  2   step:  149   train loss:  0.2389056235551834  val loss:  0.2518952488899231\n",
      "epoch:  2   step:  150   train loss:  0.2296832650899887  val loss:  0.2509894073009491\n",
      "epoch:  2   step:  151   train loss:  0.2231539636850357  val loss:  0.2484651356935501\n",
      "min_val_loss_print 0.2484651356935501\n",
      "epoch:  2   step:  152   train loss:  0.23009346425533295  val loss:  0.24764296412467957\n",
      "min_val_loss_print 0.24764296412467957\n",
      "epoch:  2   step:  153   train loss:  0.14430424571037292  val loss:  0.24780461192131042\n",
      "epoch:  2   step:  154   train loss:  0.12939083576202393  val loss:  0.24959371984004974\n",
      "epoch:  2   step:  155   train loss:  0.20663462579250336  val loss:  0.24645595252513885\n",
      "min_val_loss_print 0.24645595252513885\n",
      "epoch:  2   step:  156   train loss:  0.1933465600013733  val loss:  0.2469421625137329\n",
      "epoch:  2   step:  157   train loss:  0.2133541852235794  val loss:  0.24446524679660797\n",
      "min_val_loss_print 0.24446524679660797\n",
      "epoch:  2   step:  158   train loss:  0.21900571882724762  val loss:  0.24477560818195343\n",
      "epoch:  2   step:  159   train loss:  0.19432172179222107  val loss:  0.2464812695980072\n",
      "epoch:  2   step:  160   train loss:  0.17819057404994965  val loss:  0.24419811367988586\n",
      "min_val_loss_print 0.24419811367988586\n",
      "epoch:  2   step:  161   train loss:  0.15173675119876862  val loss:  0.24220140278339386\n",
      "min_val_loss_print 0.24220140278339386\n",
      "epoch:  2   step:  162   train loss:  0.21753889322280884  val loss:  0.24381929636001587\n",
      "epoch:  2   step:  163   train loss:  0.15311163663864136  val loss:  0.24262693524360657\n",
      "epoch:  2   step:  164   train loss:  0.10413262993097305  val loss:  0.23995879292488098\n",
      "min_val_loss_print 0.23995879292488098\n",
      "epoch:  2   step:  165   train loss:  0.18228326737880707  val loss:  0.23936660587787628\n",
      "min_val_loss_print 0.23936660587787628\n",
      "epoch:  2   step:  166   train loss:  0.2049049735069275  val loss:  0.2382316142320633\n",
      "min_val_loss_print 0.2382316142320633\n",
      "epoch:  2   step:  167   train loss:  0.1567201167345047  val loss:  0.23901857435703278\n",
      "epoch:  2   step:  168   train loss:  0.14716291427612305  val loss:  0.23927050828933716\n",
      "epoch:  2   step:  169   train loss:  0.1525041162967682  val loss:  0.2421342134475708\n",
      "epoch:  2   step:  170   train loss:  0.22876720130443573  val loss:  0.24152420461177826\n",
      "epoch:  2   step:  171   train loss:  0.16662393510341644  val loss:  0.2400347888469696\n",
      "epoch:  2   step:  172   train loss:  0.1479732245206833  val loss:  0.23918627202510834\n",
      "epoch:  2   step:  173   train loss:  0.1892472356557846  val loss:  0.23484519124031067\n",
      "min_val_loss_print 0.23484519124031067\n",
      "epoch:  2   step:  174   train loss:  0.16973723471164703  val loss:  0.23407632112503052\n",
      "min_val_loss_print 0.23407632112503052\n",
      "epoch:  2   step:  175   train loss:  0.14583098888397217  val loss:  0.23510296642780304\n",
      "epoch:  2   step:  176   train loss:  0.1572786122560501  val loss:  0.23967309296131134\n",
      "epoch:  2   step:  177   train loss:  0.1473488062620163  val loss:  0.24143236875534058\n",
      "epoch:  2   step:  178   train loss:  0.14085304737091064  val loss:  0.24050292372703552\n",
      "epoch:  2   step:  179   train loss:  0.18856294453144073  val loss:  0.23984907567501068\n",
      "epoch:  2   step:  180   train loss:  0.20235081017017365  val loss:  0.2406865507364273\n",
      "epoch:  2   step:  181   train loss:  0.13524077832698822  val loss:  0.2388855665922165\n",
      "epoch:  2   step:  182   train loss:  0.16785268485546112  val loss:  0.23350048065185547\n",
      "min_val_loss_print 0.23350048065185547\n",
      "epoch:  2   step:  183   train loss:  0.15775154531002045  val loss:  0.23163717985153198\n",
      "min_val_loss_print 0.23163717985153198\n",
      "epoch:  2   step:  184   train loss:  0.14351966977119446  val loss:  0.2323945313692093\n",
      "epoch:  2   step:  185   train loss:  0.2037169486284256  val loss:  0.237881600856781\n",
      "epoch:  2   step:  186   train loss:  0.18537074327468872  val loss:  0.2338087558746338\n",
      "epoch:  2   step:  187   train loss:  0.11756923794746399  val loss:  0.23439587652683258\n",
      "epoch:  2   step:  188   train loss:  0.15544897317886353  val loss:  0.23606672883033752\n",
      "epoch:  2   step:  189   train loss:  0.18640711903572083  val loss:  0.23681679368019104\n",
      "epoch:  2   step:  190   train loss:  0.14634881913661957  val loss:  0.23192369937896729\n",
      "epoch:  2   step:  191   train loss:  0.1452140063047409  val loss:  0.2296575903892517\n",
      "min_val_loss_print 0.2296575903892517\n",
      "epoch:  2   step:  192   train loss:  0.1489841341972351  val loss:  0.22791185975074768\n",
      "min_val_loss_print 0.22791185975074768\n",
      "epoch:  2   step:  193   train loss:  0.2035679668188095  val loss:  0.23090989887714386\n",
      "epoch:  2   step:  194   train loss:  0.1599237471818924  val loss:  0.22810031473636627\n",
      "epoch:  2   step:  195   train loss:  0.10523301362991333  val loss:  0.23208898305892944\n",
      "epoch:  2   step:  196   train loss:  0.18471553921699524  val loss:  0.23299191892147064\n",
      "epoch:  2   step:  197   train loss:  0.20318932831287384  val loss:  0.2368764728307724\n",
      "epoch:  2   step:  198   train loss:  0.14816948771476746  val loss:  0.23655283451080322\n",
      "epoch:  2   step:  199   train loss:  0.13640901446342468  val loss:  0.231497123837471\n",
      "epoch:  2   step:  200   train loss:  0.11606147140264511  val loss:  0.23089273273944855\n",
      "epoch:  2   step:  201   train loss:  0.1699409931898117  val loss:  0.2310139387845993\n",
      "epoch:  2   step:  202   train loss:  0.21121078729629517  val loss:  0.23383527994155884\n",
      "epoch:  2   step:  203   train loss:  0.22308072447776794  val loss:  0.2311972975730896\n",
      "epoch:  2   step:  204   train loss:  0.25184282660484314  val loss:  0.23271308839321136\n",
      "epoch:  2   step:  205   train loss:  0.1746184080839157  val loss:  0.23411281406879425\n",
      "epoch:  2   step:  206   train loss:  0.15711495280265808  val loss:  0.23413769900798798\n",
      "epoch:  2   step:  207   train loss:  0.23107977211475372  val loss:  0.23291482031345367\n",
      "epoch:  2   step:  208   train loss:  0.15128009021282196  val loss:  0.2345598191022873\n",
      "epoch:  2   step:  209   train loss:  0.22738897800445557  val loss:  0.23433279991149902\n",
      "epoch:  2   step:  210   train loss:  0.19088979065418243  val loss:  0.23486125469207764\n",
      "epoch:  2   step:  211   train loss:  0.13365118205547333  val loss:  0.23450404405593872\n",
      "epoch:  2   step:  212   train loss:  0.17350776493549347  val loss:  0.23562078177928925\n",
      "epoch:  2   step:  213   train loss:  0.20524245500564575  val loss:  0.23615078628063202\n",
      "epoch:  2   step:  214   train loss:  0.17462940514087677  val loss:  0.2273366004228592\n",
      "min_val_loss_print 0.2273366004228592\n",
      "epoch:  2   step:  215   train loss:  0.17072135210037231  val loss:  0.22013534605503082\n",
      "min_val_loss_print 0.22013534605503082\n",
      "epoch:  2   step:  216   train loss:  0.17620764672756195  val loss:  0.22386984527111053\n",
      "epoch:  2   step:  217   train loss:  0.16691362857818604  val loss:  0.225983664393425\n",
      "epoch:  2   step:  218   train loss:  0.19643089175224304  val loss:  0.2254263162612915\n",
      "epoch:  2   step:  219   train loss:  0.10307323187589645  val loss:  0.2275577038526535\n",
      "epoch:  2   step:  220   train loss:  0.14876358211040497  val loss:  0.22903071343898773\n",
      "epoch:  2   step:  221   train loss:  0.1942632794380188  val loss:  0.22642041742801666\n",
      "epoch:  2   step:  222   train loss:  0.16913343966007233  val loss:  0.23128439486026764\n",
      "epoch:  2   step:  223   train loss:  0.1554950624704361  val loss:  0.23253701627254486\n",
      "epoch:  2   step:  224   train loss:  0.17913669347763062  val loss:  0.22979210317134857\n",
      "epoch:  2   step:  225   train loss:  0.15266695618629456  val loss:  0.23255661129951477\n",
      "epoch:  2   step:  226   train loss:  0.1770513355731964  val loss:  0.2222980558872223\n",
      "epoch:  2   step:  227   train loss:  0.15125951170921326  val loss:  0.22141455113887787\n",
      "epoch:  2   step:  228   train loss:  0.17412018775939941  val loss:  0.2222781479358673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2   step:  229   train loss:  0.1571241319179535  val loss:  0.21698202192783356\n",
      "min_val_loss_print 0.21698202192783356\n",
      "epoch:  2   step:  230   train loss:  0.1929471492767334  val loss:  0.2154708355665207\n",
      "min_val_loss_print 0.2154708355665207\n",
      "epoch:  2   step:  231   train loss:  0.16051077842712402  val loss:  0.21415667235851288\n",
      "min_val_loss_print 0.21415667235851288\n",
      "epoch:  2   step:  232   train loss:  0.18416579067707062  val loss:  0.2127494066953659\n",
      "min_val_loss_print 0.2127494066953659\n",
      "epoch:  2   step:  233   train loss:  0.1542349010705948  val loss:  0.21451573073863983\n",
      "epoch:  2   step:  234   train loss:  0.182146817445755  val loss:  0.21550269424915314\n",
      "epoch:  2   step:  235   train loss:  0.19794854521751404  val loss:  0.21506696939468384\n",
      "epoch:  2   step:  236   train loss:  0.19085946679115295  val loss:  0.2142224758863449\n",
      "epoch:  2   step:  237   train loss:  0.17662791907787323  val loss:  0.2130611091852188\n",
      "epoch:  2   step:  238   train loss:  0.12307281047105789  val loss:  0.2125360369682312\n",
      "min_val_loss_print 0.2125360369682312\n",
      "epoch:  2   step:  239   train loss:  0.15642663836479187  val loss:  0.21558049321174622\n",
      "epoch:  2   step:  240   train loss:  0.1805819422006607  val loss:  0.21596238017082214\n",
      "epoch:  2   step:  241   train loss:  0.17102743685245514  val loss:  0.2109597623348236\n",
      "min_val_loss_print 0.2109597623348236\n",
      "epoch:  2   step:  242   train loss:  0.168148472905159  val loss:  0.2110266089439392\n",
      "epoch:  2   step:  243   train loss:  0.12351368367671967  val loss:  0.20863589644432068\n",
      "min_val_loss_print 0.20863589644432068\n",
      "epoch:  2   step:  244   train loss:  0.20303383469581604  val loss:  0.20617789030075073\n",
      "min_val_loss_print 0.20617789030075073\n",
      "epoch:  2   step:  245   train loss:  0.12494917958974838  val loss:  0.2103312611579895\n",
      "epoch:  2   step:  246   train loss:  0.17647016048431396  val loss:  0.20885451138019562\n",
      "epoch:  2   step:  247   train loss:  0.19723407924175262  val loss:  0.20897923409938812\n",
      "epoch:  2   step:  248   train loss:  0.1958531141281128  val loss:  0.20523518323898315\n",
      "min_val_loss_print 0.20523518323898315\n",
      "epoch:  2   step:  249   train loss:  0.16456526517868042  val loss:  0.19912023842334747\n",
      "min_val_loss_print 0.19912023842334747\n",
      "epoch:  2   step:  250   train loss:  0.18856295943260193  val loss:  0.1931001842021942\n",
      "min_val_loss_print 0.1931001842021942\n",
      "epoch:  2   step:  251   train loss:  0.22297453880310059  val loss:  0.1891346275806427\n",
      "min_val_loss_print 0.1891346275806427\n",
      "epoch:  2   step:  252   train loss:  0.20196685194969177  val loss:  0.1887623369693756\n",
      "min_val_loss_print 0.1887623369693756\n",
      "epoch:  2   step:  253   train loss:  0.17776359617710114  val loss:  0.18479245901107788\n",
      "min_val_loss_print 0.18479245901107788\n",
      "epoch:  2   step:  254   train loss:  0.1504853367805481  val loss:  0.18376301229000092\n",
      "min_val_loss_print 0.18376301229000092\n",
      "epoch:  2   step:  255   train loss:  0.1826690286397934  val loss:  0.17910486459732056\n",
      "min_val_loss_print 0.17910486459732056\n",
      "epoch:  2   step:  256   train loss:  0.15263019502162933  val loss:  0.17737312614917755\n",
      "min_val_loss_print 0.17737312614917755\n",
      "epoch:  2   step:  257   train loss:  0.1401611566543579  val loss:  0.17905911803245544\n",
      "epoch:  2   step:  258   train loss:  0.10814855247735977  val loss:  0.18011383712291718\n",
      "epoch:  2   step:  259   train loss:  0.12861964106559753  val loss:  0.1809913069009781\n",
      "epoch:  2   step:  260   train loss:  0.11931636184453964  val loss:  0.18442119657993317\n",
      "epoch:  2   step:  261   train loss:  0.2422613650560379  val loss:  0.18486016988754272\n",
      "epoch:  2   step:  262   train loss:  0.20350629091262817  val loss:  0.18271002173423767\n",
      "epoch:  2   step:  263   train loss:  0.2094510942697525  val loss:  0.18190747499465942\n",
      "epoch:  2   step:  264   train loss:  0.14822013676166534  val loss:  0.18647098541259766\n",
      "epoch:  2   step:  265   train loss:  0.2222699373960495  val loss:  0.1867874711751938\n",
      "epoch:  2   step:  266   train loss:  0.19564729928970337  val loss:  0.18842490017414093\n",
      "epoch:  2   step:  267   train loss:  0.15574781596660614  val loss:  0.19030819833278656\n",
      "epoch:  2   step:  268   train loss:  0.1835079789161682  val loss:  0.19153641164302826\n",
      "epoch:  2   step:  269   train loss:  0.18015898764133453  val loss:  0.19088314473628998\n",
      "epoch:  2   step:  270   train loss:  0.17899955809116364  val loss:  0.19180208444595337\n",
      "epoch:  2   step:  271   train loss:  0.17796149849891663  val loss:  0.19233687222003937\n",
      "epoch:  2   step:  272   train loss:  0.17558465898036957  val loss:  0.19711001217365265\n",
      "epoch:  2   step:  273   train loss:  0.2113191783428192  val loss:  0.1937936693429947\n",
      "epoch:  2   step:  274   train loss:  0.12135078012943268  val loss:  0.19307252764701843\n",
      "epoch:  2   step:  275   train loss:  0.1402020901441574  val loss:  0.19632357358932495\n",
      "epoch:  2   step:  276   train loss:  0.16539393365383148  val loss:  0.19613073766231537\n",
      "epoch:  2   step:  277   train loss:  0.17720551788806915  val loss:  0.19568872451782227\n",
      "epoch:  2   step:  278   train loss:  0.1862831562757492  val loss:  0.1924450546503067\n",
      "epoch:  2   step:  279   train loss:  0.18787117302417755  val loss:  0.18944211304187775\n",
      "epoch:  2   step:  280   train loss:  0.14904628694057465  val loss:  0.18721193075180054\n",
      "epoch:  2   step:  281   train loss:  0.14595822989940643  val loss:  0.18960751593112946\n",
      "epoch:  2   step:  282   train loss:  0.1201513484120369  val loss:  0.1885276436805725\n",
      "epoch:  2   step:  283   train loss:  0.2680913805961609  val loss:  0.18812498450279236\n",
      "epoch:  2   step:  284   train loss:  0.1071426123380661  val loss:  0.19076530635356903\n",
      "epoch:  2   step:  285   train loss:  0.17243173718452454  val loss:  0.18736720085144043\n",
      "epoch:  2   step:  286   train loss:  0.207916259765625  val loss:  0.18815714120864868\n",
      "epoch:  2   step:  287   train loss:  0.13897553086280823  val loss:  0.1904933601617813\n",
      "epoch:  2   step:  288   train loss:  0.1467577964067459  val loss:  0.18573957681655884\n",
      "epoch:  2   step:  289   train loss:  0.1616286188364029  val loss:  0.18647243082523346\n",
      "epoch:  2   step:  290   train loss:  0.16219477355480194  val loss:  0.1905490756034851\n",
      "epoch:  2   step:  291   train loss:  0.1285819560289383  val loss:  0.19340433180332184\n",
      "epoch:  2   step:  292   train loss:  0.1839601844549179  val loss:  0.18852834403514862\n",
      "epoch:  2   step:  293   train loss:  0.18736276030540466  val loss:  0.19201689958572388\n",
      "epoch:  2   step:  294   train loss:  0.16380347311496735  val loss:  0.18880195915699005\n",
      "epoch:  2   step:  295   train loss:  0.12322717159986496  val loss:  0.18937402963638306\n",
      "epoch:  2   step:  296   train loss:  0.15100374817848206  val loss:  0.18939334154129028\n",
      "epoch:  2   step:  297   train loss:  0.1552087664604187  val loss:  0.18717461824417114\n",
      "epoch:  2   step:  298   train loss:  0.14782056212425232  val loss:  0.18107110261917114\n",
      "epoch:  2   step:  299   train loss:  0.19466115534305573  val loss:  0.17910818755626678\n",
      "epoch:  2   step:  300   train loss:  0.10209976881742477  val loss:  0.1764959841966629\n",
      "min_val_loss_print 0.1764959841966629\n",
      "epoch:  2   step:  301   train loss:  0.14294131100177765  val loss:  0.17633354663848877\n",
      "min_val_loss_print 0.17633354663848877\n",
      "epoch:  2   step:  302   train loss:  0.1425180286169052  val loss:  0.17584043741226196\n",
      "min_val_loss_print 0.17584043741226196\n",
      "epoch:  2   step:  303   train loss:  0.15102030336856842  val loss:  0.17541126906871796\n",
      "min_val_loss_print 0.17541126906871796\n",
      "epoch:  2   step:  304   train loss:  0.18787764012813568  val loss:  0.17537952959537506\n",
      "min_val_loss_print 0.17537952959537506\n",
      "epoch:  2   step:  305   train loss:  0.15178780257701874  val loss:  0.1749904900789261\n",
      "min_val_loss_print 0.1749904900789261\n",
      "epoch:  2   step:  306   train loss:  0.15240690112113953  val loss:  0.17542313039302826\n",
      "epoch:  2   step:  307   train loss:  0.13808724284172058  val loss:  0.17417113482952118\n",
      "min_val_loss_print 0.17417113482952118\n",
      "epoch:  2   step:  308   train loss:  0.16281720995903015  val loss:  0.17457224428653717\n",
      "epoch:  2   step:  309   train loss:  0.20323380827903748  val loss:  0.17476609349250793\n",
      "epoch:  2   step:  310   train loss:  0.12751427292823792  val loss:  0.17553769052028656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2   step:  311   train loss:  0.1232171282172203  val loss:  0.17862476408481598\n",
      "epoch:  2   step:  312   train loss:  0.10506322979927063  val loss:  0.17975962162017822\n",
      "epoch:  2   step:  313   train loss:  0.16729652881622314  val loss:  0.1800621896982193\n",
      "epoch:  2   step:  314   train loss:  0.18452392518520355  val loss:  0.17945334315299988\n",
      "epoch:  2   step:  315   train loss:  0.12407390773296356  val loss:  0.17959031462669373\n",
      "epoch:  2   step:  316   train loss:  0.12368420511484146  val loss:  0.17889881134033203\n",
      "epoch:  2   step:  317   train loss:  0.14522965252399445  val loss:  0.17967747151851654\n",
      "epoch:  2   step:  318   train loss:  0.1395193487405777  val loss:  0.17773054540157318\n",
      "epoch:  2   step:  319   train loss:  0.17794059216976166  val loss:  0.17578203976154327\n",
      "epoch:  2   step:  320   train loss:  0.13133233785629272  val loss:  0.1767529547214508\n",
      "epoch:  2   step:  321   train loss:  0.14545777440071106  val loss:  0.1775086522102356\n",
      "epoch:  2   step:  322   train loss:  0.1191064640879631  val loss:  0.1764606237411499\n",
      "epoch:  2   step:  323   train loss:  0.18762807548046112  val loss:  0.17443524301052094\n",
      "epoch:  2   step:  324   train loss:  0.16948707401752472  val loss:  0.17304165661334991\n",
      "min_val_loss_print 0.17304165661334991\n",
      "epoch:  2   step:  325   train loss:  0.1867823749780655  val loss:  0.17124469578266144\n",
      "min_val_loss_print 0.17124469578266144\n",
      "epoch:  2   step:  326   train loss:  0.18773917853832245  val loss:  0.17217305302619934\n",
      "epoch:  2   step:  327   train loss:  0.14579327404499054  val loss:  0.17108440399169922\n",
      "min_val_loss_print 0.17108440399169922\n",
      "epoch:  2   step:  328   train loss:  0.12014441192150116  val loss:  0.17292185127735138\n",
      "epoch:  2   step:  329   train loss:  0.12170297652482986  val loss:  0.17466799914836884\n",
      "epoch:  2   step:  330   train loss:  0.21717111766338348  val loss:  0.17520342767238617\n",
      "epoch:  2   step:  331   train loss:  0.1487945020198822  val loss:  0.1741056591272354\n",
      "epoch:  2   step:  332   train loss:  0.1451636403799057  val loss:  0.17090784013271332\n",
      "min_val_loss_print 0.17090784013271332\n",
      "epoch:  2   step:  333   train loss:  0.16439838707447052  val loss:  0.17276211082935333\n",
      "epoch:  2   step:  334   train loss:  0.17014765739440918  val loss:  0.17236165702342987\n",
      "epoch:  2   step:  335   train loss:  0.20839761197566986  val loss:  0.17263847589492798\n",
      "epoch:  2   step:  336   train loss:  0.164664626121521  val loss:  0.17037032544612885\n",
      "min_val_loss_print 0.17037032544612885\n",
      "epoch:  2   step:  337   train loss:  0.1353951394557953  val loss:  0.17273902893066406\n",
      "epoch:  2   step:  338   train loss:  0.11874664574861526  val loss:  0.17415732145309448\n",
      "epoch:  2   step:  339   train loss:  0.1457667499780655  val loss:  0.1750311255455017\n",
      "epoch:  2   step:  340   train loss:  0.13361261785030365  val loss:  0.1746879518032074\n",
      "epoch:  2   step:  341   train loss:  0.1672724485397339  val loss:  0.17225660383701324\n",
      "epoch:  2   step:  342   train loss:  0.1678040325641632  val loss:  0.17443358898162842\n",
      "epoch:  2   step:  343   train loss:  0.19917339086532593  val loss:  0.17106975615024567\n",
      "epoch:  3   step:  0   train loss:  0.1484849452972412  val loss:  0.17087575793266296\n",
      "epoch:  3   step:  1   train loss:  0.12044387310743332  val loss:  0.17040246725082397\n",
      "epoch:  3   step:  2   train loss:  0.1548512727022171  val loss:  0.1709970384836197\n",
      "epoch:  3   step:  3   train loss:  0.18279066681861877  val loss:  0.1687231808900833\n",
      "min_val_loss_print 0.1687231808900833\n",
      "epoch:  3   step:  4   train loss:  0.16462144255638123  val loss:  0.17210592329502106\n",
      "epoch:  3   step:  5   train loss:  0.15291738510131836  val loss:  0.1727830469608307\n",
      "epoch:  3   step:  6   train loss:  0.17848598957061768  val loss:  0.17144033312797546\n",
      "epoch:  3   step:  7   train loss:  0.17470180988311768  val loss:  0.17114710807800293\n",
      "epoch:  3   step:  8   train loss:  0.11037596315145493  val loss:  0.17080441117286682\n",
      "epoch:  3   step:  9   train loss:  0.14198634028434753  val loss:  0.16878555715084076\n",
      "epoch:  3   step:  10   train loss:  0.15981806814670563  val loss:  0.1640399545431137\n",
      "min_val_loss_print 0.1640399545431137\n",
      "epoch:  3   step:  11   train loss:  0.12963442504405975  val loss:  0.16399173438549042\n",
      "min_val_loss_print 0.16399173438549042\n",
      "epoch:  3   step:  12   train loss:  0.22986023128032684  val loss:  0.15955965220928192\n",
      "min_val_loss_print 0.15955965220928192\n",
      "epoch:  3   step:  13   train loss:  0.11591606587171555  val loss:  0.15819254517555237\n",
      "min_val_loss_print 0.15819254517555237\n",
      "epoch:  3   step:  14   train loss:  0.13979168236255646  val loss:  0.15586033463478088\n",
      "min_val_loss_print 0.15586033463478088\n",
      "epoch:  3   step:  15   train loss:  0.16389669477939606  val loss:  0.1584242582321167\n",
      "epoch:  3   step:  16   train loss:  0.1407790184020996  val loss:  0.15710611641407013\n",
      "epoch:  3   step:  17   train loss:  0.10880505293607712  val loss:  0.15936751663684845\n",
      "epoch:  3   step:  18   train loss:  0.22648786008358002  val loss:  0.15671853721141815\n",
      "epoch:  3   step:  19   train loss:  0.14443275332450867  val loss:  0.16039209067821503\n",
      "epoch:  3   step:  20   train loss:  0.18533965945243835  val loss:  0.15578477084636688\n",
      "min_val_loss_print 0.15578477084636688\n",
      "epoch:  3   step:  21   train loss:  0.14486871659755707  val loss:  0.15757465362548828\n",
      "epoch:  3   step:  22   train loss:  0.11350074410438538  val loss:  0.15865978598594666\n",
      "epoch:  3   step:  23   train loss:  0.12734733521938324  val loss:  0.15740880370140076\n",
      "epoch:  3   step:  24   train loss:  0.18365709483623505  val loss:  0.1582861691713333\n",
      "epoch:  3   step:  25   train loss:  0.12890906631946564  val loss:  0.16048851609230042\n",
      "epoch:  3   step:  26   train loss:  0.16264641284942627  val loss:  0.1590680032968521\n",
      "epoch:  3   step:  27   train loss:  0.21911002695560455  val loss:  0.1534968465566635\n",
      "min_val_loss_print 0.1534968465566635\n",
      "epoch:  3   step:  28   train loss:  0.22054168581962585  val loss:  0.15454410016536713\n",
      "epoch:  3   step:  29   train loss:  0.13454684615135193  val loss:  0.1522684097290039\n",
      "min_val_loss_print 0.1522684097290039\n",
      "epoch:  3   step:  30   train loss:  0.13252103328704834  val loss:  0.14951319992542267\n",
      "min_val_loss_print 0.14951319992542267\n",
      "epoch:  3   step:  31   train loss:  0.10660345107316971  val loss:  0.1481054574251175\n",
      "min_val_loss_print 0.1481054574251175\n",
      "epoch:  3   step:  32   train loss:  0.13083888590335846  val loss:  0.15052297711372375\n",
      "epoch:  3   step:  33   train loss:  0.15906013548374176  val loss:  0.15043912827968597\n",
      "epoch:  3   step:  34   train loss:  0.12114062905311584  val loss:  0.1526489555835724\n",
      "epoch:  3   step:  35   train loss:  0.14582332968711853  val loss:  0.15533947944641113\n",
      "epoch:  3   step:  36   train loss:  0.1383245587348938  val loss:  0.15220539271831512\n",
      "epoch:  3   step:  37   train loss:  0.13678275048732758  val loss:  0.1527433693408966\n",
      "epoch:  3   step:  38   train loss:  0.10290562361478806  val loss:  0.15532007813453674\n",
      "epoch:  3   step:  39   train loss:  0.11615092307329178  val loss:  0.1614963561296463\n",
      "epoch:  3   step:  40   train loss:  0.14041920006275177  val loss:  0.1620553731918335\n",
      "epoch:  3   step:  41   train loss:  0.10925016552209854  val loss:  0.16385646164417267\n",
      "epoch:  3   step:  42   train loss:  0.11334652453660965  val loss:  0.16375534236431122\n",
      "epoch:  3   step:  43   train loss:  0.14725112915039062  val loss:  0.16031000018119812\n",
      "epoch:  3   step:  44   train loss:  0.1402295082807541  val loss:  0.16209867596626282\n",
      "epoch:  3   step:  45   train loss:  0.10984393954277039  val loss:  0.16115005314350128\n",
      "epoch:  3   step:  46   train loss:  0.16448424756526947  val loss:  0.15690235793590546\n",
      "epoch:  3   step:  47   train loss:  0.1727924644947052  val loss:  0.155884250998497\n",
      "epoch:  3   step:  48   train loss:  0.10316877067089081  val loss:  0.1564515233039856\n",
      "epoch:  3   step:  49   train loss:  0.10891925543546677  val loss:  0.15963494777679443\n",
      "epoch:  3   step:  50   train loss:  0.18306414783000946  val loss:  0.15786747634410858\n",
      "epoch:  3   step:  51   train loss:  0.10947447270154953  val loss:  0.16057562828063965\n",
      "epoch:  3   step:  52   train loss:  0.16073670983314514  val loss:  0.15882620215415955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  3   step:  53   train loss:  0.1608315259218216  val loss:  0.15904442965984344\n",
      "epoch:  3   step:  54   train loss:  0.1297851800918579  val loss:  0.1587950587272644\n",
      "epoch:  3   step:  55   train loss:  0.11079767346382141  val loss:  0.155992791056633\n",
      "epoch:  3   step:  56   train loss:  0.12401534616947174  val loss:  0.15510155260562897\n",
      "epoch:  3   step:  57   train loss:  0.12750287353992462  val loss:  0.15488018095493317\n",
      "epoch:  3   step:  58   train loss:  0.14996568858623505  val loss:  0.1589602380990982\n",
      "epoch:  3   step:  59   train loss:  0.10170477628707886  val loss:  0.15971000492572784\n",
      "epoch:  3   step:  60   train loss:  0.16047383844852448  val loss:  0.15924590826034546\n",
      "epoch:  3   step:  61   train loss:  0.1768510788679123  val loss:  0.15796653926372528\n",
      "epoch:  3   step:  62   train loss:  0.1272347867488861  val loss:  0.1551271378993988\n",
      "epoch:  3   step:  63   train loss:  0.12263145297765732  val loss:  0.15273962914943695\n",
      "epoch:  3   step:  64   train loss:  0.13225339353084564  val loss:  0.15552450716495514\n",
      "epoch:  3   step:  65   train loss:  0.21371003985404968  val loss:  0.15817534923553467\n",
      "epoch:  3   step:  66   train loss:  0.12728814780712128  val loss:  0.16204895079135895\n",
      "epoch:  3   step:  67   train loss:  0.12085914611816406  val loss:  0.16321900486946106\n",
      "epoch:  3   step:  68   train loss:  0.10279098898172379  val loss:  0.15888701379299164\n",
      "epoch:  3   step:  69   train loss:  0.14993390440940857  val loss:  0.15873387455940247\n",
      "epoch:  3   step:  70   train loss:  0.21122975647449493  val loss:  0.15709848701953888\n",
      "epoch:  3   step:  71   train loss:  0.1859937161207199  val loss:  0.15664897859096527\n",
      "epoch:  3   step:  72   train loss:  0.12515708804130554  val loss:  0.1552026867866516\n",
      "epoch:  3   step:  73   train loss:  0.15087847411632538  val loss:  0.157232403755188\n",
      "epoch:  3   step:  74   train loss:  0.1524045318365097  val loss:  0.15676763653755188\n",
      "epoch:  3   step:  75   train loss:  0.165399968624115  val loss:  0.1551264524459839\n",
      "epoch:  3   step:  76   train loss:  0.1347299963235855  val loss:  0.15173618495464325\n",
      "epoch:  3   step:  77   train loss:  0.15583345293998718  val loss:  0.1522066742181778\n",
      "epoch:  3   step:  78   train loss:  0.14317886531352997  val loss:  0.1525963544845581\n",
      "epoch:  3   step:  79   train loss:  0.15677200257778168  val loss:  0.15306369960308075\n",
      "epoch:  3   step:  80   train loss:  0.14887632429599762  val loss:  0.15068508684635162\n",
      "epoch:  3   step:  81   train loss:  0.13875752687454224  val loss:  0.14968475699424744\n",
      "epoch:  3   step:  82   train loss:  0.14649537205696106  val loss:  0.14943432807922363\n",
      "epoch:  3   step:  83   train loss:  0.15708321332931519  val loss:  0.15015281736850739\n",
      "epoch:  3   step:  84   train loss:  0.11508285254240036  val loss:  0.15104994177818298\n",
      "epoch:  3   step:  85   train loss:  0.13257303833961487  val loss:  0.1509627103805542\n",
      "epoch:  3   step:  86   train loss:  0.17884498834609985  val loss:  0.15360908210277557\n",
      "epoch:  3   step:  87   train loss:  0.12465523928403854  val loss:  0.15588419139385223\n",
      "epoch:  3   step:  88   train loss:  0.13047929108142853  val loss:  0.15465721487998962\n",
      "epoch:  3   step:  89   train loss:  0.16506308317184448  val loss:  0.1532430201768875\n",
      "epoch:  3   step:  90   train loss:  0.1222456619143486  val loss:  0.15224479138851166\n",
      "epoch:  3   step:  91   train loss:  0.14402011036872864  val loss:  0.15234366059303284\n",
      "epoch:  3   step:  92   train loss:  0.11165314167737961  val loss:  0.1530427634716034\n",
      "epoch:  3   step:  93   train loss:  0.1352052539587021  val loss:  0.1529257446527481\n",
      "epoch:  3   step:  94   train loss:  0.1568487137556076  val loss:  0.15420906245708466\n",
      "epoch:  3   step:  95   train loss:  0.10313490033149719  val loss:  0.1544465720653534\n",
      "epoch:  3   step:  96   train loss:  0.11187004297971725  val loss:  0.15264716744422913\n",
      "epoch:  3   step:  97   train loss:  0.16168786585330963  val loss:  0.15184994041919708\n",
      "epoch:  3   step:  98   train loss:  0.11662927269935608  val loss:  0.1497591882944107\n",
      "epoch:  3   step:  99   train loss:  0.11753290146589279  val loss:  0.14829280972480774\n",
      "epoch:  3   step:  100   train loss:  0.16127055883407593  val loss:  0.14827291667461395\n",
      "epoch:  3   step:  101   train loss:  0.12665866315364838  val loss:  0.1492585986852646\n",
      "epoch:  3   step:  102   train loss:  0.10208779573440552  val loss:  0.14708371460437775\n",
      "min_val_loss_print 0.14708371460437775\n",
      "epoch:  3   step:  103   train loss:  0.13639205694198608  val loss:  0.14664581418037415\n",
      "min_val_loss_print 0.14664581418037415\n",
      "epoch:  3   step:  104   train loss:  0.109096460044384  val loss:  0.15030618011951447\n",
      "epoch:  3   step:  105   train loss:  0.16639061272144318  val loss:  0.1507108509540558\n",
      "epoch:  3   step:  106   train loss:  0.12051858752965927  val loss:  0.14814883470535278\n",
      "epoch:  3   step:  107   train loss:  0.08440132439136505  val loss:  0.1466725617647171\n",
      "epoch:  3   step:  108   train loss:  0.142237588763237  val loss:  0.14641055464744568\n",
      "min_val_loss_print 0.14641055464744568\n",
      "epoch:  3   step:  109   train loss:  0.21828922629356384  val loss:  0.14705006778240204\n",
      "epoch:  3   step:  110   train loss:  0.14025354385375977  val loss:  0.14786085486412048\n",
      "epoch:  3   step:  111   train loss:  0.16535018384456635  val loss:  0.15139178931713104\n",
      "epoch:  3   step:  112   train loss:  0.11171904951334  val loss:  0.15331801772117615\n",
      "epoch:  3   step:  113   train loss:  0.1584915667772293  val loss:  0.1542472094297409\n",
      "epoch:  3   step:  114   train loss:  0.12222534418106079  val loss:  0.15465769171714783\n",
      "epoch:  3   step:  115   train loss:  0.12916132807731628  val loss:  0.15475329756736755\n",
      "epoch:  3   step:  116   train loss:  0.17855128645896912  val loss:  0.15331345796585083\n",
      "epoch:  3   step:  117   train loss:  0.12356339395046234  val loss:  0.15498177707195282\n",
      "epoch:  3   step:  118   train loss:  0.12192197144031525  val loss:  0.1535986214876175\n",
      "epoch:  3   step:  119   train loss:  0.12520043551921844  val loss:  0.1547044813632965\n",
      "epoch:  3   step:  120   train loss:  0.1375342458486557  val loss:  0.15342429280281067\n",
      "epoch:  3   step:  121   train loss:  0.13503529131412506  val loss:  0.15267279744148254\n",
      "epoch:  3   step:  122   train loss:  0.16157859563827515  val loss:  0.1521528661251068\n",
      "epoch:  3   step:  123   train loss:  0.1572944074869156  val loss:  0.15097253024578094\n",
      "epoch:  3   step:  124   train loss:  0.1761905401945114  val loss:  0.1478031873703003\n",
      "epoch:  3   step:  125   train loss:  0.15881159901618958  val loss:  0.14853759109973907\n",
      "epoch:  3   step:  126   train loss:  0.1044236272573471  val loss:  0.14665496349334717\n",
      "epoch:  3   step:  127   train loss:  0.08642817288637161  val loss:  0.14428788423538208\n",
      "min_val_loss_print 0.14428788423538208\n",
      "epoch:  3   step:  128   train loss:  0.13232122361660004  val loss:  0.14121899008750916\n",
      "min_val_loss_print 0.14121899008750916\n",
      "epoch:  3   step:  129   train loss:  0.16752973198890686  val loss:  0.14136803150177002\n",
      "epoch:  3   step:  130   train loss:  0.1376805156469345  val loss:  0.1423638015985489\n",
      "epoch:  3   step:  131   train loss:  0.10123733431100845  val loss:  0.1439356505870819\n",
      "epoch:  3   step:  132   train loss:  0.17341241240501404  val loss:  0.14001739025115967\n",
      "min_val_loss_print 0.14001739025115967\n",
      "epoch:  3   step:  133   train loss:  0.11586116999387741  val loss:  0.13863147795200348\n",
      "min_val_loss_print 0.13863147795200348\n",
      "epoch:  3   step:  134   train loss:  0.1005321741104126  val loss:  0.1400376707315445\n",
      "epoch:  3   step:  135   train loss:  0.18662609159946442  val loss:  0.14227887988090515\n",
      "epoch:  3   step:  136   train loss:  0.13442085683345795  val loss:  0.14015239477157593\n",
      "epoch:  3   step:  137   train loss:  0.14952953159809113  val loss:  0.13989073038101196\n",
      "epoch:  3   step:  138   train loss:  0.14811865985393524  val loss:  0.14076200127601624\n",
      "epoch:  3   step:  139   train loss:  0.15680263936519623  val loss:  0.1409868746995926\n",
      "epoch:  3   step:  140   train loss:  0.12431389838457108  val loss:  0.140115424990654\n",
      "epoch:  3   step:  141   train loss:  0.10994129627943039  val loss:  0.13883145153522491\n",
      "epoch:  3   step:  142   train loss:  0.13412687182426453  val loss:  0.13843698799610138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_val_loss_print 0.13843698799610138\n",
      "epoch:  3   step:  143   train loss:  0.1639920175075531  val loss:  0.14082038402557373\n",
      "epoch:  3   step:  144   train loss:  0.13240188360214233  val loss:  0.13971932232379913\n",
      "epoch:  3   step:  145   train loss:  0.1127421110868454  val loss:  0.14137262105941772\n",
      "epoch:  3   step:  146   train loss:  0.15112397074699402  val loss:  0.14123637974262238\n",
      "epoch:  3   step:  147   train loss:  0.0928303524851799  val loss:  0.1401059776544571\n",
      "epoch:  3   step:  148   train loss:  0.12469564378261566  val loss:  0.13993781805038452\n",
      "epoch:  3   step:  149   train loss:  0.09727621078491211  val loss:  0.14102457463741302\n",
      "epoch:  3   step:  150   train loss:  0.14357168972492218  val loss:  0.14245927333831787\n",
      "epoch:  3   step:  151   train loss:  0.15765948593616486  val loss:  0.14291660487651825\n",
      "epoch:  3   step:  152   train loss:  0.13970932364463806  val loss:  0.14159756898880005\n",
      "epoch:  3   step:  153   train loss:  0.10671179741621017  val loss:  0.14211171865463257\n",
      "epoch:  3   step:  154   train loss:  0.1240231841802597  val loss:  0.14406508207321167\n",
      "epoch:  3   step:  155   train loss:  0.14735180139541626  val loss:  0.14419522881507874\n",
      "epoch:  3   step:  156   train loss:  0.08846848458051682  val loss:  0.14581461250782013\n",
      "epoch:  3   step:  157   train loss:  0.09350763261318207  val loss:  0.14332742989063263\n",
      "epoch:  3   step:  158   train loss:  0.14856061339378357  val loss:  0.14207206666469574\n",
      "epoch:  3   step:  159   train loss:  0.11687060445547104  val loss:  0.14148522913455963\n",
      "epoch:  3   step:  160   train loss:  0.11899726837873459  val loss:  0.13869819045066833\n",
      "epoch:  3   step:  161   train loss:  0.10710062831640244  val loss:  0.13645300269126892\n",
      "min_val_loss_print 0.13645300269126892\n",
      "epoch:  3   step:  162   train loss:  0.11798166483640671  val loss:  0.13540419936180115\n",
      "min_val_loss_print 0.13540419936180115\n",
      "epoch:  3   step:  163   train loss:  0.15756119787693024  val loss:  0.13401946425437927\n",
      "min_val_loss_print 0.13401946425437927\n",
      "epoch:  3   step:  164   train loss:  0.14488551020622253  val loss:  0.136766716837883\n",
      "epoch:  3   step:  165   train loss:  0.12890975177288055  val loss:  0.1345023810863495\n",
      "epoch:  3   step:  166   train loss:  0.10787784308195114  val loss:  0.13729627430438995\n",
      "epoch:  3   step:  167   train loss:  0.1294897198677063  val loss:  0.13422156870365143\n",
      "epoch:  3   step:  168   train loss:  0.1434464007616043  val loss:  0.13439087569713593\n",
      "epoch:  3   step:  169   train loss:  0.13898231089115143  val loss:  0.133152574300766\n",
      "min_val_loss_print 0.133152574300766\n",
      "epoch:  3   step:  170   train loss:  0.14785751700401306  val loss:  0.1327405571937561\n",
      "min_val_loss_print 0.1327405571937561\n",
      "epoch:  3   step:  171   train loss:  0.10575084388256073  val loss:  0.13277317583560944\n",
      "epoch:  3   step:  172   train loss:  0.12554603815078735  val loss:  0.13371844589710236\n",
      "epoch:  3   step:  173   train loss:  0.13728637993335724  val loss:  0.13447679579257965\n",
      "epoch:  3   step:  174   train loss:  0.11172836273908615  val loss:  0.13506115972995758\n",
      "epoch:  3   step:  175   train loss:  0.13534961640834808  val loss:  0.13667748868465424\n",
      "epoch:  3   step:  176   train loss:  0.11141641438007355  val loss:  0.1368975043296814\n",
      "epoch:  3   step:  177   train loss:  0.1146528497338295  val loss:  0.1373923122882843\n",
      "epoch:  3   step:  178   train loss:  0.12719717621803284  val loss:  0.13866925239562988\n",
      "epoch:  3   step:  179   train loss:  0.10305976122617722  val loss:  0.1400403380393982\n",
      "epoch:  3   step:  180   train loss:  0.1515354961156845  val loss:  0.14126873016357422\n",
      "epoch:  3   step:  181   train loss:  0.18002472817897797  val loss:  0.13986913859844208\n",
      "epoch:  3   step:  182   train loss:  0.10189003497362137  val loss:  0.1381373405456543\n",
      "epoch:  3   step:  183   train loss:  0.1145513728260994  val loss:  0.13737858831882477\n",
      "epoch:  3   step:  184   train loss:  0.12928785383701324  val loss:  0.1388554573059082\n",
      "epoch:  3   step:  185   train loss:  0.1223100870847702  val loss:  0.13659504055976868\n",
      "epoch:  3   step:  186   train loss:  0.1454140990972519  val loss:  0.13569772243499756\n",
      "epoch:  3   step:  187   train loss:  0.15245826542377472  val loss:  0.13389930129051208\n",
      "epoch:  3   step:  188   train loss:  0.12204389274120331  val loss:  0.1336396038532257\n",
      "epoch:  3   step:  189   train loss:  0.1253979355096817  val loss:  0.13376684486865997\n",
      "epoch:  3   step:  190   train loss:  0.1424400806427002  val loss:  0.133092999458313\n",
      "epoch:  3   step:  191   train loss:  0.1061130240559578  val loss:  0.133150115609169\n",
      "epoch:  3   step:  192   train loss:  0.10709232836961746  val loss:  0.13223481178283691\n",
      "min_val_loss_print 0.13223481178283691\n",
      "epoch:  3   step:  193   train loss:  0.12662899494171143  val loss:  0.13396596908569336\n",
      "epoch:  3   step:  194   train loss:  0.12262581288814545  val loss:  0.13305211067199707\n",
      "epoch:  3   step:  195   train loss:  0.1028970330953598  val loss:  0.13417492806911469\n",
      "epoch:  3   step:  196   train loss:  0.14294733107089996  val loss:  0.13348665833473206\n",
      "epoch:  3   step:  197   train loss:  0.1425434798002243  val loss:  0.1318831741809845\n",
      "min_val_loss_print 0.1318831741809845\n",
      "epoch:  3   step:  198   train loss:  0.126335009932518  val loss:  0.13186775147914886\n",
      "min_val_loss_print 0.13186775147914886\n",
      "epoch:  3   step:  199   train loss:  0.14702929556369781  val loss:  0.13292726874351501\n",
      "epoch:  3   step:  200   train loss:  0.11216960847377777  val loss:  0.1314203143119812\n",
      "min_val_loss_print 0.1314203143119812\n",
      "epoch:  3   step:  201   train loss:  0.16897614300251007  val loss:  0.1314232498407364\n",
      "epoch:  3   step:  202   train loss:  0.12989108264446259  val loss:  0.13039745390415192\n",
      "min_val_loss_print 0.13039745390415192\n",
      "epoch:  3   step:  203   train loss:  0.11111703515052795  val loss:  0.13193030655384064\n",
      "epoch:  3   step:  204   train loss:  0.16282841563224792  val loss:  0.1317104995250702\n",
      "epoch:  3   step:  205   train loss:  0.09752973914146423  val loss:  0.1299581676721573\n",
      "min_val_loss_print 0.1299581676721573\n",
      "epoch:  3   step:  206   train loss:  0.16458533704280853  val loss:  0.12843434512615204\n",
      "min_val_loss_print 0.12843434512615204\n",
      "epoch:  3   step:  207   train loss:  0.13025115430355072  val loss:  0.1291721761226654\n",
      "epoch:  3   step:  208   train loss:  0.1584583967924118  val loss:  0.12856721878051758\n",
      "epoch:  3   step:  209   train loss:  0.09670279175043106  val loss:  0.13021840155124664\n",
      "epoch:  3   step:  210   train loss:  0.13709792494773865  val loss:  0.12932182848453522\n",
      "epoch:  3   step:  211   train loss:  0.15014223754405975  val loss:  0.12853412330150604\n",
      "epoch:  3   step:  212   train loss:  0.17016661167144775  val loss:  0.13002657890319824\n",
      "epoch:  3   step:  213   train loss:  0.22531920671463013  val loss:  0.12881256639957428\n",
      "epoch:  3   step:  214   train loss:  0.10795959085226059  val loss:  0.1292080283164978\n",
      "epoch:  3   step:  215   train loss:  0.08796761184930801  val loss:  0.12990066409111023\n",
      "epoch:  3   step:  216   train loss:  0.10406535118818283  val loss:  0.13029146194458008\n",
      "epoch:  3   step:  217   train loss:  0.10778108239173889  val loss:  0.12728388607501984\n",
      "min_val_loss_print 0.12728388607501984\n",
      "epoch:  3   step:  218   train loss:  0.116459921002388  val loss:  0.1285039186477661\n",
      "epoch:  3   step:  219   train loss:  0.11299418658018112  val loss:  0.13099128007888794\n",
      "epoch:  3   step:  220   train loss:  0.15765738487243652  val loss:  0.13068903982639313\n",
      "epoch:  3   step:  221   train loss:  0.16725046932697296  val loss:  0.12759804725646973\n",
      "epoch:  3   step:  222   train loss:  0.17109590768814087  val loss:  0.12939900159835815\n",
      "epoch:  3   step:  223   train loss:  0.1329275220632553  val loss:  0.13025638461112976\n",
      "epoch:  3   step:  224   train loss:  0.1368771642446518  val loss:  0.12971711158752441\n",
      "epoch:  3   step:  225   train loss:  0.1384604275226593  val loss:  0.13010820746421814\n",
      "epoch:  3   step:  226   train loss:  0.09441222250461578  val loss:  0.1280691921710968\n",
      "epoch:  3   step:  227   train loss:  0.1132802814245224  val loss:  0.12263167649507523\n",
      "min_val_loss_print 0.12263167649507523\n",
      "epoch:  3   step:  228   train loss:  0.12684334814548492  val loss:  0.12290865927934647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  3   step:  229   train loss:  0.1485985368490219  val loss:  0.1237044557929039\n",
      "epoch:  3   step:  230   train loss:  0.0808866024017334  val loss:  0.12498218566179276\n",
      "epoch:  3   step:  231   train loss:  0.11460819095373154  val loss:  0.11973370611667633\n",
      "min_val_loss_print 0.11973370611667633\n",
      "epoch:  3   step:  232   train loss:  0.1460554450750351  val loss:  0.12127569317817688\n",
      "epoch:  3   step:  233   train loss:  0.15976321697235107  val loss:  0.12148803472518921\n",
      "epoch:  3   step:  234   train loss:  0.13936784863471985  val loss:  0.12183041870594025\n",
      "epoch:  3   step:  235   train loss:  0.07634348422288895  val loss:  0.1200355738401413\n",
      "epoch:  3   step:  236   train loss:  0.10481742769479752  val loss:  0.11799491196870804\n",
      "min_val_loss_print 0.11799491196870804\n",
      "epoch:  3   step:  237   train loss:  0.15119494497776031  val loss:  0.12066705524921417\n",
      "epoch:  3   step:  238   train loss:  0.0963897556066513  val loss:  0.12194078415632248\n",
      "epoch:  3   step:  239   train loss:  0.14029289782047272  val loss:  0.12067732959985733\n",
      "epoch:  3   step:  240   train loss:  0.16685126721858978  val loss:  0.12175971269607544\n",
      "epoch:  3   step:  241   train loss:  0.09259500354528427  val loss:  0.1228063553571701\n",
      "epoch:  3   step:  242   train loss:  0.15598690509796143  val loss:  0.12216930836439133\n",
      "epoch:  3   step:  243   train loss:  0.10138382762670517  val loss:  0.12346108257770538\n",
      "epoch:  3   step:  244   train loss:  0.11120171844959259  val loss:  0.12417247146368027\n",
      "epoch:  3   step:  245   train loss:  0.13271421194076538  val loss:  0.12317253649234772\n",
      "epoch:  3   step:  246   train loss:  0.11607973277568817  val loss:  0.1226651668548584\n",
      "epoch:  3   step:  247   train loss:  0.12915165722370148  val loss:  0.12245713919401169\n",
      "epoch:  3   step:  248   train loss:  0.1367795318365097  val loss:  0.1212160736322403\n",
      "epoch:  3   step:  249   train loss:  0.09143482893705368  val loss:  0.12186844646930695\n",
      "epoch:  3   step:  250   train loss:  0.08533298969268799  val loss:  0.12235002964735031\n",
      "epoch:  3   step:  251   train loss:  0.12875118851661682  val loss:  0.1241864413022995\n",
      "epoch:  3   step:  252   train loss:  0.15760664641857147  val loss:  0.12197650969028473\n",
      "epoch:  3   step:  253   train loss:  0.09088685363531113  val loss:  0.12143275886774063\n",
      "epoch:  3   step:  254   train loss:  0.09785272926092148  val loss:  0.12108930945396423\n",
      "epoch:  3   step:  255   train loss:  0.11034391075372696  val loss:  0.11938972771167755\n",
      "epoch:  3   step:  256   train loss:  0.09531428664922714  val loss:  0.12047924846410751\n",
      "epoch:  3   step:  257   train loss:  0.12359428405761719  val loss:  0.12058363109827042\n",
      "epoch:  3   step:  258   train loss:  0.17266754806041718  val loss:  0.11882679164409637\n",
      "epoch:  3   step:  259   train loss:  0.14118333160877228  val loss:  0.11812557280063629\n",
      "epoch:  3   step:  260   train loss:  0.15595151484012604  val loss:  0.11879992485046387\n",
      "epoch:  3   step:  261   train loss:  0.10323188453912735  val loss:  0.11954616010189056\n",
      "epoch:  3   step:  262   train loss:  0.09990204125642776  val loss:  0.12062162160873413\n",
      "epoch:  3   step:  263   train loss:  0.17441639304161072  val loss:  0.12192706018686295\n",
      "epoch:  3   step:  264   train loss:  0.08787930756807327  val loss:  0.12265893071889877\n",
      "epoch:  3   step:  265   train loss:  0.11077157407999039  val loss:  0.12154491245746613\n",
      "epoch:  3   step:  266   train loss:  0.08270386606454849  val loss:  0.12138292193412781\n",
      "epoch:  3   step:  267   train loss:  0.08557695895433426  val loss:  0.12069934606552124\n",
      "epoch:  3   step:  268   train loss:  0.09653716534376144  val loss:  0.12078167498111725\n",
      "epoch:  3   step:  269   train loss:  0.14196892082691193  val loss:  0.12130215018987656\n",
      "epoch:  3   step:  270   train loss:  0.08678033947944641  val loss:  0.12251293659210205\n",
      "epoch:  3   step:  271   train loss:  0.08976832777261734  val loss:  0.12343212217092514\n",
      "epoch:  3   step:  272   train loss:  0.09741173684597015  val loss:  0.12351112067699432\n",
      "epoch:  3   step:  273   train loss:  0.1334003359079361  val loss:  0.12245927006006241\n",
      "epoch:  3   step:  274   train loss:  0.06630130112171173  val loss:  0.12445002794265747\n",
      "epoch:  3   step:  275   train loss:  0.12257606536149979  val loss:  0.12371256202459335\n",
      "epoch:  3   step:  276   train loss:  0.13758830726146698  val loss:  0.12292644381523132\n",
      "epoch:  3   step:  277   train loss:  0.11636966466903687  val loss:  0.12399277091026306\n",
      "epoch:  3   step:  278   train loss:  0.10320603847503662  val loss:  0.12475880235433578\n",
      "epoch:  3   step:  279   train loss:  0.10826171934604645  val loss:  0.12492793798446655\n",
      "epoch:  3   step:  280   train loss:  0.11580445617437363  val loss:  0.12488546222448349\n",
      "epoch:  3   step:  281   train loss:  0.07922519743442535  val loss:  0.12383873015642166\n",
      "epoch:  3   step:  282   train loss:  0.14537860453128815  val loss:  0.12256425619125366\n",
      "epoch:  3   step:  283   train loss:  0.12747883796691895  val loss:  0.12080900371074677\n",
      "epoch:  3   step:  284   train loss:  0.09550631791353226  val loss:  0.12020748853683472\n",
      "epoch:  3   step:  285   train loss:  0.10545341670513153  val loss:  0.11946222931146622\n",
      "epoch:  3   step:  286   train loss:  0.11923713237047195  val loss:  0.11975997686386108\n",
      "epoch:  3   step:  287   train loss:  0.1291605681180954  val loss:  0.11900482326745987\n",
      "epoch:  3   step:  288   train loss:  0.13057267665863037  val loss:  0.11951185762882233\n",
      "epoch:  3   step:  289   train loss:  0.12259716540575027  val loss:  0.11886657029390335\n",
      "epoch:  3   step:  290   train loss:  0.14514295756816864  val loss:  0.11649387329816818\n",
      "min_val_loss_print 0.11649387329816818\n",
      "epoch:  3   step:  291   train loss:  0.09425101429224014  val loss:  0.11739423125982285\n",
      "epoch:  3   step:  292   train loss:  0.12463106960058212  val loss:  0.11653302609920502\n",
      "epoch:  3   step:  293   train loss:  0.12873706221580505  val loss:  0.11737871915102005\n",
      "epoch:  3   step:  294   train loss:  0.10586084425449371  val loss:  0.11912181228399277\n",
      "epoch:  3   step:  295   train loss:  0.10231759399175644  val loss:  0.11828933656215668\n",
      "epoch:  3   step:  296   train loss:  0.10351260006427765  val loss:  0.11611638218164444\n",
      "min_val_loss_print 0.11611638218164444\n",
      "epoch:  3   step:  297   train loss:  0.1298593431711197  val loss:  0.11734585464000702\n",
      "epoch:  3   step:  298   train loss:  0.17667824029922485  val loss:  0.11345920711755753\n",
      "min_val_loss_print 0.11345920711755753\n",
      "epoch:  3   step:  299   train loss:  0.09417369216680527  val loss:  0.11413493007421494\n",
      "epoch:  3   step:  300   train loss:  0.139596089720726  val loss:  0.11273395270109177\n",
      "min_val_loss_print 0.11273395270109177\n",
      "epoch:  3   step:  301   train loss:  0.16141590476036072  val loss:  0.11482115834951401\n",
      "epoch:  3   step:  302   train loss:  0.11406023055315018  val loss:  0.11593881249427795\n",
      "epoch:  3   step:  303   train loss:  0.10337580740451813  val loss:  0.11602065712213516\n",
      "epoch:  3   step:  304   train loss:  0.16938427090644836  val loss:  0.11598637700080872\n",
      "epoch:  3   step:  305   train loss:  0.09824234247207642  val loss:  0.11558195948600769\n",
      "epoch:  3   step:  306   train loss:  0.15410597622394562  val loss:  0.11531304568052292\n",
      "epoch:  3   step:  307   train loss:  0.11085833609104156  val loss:  0.11436982452869415\n",
      "epoch:  3   step:  308   train loss:  0.10628234595060349  val loss:  0.1158333569765091\n",
      "epoch:  3   step:  309   train loss:  0.09384137392044067  val loss:  0.11849651485681534\n",
      "epoch:  3   step:  310   train loss:  0.11062388122081757  val loss:  0.11728854477405548\n",
      "epoch:  3   step:  311   train loss:  0.13215313851833344  val loss:  0.11460913717746735\n",
      "epoch:  3   step:  312   train loss:  0.12452729046344757  val loss:  0.1156403198838234\n",
      "epoch:  3   step:  313   train loss:  0.09548003226518631  val loss:  0.11689209938049316\n",
      "epoch:  3   step:  314   train loss:  0.13403907418251038  val loss:  0.1174313947558403\n",
      "epoch:  3   step:  315   train loss:  0.11047603189945221  val loss:  0.11750892549753189\n",
      "epoch:  3   step:  316   train loss:  0.09095054119825363  val loss:  0.11699502170085907\n",
      "epoch:  3   step:  317   train loss:  0.09986904263496399  val loss:  0.11816241592168808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  3   step:  318   train loss:  0.15671434998512268  val loss:  0.11906161159276962\n",
      "epoch:  3   step:  319   train loss:  0.15328165888786316  val loss:  0.11804685741662979\n",
      "epoch:  3   step:  320   train loss:  0.09669668972492218  val loss:  0.11697801202535629\n",
      "epoch:  3   step:  321   train loss:  0.12365417927503586  val loss:  0.11604584753513336\n",
      "epoch:  3   step:  322   train loss:  0.10947273671627045  val loss:  0.11492559313774109\n",
      "epoch:  3   step:  323   train loss:  0.14636316895484924  val loss:  0.11476223170757294\n",
      "epoch:  3   step:  324   train loss:  0.1034451425075531  val loss:  0.11446113139390945\n",
      "epoch:  3   step:  325   train loss:  0.11401624977588654  val loss:  0.11479175090789795\n",
      "epoch:  3   step:  326   train loss:  0.14565029740333557  val loss:  0.11397901177406311\n",
      "epoch:  3   step:  327   train loss:  0.07952578365802765  val loss:  0.11475155502557755\n",
      "epoch:  3   step:  328   train loss:  0.12291040271520615  val loss:  0.11494109779596329\n",
      "epoch:  3   step:  329   train loss:  0.1728649139404297  val loss:  0.11473021656274796\n",
      "epoch:  3   step:  330   train loss:  0.11303714662790298  val loss:  0.11424277722835541\n",
      "epoch:  3   step:  331   train loss:  0.13959719240665436  val loss:  0.11348669230937958\n",
      "epoch:  3   step:  332   train loss:  0.16616347432136536  val loss:  0.1136007010936737\n",
      "epoch:  3   step:  333   train loss:  0.08366761356592178  val loss:  0.1138397827744484\n",
      "epoch:  3   step:  334   train loss:  0.1684037744998932  val loss:  0.11442799121141434\n",
      "epoch:  3   step:  335   train loss:  0.10511714965105057  val loss:  0.11375430971384048\n",
      "epoch:  3   step:  336   train loss:  0.1590689718723297  val loss:  0.1126452311873436\n",
      "min_val_loss_print 0.1126452311873436\n",
      "epoch:  3   step:  337   train loss:  0.092829130589962  val loss:  0.11159524321556091\n",
      "min_val_loss_print 0.11159524321556091\n",
      "epoch:  3   step:  338   train loss:  0.06484370678663254  val loss:  0.11080458760261536\n",
      "min_val_loss_print 0.11080458760261536\n",
      "epoch:  3   step:  339   train loss:  0.13289684057235718  val loss:  0.11039569973945618\n",
      "min_val_loss_print 0.11039569973945618\n",
      "epoch:  3   step:  340   train loss:  0.13635802268981934  val loss:  0.10887411236763\n",
      "min_val_loss_print 0.10887411236763\n",
      "epoch:  3   step:  341   train loss:  0.10612048208713531  val loss:  0.10980573296546936\n",
      "epoch:  3   step:  342   train loss:  0.08716367185115814  val loss:  0.10882328450679779\n",
      "min_val_loss_print 0.10882328450679779\n",
      "epoch:  3   step:  343   train loss:  0.1691323220729828  val loss:  0.10820182412862778\n",
      "min_val_loss_print 0.10820182412862778\n",
      "epoch:  4   step:  0   train loss:  0.12585027515888214  val loss:  0.10788799822330475\n",
      "min_val_loss_print 0.10788799822330475\n",
      "epoch:  4   step:  1   train loss:  0.12003619223833084  val loss:  0.10785338282585144\n",
      "min_val_loss_print 0.10785338282585144\n",
      "epoch:  4   step:  2   train loss:  0.11652471125125885  val loss:  0.10770604014396667\n",
      "min_val_loss_print 0.10770604014396667\n",
      "epoch:  4   step:  3   train loss:  0.11501587927341461  val loss:  0.10978355258703232\n",
      "epoch:  4   step:  4   train loss:  0.09414931386709213  val loss:  0.11028469353914261\n",
      "epoch:  4   step:  5   train loss:  0.10386583209037781  val loss:  0.10971719026565552\n",
      "epoch:  4   step:  6   train loss:  0.08879181742668152  val loss:  0.10811072587966919\n",
      "epoch:  4   step:  7   train loss:  0.12504902482032776  val loss:  0.10881701856851578\n",
      "epoch:  4   step:  8   train loss:  0.11915642023086548  val loss:  0.1082247644662857\n",
      "epoch:  4   step:  9   train loss:  0.10279925912618637  val loss:  0.10848549008369446\n",
      "epoch:  4   step:  10   train loss:  0.11444970220327377  val loss:  0.10658776015043259\n",
      "min_val_loss_print 0.10658776015043259\n",
      "epoch:  4   step:  11   train loss:  0.10366956889629364  val loss:  0.10686509311199188\n",
      "epoch:  4   step:  12   train loss:  0.11229971051216125  val loss:  0.10502022504806519\n",
      "min_val_loss_print 0.10502022504806519\n",
      "epoch:  4   step:  13   train loss:  0.13577835261821747  val loss:  0.10278142988681793\n",
      "min_val_loss_print 0.10278142988681793\n",
      "epoch:  4   step:  14   train loss:  0.07987439632415771  val loss:  0.10255948454141617\n",
      "min_val_loss_print 0.10255948454141617\n",
      "epoch:  4   step:  15   train loss:  0.09129852056503296  val loss:  0.10222732275724411\n",
      "min_val_loss_print 0.10222732275724411\n",
      "epoch:  4   step:  16   train loss:  0.07559053599834442  val loss:  0.10251589864492416\n",
      "epoch:  4   step:  17   train loss:  0.10638406127691269  val loss:  0.10234871506690979\n",
      "epoch:  4   step:  18   train loss:  0.09888896346092224  val loss:  0.1022566705942154\n",
      "epoch:  4   step:  19   train loss:  0.136641263961792  val loss:  0.1014152392745018\n",
      "min_val_loss_print 0.1014152392745018\n",
      "epoch:  4   step:  20   train loss:  0.12692031264305115  val loss:  0.10268189758062363\n",
      "epoch:  4   step:  21   train loss:  0.123568095266819  val loss:  0.10255932062864304\n",
      "epoch:  4   step:  22   train loss:  0.07608497887849808  val loss:  0.10232307016849518\n",
      "epoch:  4   step:  23   train loss:  0.1605839729309082  val loss:  0.10276201367378235\n",
      "epoch:  4   step:  24   train loss:  0.0771799236536026  val loss:  0.10289867222309113\n",
      "epoch:  4   step:  25   train loss:  0.10480654239654541  val loss:  0.10335259139537811\n",
      "epoch:  4   step:  26   train loss:  0.11351063847541809  val loss:  0.10270363092422485\n",
      "epoch:  4   step:  27   train loss:  0.12476137280464172  val loss:  0.1012943685054779\n",
      "min_val_loss_print 0.1012943685054779\n",
      "epoch:  4   step:  28   train loss:  0.1100594773888588  val loss:  0.10050249099731445\n",
      "min_val_loss_print 0.10050249099731445\n",
      "epoch:  4   step:  29   train loss:  0.06607935577630997  val loss:  0.10066383332014084\n",
      "epoch:  4   step:  30   train loss:  0.12672226130962372  val loss:  0.10139148682355881\n",
      "epoch:  4   step:  31   train loss:  0.1295527070760727  val loss:  0.10077676922082901\n",
      "epoch:  4   step:  32   train loss:  0.07428804785013199  val loss:  0.10141443461179733\n",
      "epoch:  4   step:  33   train loss:  0.12563689053058624  val loss:  0.10185922682285309\n",
      "epoch:  4   step:  34   train loss:  0.08259212225675583  val loss:  0.10165957361459732\n",
      "epoch:  4   step:  35   train loss:  0.11024156957864761  val loss:  0.1018705666065216\n",
      "epoch:  4   step:  36   train loss:  0.1845901757478714  val loss:  0.10166003555059433\n",
      "epoch:  4   step:  37   train loss:  0.14668302237987518  val loss:  0.10126230865716934\n",
      "epoch:  4   step:  38   train loss:  0.10726681351661682  val loss:  0.10121380537748337\n",
      "epoch:  4   step:  39   train loss:  0.08670227229595184  val loss:  0.10106183588504791\n",
      "epoch:  4   step:  40   train loss:  0.08346262574195862  val loss:  0.1016942709684372\n",
      "epoch:  4   step:  41   train loss:  0.14657528698444366  val loss:  0.10224276036024094\n",
      "epoch:  4   step:  42   train loss:  0.1124601811170578  val loss:  0.10267484933137894\n",
      "epoch:  4   step:  43   train loss:  0.12933337688446045  val loss:  0.10372145473957062\n",
      "epoch:  4   step:  44   train loss:  0.18196387588977814  val loss:  0.10313941538333893\n",
      "epoch:  4   step:  45   train loss:  0.09214311093091965  val loss:  0.10221759974956512\n",
      "epoch:  4   step:  46   train loss:  0.11239112913608551  val loss:  0.10178495943546295\n",
      "epoch:  4   step:  47   train loss:  0.10822805017232895  val loss:  0.10042323917150497\n",
      "min_val_loss_print 0.10042323917150497\n",
      "epoch:  4   step:  48   train loss:  0.10153568536043167  val loss:  0.10024914145469666\n",
      "min_val_loss_print 0.10024914145469666\n",
      "epoch:  4   step:  49   train loss:  0.09544842690229416  val loss:  0.09990308433771133\n",
      "min_val_loss_print 0.09990308433771133\n",
      "epoch:  4   step:  50   train loss:  0.1010885089635849  val loss:  0.09950416535139084\n",
      "min_val_loss_print 0.09950416535139084\n",
      "epoch:  4   step:  51   train loss:  0.08425958454608917  val loss:  0.09945453703403473\n",
      "min_val_loss_print 0.09945453703403473\n",
      "epoch:  4   step:  52   train loss:  0.10314766317605972  val loss:  0.09929654002189636\n",
      "min_val_loss_print 0.09929654002189636\n",
      "epoch:  4   step:  53   train loss:  0.10189969837665558  val loss:  0.09880106896162033\n",
      "min_val_loss_print 0.09880106896162033\n",
      "epoch:  4   step:  54   train loss:  0.1007080078125  val loss:  0.09829556941986084\n",
      "min_val_loss_print 0.09829556941986084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  4   step:  55   train loss:  0.12723985314369202  val loss:  0.09827636927366257\n",
      "min_val_loss_print 0.09827636927366257\n",
      "epoch:  4   step:  56   train loss:  0.11753027141094208  val loss:  0.09828615933656693\n",
      "epoch:  4   step:  57   train loss:  0.07763270288705826  val loss:  0.09750400483608246\n",
      "min_val_loss_print 0.09750400483608246\n",
      "epoch:  4   step:  58   train loss:  0.10510256141424179  val loss:  0.09828730672597885\n",
      "epoch:  4   step:  59   train loss:  0.1744312196969986  val loss:  0.09773008525371552\n",
      "epoch:  4   step:  60   train loss:  0.08877535909414291  val loss:  0.09744685888290405\n",
      "min_val_loss_print 0.09744685888290405\n",
      "epoch:  4   step:  61   train loss:  0.0989067554473877  val loss:  0.09883452206850052\n",
      "epoch:  4   step:  62   train loss:  0.11460481584072113  val loss:  0.09773645550012589\n",
      "epoch:  4   step:  63   train loss:  0.13454020023345947  val loss:  0.09755124151706696\n",
      "epoch:  4   step:  64   train loss:  0.10952122509479523  val loss:  0.09848413616418839\n",
      "epoch:  4   step:  65   train loss:  0.10421086102724075  val loss:  0.09931567311286926\n",
      "epoch:  4   step:  66   train loss:  0.08025940507650375  val loss:  0.10047835856676102\n",
      "epoch:  4   step:  67   train loss:  0.0772005245089531  val loss:  0.09946921467781067\n",
      "epoch:  4   step:  68   train loss:  0.07064799219369888  val loss:  0.09906124323606491\n",
      "epoch:  4   step:  69   train loss:  0.09489461034536362  val loss:  0.098496213555336\n",
      "epoch:  4   step:  70   train loss:  0.08937066048383713  val loss:  0.09932006895542145\n",
      "epoch:  4   step:  71   train loss:  0.07515536993741989  val loss:  0.09977869689464569\n",
      "epoch:  4   step:  72   train loss:  0.07456810027360916  val loss:  0.09995311498641968\n",
      "epoch:  4   step:  73   train loss:  0.12072064727544785  val loss:  0.09870947897434235\n",
      "epoch:  4   step:  74   train loss:  0.08159542828798294  val loss:  0.09788352996110916\n",
      "epoch:  4   step:  75   train loss:  0.10482248663902283  val loss:  0.09803896397352219\n",
      "epoch:  4   step:  76   train loss:  0.09773004055023193  val loss:  0.09810636192560196\n",
      "epoch:  4   step:  77   train loss:  0.0828821137547493  val loss:  0.09822776913642883\n",
      "epoch:  4   step:  78   train loss:  0.12563782930374146  val loss:  0.09867224097251892\n",
      "epoch:  4   step:  79   train loss:  0.11188808083534241  val loss:  0.09782974421977997\n",
      "epoch:  4   step:  80   train loss:  0.07446759194135666  val loss:  0.09712615609169006\n",
      "min_val_loss_print 0.09712615609169006\n",
      "epoch:  4   step:  81   train loss:  0.09516581892967224  val loss:  0.09805544465780258\n",
      "epoch:  4   step:  82   train loss:  0.14925037324428558  val loss:  0.09824477881193161\n",
      "epoch:  4   step:  83   train loss:  0.07814155519008636  val loss:  0.1001855731010437\n",
      "epoch:  4   step:  84   train loss:  0.06573446840047836  val loss:  0.09925839304924011\n",
      "epoch:  4   step:  85   train loss:  0.13740912079811096  val loss:  0.09828216582536697\n",
      "epoch:  4   step:  86   train loss:  0.10141444951295853  val loss:  0.09766621142625809\n",
      "epoch:  4   step:  87   train loss:  0.10546378046274185  val loss:  0.0971466526389122\n",
      "epoch:  4   step:  88   train loss:  0.09871244430541992  val loss:  0.09738186746835709\n",
      "epoch:  4   step:  89   train loss:  0.09022774547338486  val loss:  0.0963170975446701\n",
      "min_val_loss_print 0.0963170975446701\n",
      "epoch:  4   step:  90   train loss:  0.13069726526737213  val loss:  0.09467838704586029\n",
      "min_val_loss_print 0.09467838704586029\n",
      "epoch:  4   step:  91   train loss:  0.15375226736068726  val loss:  0.09424397349357605\n",
      "min_val_loss_print 0.09424397349357605\n",
      "epoch:  4   step:  92   train loss:  0.14303924143314362  val loss:  0.09538770467042923\n",
      "epoch:  4   step:  93   train loss:  0.08688130229711533  val loss:  0.09607113152742386\n",
      "epoch:  4   step:  94   train loss:  0.0983826220035553  val loss:  0.09693858027458191\n",
      "epoch:  4   step:  95   train loss:  0.1055133119225502  val loss:  0.09610667824745178\n",
      "epoch:  4   step:  96   train loss:  0.11591161042451859  val loss:  0.09551519900560379\n",
      "epoch:  4   step:  97   train loss:  0.15193982422351837  val loss:  0.09623469412326813\n",
      "epoch:  4   step:  98   train loss:  0.11351874470710754  val loss:  0.09688768535852432\n",
      "epoch:  4   step:  99   train loss:  0.08817964047193527  val loss:  0.09756985306739807\n",
      "epoch:  4   step:  100   train loss:  0.06267090141773224  val loss:  0.09914544969797134\n",
      "epoch:  4   step:  101   train loss:  0.12614494562149048  val loss:  0.09849226474761963\n",
      "epoch:  4   step:  102   train loss:  0.08596433699131012  val loss:  0.09908490628004074\n",
      "epoch:  4   step:  103   train loss:  0.09965266287326813  val loss:  0.09881863743066788\n",
      "epoch:  4   step:  104   train loss:  0.08813048154115677  val loss:  0.0992497131228447\n",
      "epoch:  4   step:  105   train loss:  0.10764984041452408  val loss:  0.09949301183223724\n",
      "epoch:  4   step:  106   train loss:  0.10899204015731812  val loss:  0.10063257813453674\n",
      "epoch:  4   step:  107   train loss:  0.08846849948167801  val loss:  0.101810023188591\n",
      "epoch:  4   step:  108   train loss:  0.09111013263463974  val loss:  0.100532665848732\n",
      "epoch:  4   step:  109   train loss:  0.07629282027482986  val loss:  0.0986485555768013\n",
      "epoch:  4   step:  110   train loss:  0.11578652262687683  val loss:  0.09921132028102875\n",
      "epoch:  4   step:  111   train loss:  0.13166594505310059  val loss:  0.09918002039194107\n",
      "epoch:  4   step:  112   train loss:  0.07089976221323013  val loss:  0.09919659048318863\n",
      "epoch:  4   step:  113   train loss:  0.0694441869854927  val loss:  0.0987805724143982\n",
      "epoch:  4   step:  114   train loss:  0.1124843880534172  val loss:  0.09966728091239929\n",
      "epoch:  4   step:  115   train loss:  0.10064543038606644  val loss:  0.10032206028699875\n",
      "epoch:  4   step:  116   train loss:  0.1479410082101822  val loss:  0.0990222841501236\n",
      "epoch:  4   step:  117   train loss:  0.10176953673362732  val loss:  0.0979776456952095\n",
      "epoch:  4   step:  118   train loss:  0.07801896333694458  val loss:  0.0968153104186058\n",
      "epoch:  4   step:  119   train loss:  0.07842667400836945  val loss:  0.09640243649482727\n",
      "epoch:  4   step:  120   train loss:  0.11530783772468567  val loss:  0.09534990042448044\n",
      "epoch:  4   step:  121   train loss:  0.13273382186889648  val loss:  0.09487651288509369\n",
      "epoch:  4   step:  122   train loss:  0.07771071046590805  val loss:  0.09418243169784546\n",
      "min_val_loss_print 0.09418243169784546\n",
      "epoch:  4   step:  123   train loss:  0.09470716118812561  val loss:  0.09555867314338684\n",
      "epoch:  4   step:  124   train loss:  0.11159668862819672  val loss:  0.09581772983074188\n",
      "epoch:  4   step:  125   train loss:  0.091443732380867  val loss:  0.09449765831232071\n",
      "epoch:  4   step:  126   train loss:  0.11310102045536041  val loss:  0.09440077841281891\n",
      "epoch:  4   step:  127   train loss:  0.10966265946626663  val loss:  0.09497693926095963\n",
      "epoch:  4   step:  128   train loss:  0.07392960041761398  val loss:  0.09496835619211197\n",
      "epoch:  4   step:  129   train loss:  0.08579730987548828  val loss:  0.09481682628393173\n",
      "epoch:  4   step:  130   train loss:  0.09451154619455338  val loss:  0.09452953189611435\n",
      "epoch:  4   step:  131   train loss:  0.09524381160736084  val loss:  0.09387567639350891\n",
      "min_val_loss_print 0.09387567639350891\n",
      "epoch:  4   step:  132   train loss:  0.103567935526371  val loss:  0.09258557111024857\n",
      "min_val_loss_print 0.09258557111024857\n",
      "epoch:  4   step:  133   train loss:  0.08726765960454941  val loss:  0.09210870414972305\n",
      "min_val_loss_print 0.09210870414972305\n",
      "epoch:  4   step:  134   train loss:  0.06021076440811157  val loss:  0.09193389117717743\n",
      "min_val_loss_print 0.09193389117717743\n",
      "epoch:  4   step:  135   train loss:  0.09654203802347183  val loss:  0.09204605966806412\n",
      "epoch:  4   step:  136   train loss:  0.11245589703321457  val loss:  0.09113597869873047\n",
      "min_val_loss_print 0.09113597869873047\n",
      "epoch:  4   step:  137   train loss:  0.10504258424043655  val loss:  0.0903429239988327\n",
      "min_val_loss_print 0.0903429239988327\n",
      "epoch:  4   step:  138   train loss:  0.09691301733255386  val loss:  0.09004572033882141\n",
      "min_val_loss_print 0.09004572033882141\n",
      "epoch:  4   step:  139   train loss:  0.10771747678518295  val loss:  0.08994700014591217\n",
      "min_val_loss_print 0.08994700014591217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  4   step:  140   train loss:  0.11239895224571228  val loss:  0.09030330926179886\n",
      "epoch:  4   step:  141   train loss:  0.06891579180955887  val loss:  0.08853151649236679\n",
      "min_val_loss_print 0.08853151649236679\n",
      "epoch:  4   step:  142   train loss:  0.09321175515651703  val loss:  0.08889558166265488\n",
      "epoch:  4   step:  143   train loss:  0.08490582555532455  val loss:  0.0892413780093193\n",
      "epoch:  4   step:  144   train loss:  0.12537771463394165  val loss:  0.08918654173612595\n",
      "epoch:  4   step:  145   train loss:  0.11003833264112473  val loss:  0.08903723955154419\n",
      "epoch:  4   step:  146   train loss:  0.09048596024513245  val loss:  0.0894182026386261\n",
      "epoch:  4   step:  147   train loss:  0.11389684677124023  val loss:  0.09065332263708115\n",
      "epoch:  4   step:  148   train loss:  0.09167551249265671  val loss:  0.09083275496959686\n",
      "epoch:  4   step:  149   train loss:  0.08500052243471146  val loss:  0.09167305380105972\n",
      "epoch:  4   step:  150   train loss:  0.08185748010873795  val loss:  0.09254680573940277\n",
      "epoch:  4   step:  151   train loss:  0.09609472006559372  val loss:  0.09294425696134567\n",
      "epoch:  4   step:  152   train loss:  0.11252707988023758  val loss:  0.09330876171588898\n",
      "epoch:  4   step:  153   train loss:  0.0978054627776146  val loss:  0.09429243206977844\n",
      "epoch:  4   step:  154   train loss:  0.09084232151508331  val loss:  0.09449194371700287\n",
      "epoch:  4   step:  155   train loss:  0.0960172787308693  val loss:  0.09415421634912491\n",
      "epoch:  4   step:  156   train loss:  0.09761246293783188  val loss:  0.0943731889128685\n",
      "epoch:  4   step:  157   train loss:  0.08966721594333649  val loss:  0.0938371866941452\n",
      "epoch:  4   step:  158   train loss:  0.09982502460479736  val loss:  0.0947939082980156\n",
      "epoch:  4   step:  159   train loss:  0.11347411572933197  val loss:  0.09314355999231339\n",
      "epoch:  4   step:  160   train loss:  0.10541758686304092  val loss:  0.09312283992767334\n",
      "epoch:  4   step:  161   train loss:  0.1040520966053009  val loss:  0.094547338783741\n",
      "epoch:  4   step:  162   train loss:  0.07302504032850266  val loss:  0.09534278512001038\n",
      "epoch:  4   step:  163   train loss:  0.09322233498096466  val loss:  0.09428894519805908\n",
      "epoch:  4   step:  164   train loss:  0.11857905983924866  val loss:  0.09587591886520386\n",
      "epoch:  4   step:  165   train loss:  0.08483512699604034  val loss:  0.09600911289453506\n",
      "epoch:  4   step:  166   train loss:  0.07795622944831848  val loss:  0.09712567180395126\n",
      "epoch:  4   step:  167   train loss:  0.09529640525579453  val loss:  0.0973745584487915\n",
      "epoch:  4   step:  168   train loss:  0.09438472986221313  val loss:  0.09722152352333069\n",
      "epoch:  4   step:  169   train loss:  0.09178461134433746  val loss:  0.09636332094669342\n",
      "epoch:  4   step:  170   train loss:  0.07995026558637619  val loss:  0.09557827562093735\n",
      "epoch:  4   step:  171   train loss:  0.10596843808889389  val loss:  0.09378557652235031\n",
      "epoch:  4   step:  172   train loss:  0.08212906867265701  val loss:  0.09390328079462051\n",
      "epoch:  4   step:  173   train loss:  0.14936161041259766  val loss:  0.09328881651163101\n",
      "epoch:  4   step:  174   train loss:  0.11723887175321579  val loss:  0.0923164114356041\n",
      "epoch:  4   step:  175   train loss:  0.08966697007417679  val loss:  0.0923156589269638\n",
      "epoch:  4   step:  176   train loss:  0.11872708797454834  val loss:  0.09319733828306198\n",
      "epoch:  4   step:  177   train loss:  0.08929193764925003  val loss:  0.09180355817079544\n",
      "epoch:  4   step:  178   train loss:  0.1558750867843628  val loss:  0.09080274403095245\n",
      "epoch:  4   step:  179   train loss:  0.10517393797636032  val loss:  0.09061074256896973\n",
      "epoch:  4   step:  180   train loss:  0.09344042092561722  val loss:  0.09081404656171799\n",
      "epoch:  4   step:  181   train loss:  0.09552526473999023  val loss:  0.09155053645372391\n",
      "epoch:  4   step:  182   train loss:  0.08798824995756149  val loss:  0.09195840358734131\n",
      "epoch:  4   step:  183   train loss:  0.08502623438835144  val loss:  0.09166525304317474\n",
      "epoch:  4   step:  184   train loss:  0.1139143705368042  val loss:  0.09066824615001678\n",
      "epoch:  4   step:  185   train loss:  0.1003522276878357  val loss:  0.08957124501466751\n",
      "epoch:  4   step:  186   train loss:  0.08407995849847794  val loss:  0.08974788337945938\n",
      "epoch:  4   step:  187   train loss:  0.08964350819587708  val loss:  0.08945806324481964\n",
      "epoch:  4   step:  188   train loss:  0.13082461059093475  val loss:  0.08860129117965698\n",
      "epoch:  4   step:  189   train loss:  0.07783763855695724  val loss:  0.08899141103029251\n",
      "epoch:  4   step:  190   train loss:  0.05737978592514992  val loss:  0.08814219385385513\n",
      "min_val_loss_print 0.08814219385385513\n",
      "epoch:  4   step:  191   train loss:  0.08982367068529129  val loss:  0.08821284025907516\n",
      "epoch:  4   step:  192   train loss:  0.06859632581472397  val loss:  0.08716381341218948\n",
      "min_val_loss_print 0.08716381341218948\n",
      "epoch:  4   step:  193   train loss:  0.14925149083137512  val loss:  0.08720967918634415\n",
      "epoch:  4   step:  194   train loss:  0.1098034530878067  val loss:  0.08622570335865021\n",
      "min_val_loss_print 0.08622570335865021\n",
      "epoch:  4   step:  195   train loss:  0.10260134935379028  val loss:  0.08623004704713821\n",
      "epoch:  4   step:  196   train loss:  0.08881079405546188  val loss:  0.08588066697120667\n",
      "min_val_loss_print 0.08588066697120667\n",
      "epoch:  4   step:  197   train loss:  0.10706712305545807  val loss:  0.08508174866437912\n",
      "min_val_loss_print 0.08508174866437912\n",
      "epoch:  4   step:  198   train loss:  0.12230168282985687  val loss:  0.08409406989812851\n",
      "min_val_loss_print 0.08409406989812851\n",
      "epoch:  4   step:  199   train loss:  0.09411851316690445  val loss:  0.08478106558322906\n",
      "epoch:  4   step:  200   train loss:  0.1625417321920395  val loss:  0.08590167760848999\n",
      "epoch:  4   step:  201   train loss:  0.10462608188390732  val loss:  0.08691493421792984\n",
      "epoch:  4   step:  202   train loss:  0.08893521875143051  val loss:  0.08755367249250412\n",
      "epoch:  4   step:  203   train loss:  0.11710003763437271  val loss:  0.08729445189237595\n",
      "epoch:  4   step:  204   train loss:  0.06170463562011719  val loss:  0.08837928622961044\n",
      "epoch:  4   step:  205   train loss:  0.12206831574440002  val loss:  0.0878043845295906\n",
      "epoch:  4   step:  206   train loss:  0.11449402570724487  val loss:  0.08846486359834671\n",
      "epoch:  4   step:  207   train loss:  0.06133892014622688  val loss:  0.08817193657159805\n",
      "epoch:  4   step:  208   train loss:  0.09036145359277725  val loss:  0.08852026611566544\n",
      "epoch:  4   step:  209   train loss:  0.05710515379905701  val loss:  0.08786362409591675\n",
      "epoch:  4   step:  210   train loss:  0.12071773409843445  val loss:  0.08812122792005539\n",
      "epoch:  4   step:  211   train loss:  0.09263032674789429  val loss:  0.08842521905899048\n",
      "epoch:  4   step:  212   train loss:  0.09975305199623108  val loss:  0.08846228569746017\n",
      "epoch:  4   step:  213   train loss:  0.12135844677686691  val loss:  0.0900023952126503\n",
      "epoch:  4   step:  214   train loss:  0.10797513276338577  val loss:  0.09085001051425934\n",
      "epoch:  4   step:  215   train loss:  0.08301649242639542  val loss:  0.09022878110408783\n",
      "epoch:  4   step:  216   train loss:  0.07975003123283386  val loss:  0.09105150401592255\n",
      "epoch:  4   step:  217   train loss:  0.08791589736938477  val loss:  0.0903497040271759\n",
      "epoch:  4   step:  218   train loss:  0.08152645081281662  val loss:  0.08915495872497559\n",
      "epoch:  4   step:  219   train loss:  0.07781053334474564  val loss:  0.08882395178079605\n",
      "epoch:  4   step:  220   train loss:  0.11809863895177841  val loss:  0.08914179354906082\n",
      "epoch:  4   step:  221   train loss:  0.10018081963062286  val loss:  0.08933978527784348\n",
      "epoch:  4   step:  222   train loss:  0.07477953284978867  val loss:  0.08998782187700272\n",
      "epoch:  4   step:  223   train loss:  0.10090257972478867  val loss:  0.09067446738481522\n",
      "epoch:  4   step:  224   train loss:  0.052673228085041046  val loss:  0.0900614857673645\n",
      "epoch:  4   step:  225   train loss:  0.08472425490617752  val loss:  0.09014229476451874\n",
      "epoch:  4   step:  226   train loss:  0.07493647187948227  val loss:  0.08995778858661652\n",
      "epoch:  4   step:  227   train loss:  0.11292480677366257  val loss:  0.08874068409204483\n",
      "epoch:  4   step:  228   train loss:  0.09046061336994171  val loss:  0.0885818749666214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  4   step:  229   train loss:  0.0897243395447731  val loss:  0.08825542777776718\n",
      "epoch:  4   step:  230   train loss:  0.0910714864730835  val loss:  0.08668581396341324\n",
      "epoch:  4   step:  231   train loss:  0.0830136314034462  val loss:  0.08556889742612839\n",
      "epoch:  4   step:  232   train loss:  0.10082422196865082  val loss:  0.08455517888069153\n",
      "epoch:  4   step:  233   train loss:  0.09119290858507156  val loss:  0.08436259627342224\n",
      "epoch:  4   step:  234   train loss:  0.07469107955694199  val loss:  0.08391612768173218\n",
      "min_val_loss_print 0.08391612768173218\n",
      "epoch:  4   step:  235   train loss:  0.08535836637020111  val loss:  0.08351824432611465\n",
      "min_val_loss_print 0.08351824432611465\n",
      "epoch:  4   step:  236   train loss:  0.06722726672887802  val loss:  0.08272094279527664\n",
      "min_val_loss_print 0.08272094279527664\n",
      "epoch:  4   step:  237   train loss:  0.12178310751914978  val loss:  0.08226968348026276\n",
      "min_val_loss_print 0.08226968348026276\n",
      "epoch:  4   step:  238   train loss:  0.0896383672952652  val loss:  0.08230028301477432\n",
      "epoch:  4   step:  239   train loss:  0.08552096039056778  val loss:  0.08215735107660294\n",
      "min_val_loss_print 0.08215735107660294\n",
      "epoch:  4   step:  240   train loss:  0.10204555094242096  val loss:  0.08262637257575989\n",
      "epoch:  4   step:  241   train loss:  0.052407052367925644  val loss:  0.08273027837276459\n",
      "epoch:  4   step:  242   train loss:  0.14750999212265015  val loss:  0.08421605825424194\n",
      "epoch:  4   step:  243   train loss:  0.11557941883802414  val loss:  0.08599962294101715\n",
      "epoch:  4   step:  244   train loss:  0.05584045127034187  val loss:  0.087680883705616\n",
      "epoch:  4   step:  245   train loss:  0.11325040459632874  val loss:  0.08826130628585815\n",
      "epoch:  4   step:  246   train loss:  0.1034967377781868  val loss:  0.08798414468765259\n",
      "epoch:  4   step:  247   train loss:  0.07744661718606949  val loss:  0.08677665144205093\n",
      "epoch:  4   step:  248   train loss:  0.12725304067134857  val loss:  0.08590706437826157\n",
      "epoch:  4   step:  249   train loss:  0.11967000365257263  val loss:  0.08651968091726303\n",
      "epoch:  4   step:  250   train loss:  0.06220339611172676  val loss:  0.0872606560587883\n",
      "epoch:  4   step:  251   train loss:  0.1343836486339569  val loss:  0.08822526037693024\n",
      "epoch:  4   step:  252   train loss:  0.08342351764440536  val loss:  0.08833874017000198\n",
      "epoch:  4   step:  253   train loss:  0.09667766094207764  val loss:  0.08868508040904999\n",
      "epoch:  4   step:  254   train loss:  0.07899127155542374  val loss:  0.08805662393569946\n",
      "epoch:  4   step:  255   train loss:  0.12207140028476715  val loss:  0.08826451748609543\n",
      "epoch:  4   step:  256   train loss:  0.07043420523405075  val loss:  0.08846397697925568\n",
      "epoch:  4   step:  257   train loss:  0.09820298850536346  val loss:  0.08700638264417648\n",
      "epoch:  4   step:  258   train loss:  0.10207477957010269  val loss:  0.08629757910966873\n",
      "epoch:  4   step:  259   train loss:  0.11460994184017181  val loss:  0.08658221364021301\n",
      "epoch:  4   step:  260   train loss:  0.0905270054936409  val loss:  0.08667890727519989\n",
      "epoch:  4   step:  261   train loss:  0.09720417112112045  val loss:  0.08779371529817581\n",
      "epoch:  4   step:  262   train loss:  0.09690309315919876  val loss:  0.08741264045238495\n",
      "epoch:  4   step:  263   train loss:  0.132522314786911  val loss:  0.08767013996839523\n",
      "epoch:  4   step:  264   train loss:  0.10180597752332687  val loss:  0.08685565739870071\n",
      "epoch:  4   step:  265   train loss:  0.06504940241575241  val loss:  0.0852818563580513\n",
      "epoch:  4   step:  266   train loss:  0.06740391254425049  val loss:  0.08436528593301773\n",
      "epoch:  4   step:  267   train loss:  0.08296225219964981  val loss:  0.08438993990421295\n",
      "epoch:  4   step:  268   train loss:  0.09648022055625916  val loss:  0.08419691026210785\n",
      "epoch:  4   step:  269   train loss:  0.09426260739564896  val loss:  0.0850265845656395\n",
      "epoch:  4   step:  270   train loss:  0.05385712534189224  val loss:  0.08609914034605026\n",
      "epoch:  4   step:  271   train loss:  0.10074638575315475  val loss:  0.08652877062559128\n",
      "epoch:  4   step:  272   train loss:  0.08379017561674118  val loss:  0.08661650866270065\n",
      "epoch:  4   step:  273   train loss:  0.10042698681354523  val loss:  0.08716394752264023\n",
      "epoch:  4   step:  274   train loss:  0.13612672686576843  val loss:  0.08792727440595627\n",
      "epoch:  4   step:  275   train loss:  0.049519121646881104  val loss:  0.088023841381073\n",
      "epoch:  4   step:  276   train loss:  0.07689674943685532  val loss:  0.08864956349134445\n",
      "epoch:  4   step:  277   train loss:  0.05814138799905777  val loss:  0.08853067457675934\n",
      "epoch:  4   step:  278   train loss:  0.07100407779216766  val loss:  0.08767841756343842\n",
      "epoch:  4   step:  279   train loss:  0.1482221782207489  val loss:  0.08870995789766312\n",
      "epoch:  4   step:  280   train loss:  0.1506556123495102  val loss:  0.08795849233865738\n",
      "epoch:  4   step:  281   train loss:  0.12055491656064987  val loss:  0.08773145824670792\n",
      "epoch:  4   step:  282   train loss:  0.1085854023694992  val loss:  0.08541283011436462\n",
      "epoch:  4   step:  283   train loss:  0.09375032037496567  val loss:  0.08611387014389038\n",
      "epoch:  4   step:  284   train loss:  0.0916840061545372  val loss:  0.08580967783927917\n",
      "epoch:  4   step:  285   train loss:  0.0870542898774147  val loss:  0.08567425608634949\n",
      "epoch:  4   step:  286   train loss:  0.23918001353740692  val loss:  0.08492632955312729\n",
      "epoch:  4   step:  287   train loss:  0.10652875900268555  val loss:  0.08378340303897858\n",
      "epoch:  4   step:  288   train loss:  0.10619749873876572  val loss:  0.08363345265388489\n",
      "epoch:  4   step:  289   train loss:  0.08078310638666153  val loss:  0.08103568106889725\n",
      "min_val_loss_print 0.08103568106889725\n",
      "epoch:  4   step:  290   train loss:  0.09962636977434158  val loss:  0.08038946241140366\n",
      "min_val_loss_print 0.08038946241140366\n",
      "epoch:  4   step:  291   train loss:  0.14437296986579895  val loss:  0.08063384890556335\n",
      "epoch:  4   step:  292   train loss:  0.09916235506534576  val loss:  0.0798439309000969\n",
      "min_val_loss_print 0.0798439309000969\n",
      "epoch:  4   step:  293   train loss:  0.08318743109703064  val loss:  0.07979834079742432\n",
      "min_val_loss_print 0.07979834079742432\n",
      "epoch:  4   step:  294   train loss:  0.07632175832986832  val loss:  0.07954689115285873\n",
      "min_val_loss_print 0.07954689115285873\n",
      "epoch:  4   step:  295   train loss:  0.12774671614170074  val loss:  0.07897073030471802\n",
      "min_val_loss_print 0.07897073030471802\n",
      "epoch:  4   step:  296   train loss:  0.0851433053612709  val loss:  0.07908887416124344\n",
      "epoch:  4   step:  297   train loss:  0.07123774290084839  val loss:  0.079459547996521\n",
      "epoch:  4   step:  298   train loss:  0.08788689970970154  val loss:  0.07934414595365524\n",
      "epoch:  4   step:  299   train loss:  0.09297097474336624  val loss:  0.07776100188493729\n",
      "min_val_loss_print 0.07776100188493729\n",
      "epoch:  4   step:  300   train loss:  0.08348284661769867  val loss:  0.07483933120965958\n",
      "min_val_loss_print 0.07483933120965958\n",
      "epoch:  4   step:  301   train loss:  0.09340617060661316  val loss:  0.07429254800081253\n",
      "min_val_loss_print 0.07429254800081253\n",
      "epoch:  4   step:  302   train loss:  0.07234063744544983  val loss:  0.07385881245136261\n",
      "min_val_loss_print 0.07385881245136261\n",
      "epoch:  4   step:  303   train loss:  0.09553425014019012  val loss:  0.07438832521438599\n",
      "epoch:  4   step:  304   train loss:  0.09288053214550018  val loss:  0.07395613938570023\n",
      "epoch:  4   step:  305   train loss:  0.12469281256198883  val loss:  0.07440218329429626\n",
      "epoch:  4   step:  306   train loss:  0.06627177447080612  val loss:  0.07483935356140137\n",
      "epoch:  4   step:  307   train loss:  0.08103856444358826  val loss:  0.07470855861902237\n",
      "epoch:  4   step:  308   train loss:  0.09069611132144928  val loss:  0.0733945444226265\n",
      "min_val_loss_print 0.0733945444226265\n",
      "epoch:  4   step:  309   train loss:  0.09237925708293915  val loss:  0.07229827344417572\n",
      "min_val_loss_print 0.07229827344417572\n",
      "epoch:  4   step:  310   train loss:  0.08076155185699463  val loss:  0.07202886790037155\n",
      "min_val_loss_print 0.07202886790037155\n",
      "epoch:  4   step:  311   train loss:  0.10711874067783356  val loss:  0.07193680107593536\n",
      "min_val_loss_print 0.07193680107593536\n",
      "epoch:  4   step:  312   train loss:  0.10874665528535843  val loss:  0.07079213857650757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_val_loss_print 0.07079213857650757\n",
      "epoch:  4   step:  313   train loss:  0.06463782489299774  val loss:  0.0709737166762352\n",
      "epoch:  4   step:  314   train loss:  0.11225679516792297  val loss:  0.07063163816928864\n",
      "min_val_loss_print 0.07063163816928864\n",
      "epoch:  4   step:  315   train loss:  0.08773162215948105  val loss:  0.07011694461107254\n",
      "min_val_loss_print 0.07011694461107254\n",
      "epoch:  4   step:  316   train loss:  0.07392939180135727  val loss:  0.06981972604990005\n",
      "min_val_loss_print 0.06981972604990005\n",
      "epoch:  4   step:  317   train loss:  0.07983317971229553  val loss:  0.06991903483867645\n",
      "epoch:  4   step:  318   train loss:  0.11872753500938416  val loss:  0.07066076993942261\n",
      "epoch:  4   step:  319   train loss:  0.13638600707054138  val loss:  0.07055983692407608\n",
      "epoch:  4   step:  320   train loss:  0.07296063005924225  val loss:  0.07022234052419662\n",
      "epoch:  4   step:  321   train loss:  0.09125600755214691  val loss:  0.06940504908561707\n",
      "min_val_loss_print 0.06940504908561707\n",
      "epoch:  4   step:  322   train loss:  0.06250496208667755  val loss:  0.06986343860626221\n",
      "epoch:  4   step:  323   train loss:  0.07193829119205475  val loss:  0.06926130503416061\n",
      "min_val_loss_print 0.06926130503416061\n",
      "epoch:  4   step:  324   train loss:  0.05412149056792259  val loss:  0.06916233897209167\n",
      "min_val_loss_print 0.06916233897209167\n",
      "epoch:  4   step:  325   train loss:  0.11695131659507751  val loss:  0.06956254690885544\n",
      "epoch:  4   step:  326   train loss:  0.11147460341453552  val loss:  0.06981337070465088\n",
      "epoch:  4   step:  327   train loss:  0.054014161229133606  val loss:  0.06997818499803543\n",
      "epoch:  4   step:  328   train loss:  0.05503913760185242  val loss:  0.06981422007083893\n",
      "epoch:  4   step:  329   train loss:  0.07502079010009766  val loss:  0.06992314755916595\n",
      "epoch:  4   step:  330   train loss:  0.0677540972828865  val loss:  0.07014740258455276\n",
      "epoch:  4   step:  331   train loss:  0.0935930609703064  val loss:  0.0705348402261734\n",
      "epoch:  4   step:  332   train loss:  0.09038570523262024  val loss:  0.07087021321058273\n",
      "epoch:  4   step:  333   train loss:  0.07792870700359344  val loss:  0.0717327669262886\n",
      "epoch:  4   step:  334   train loss:  0.08674325793981552  val loss:  0.07130306214094162\n",
      "epoch:  4   step:  335   train loss:  0.08689116686582565  val loss:  0.07063878327608109\n",
      "epoch:  4   step:  336   train loss:  0.09216463565826416  val loss:  0.06893736869096756\n",
      "min_val_loss_print 0.06893736869096756\n",
      "epoch:  4   step:  337   train loss:  0.09765909612178802  val loss:  0.06921259313821793\n",
      "epoch:  4   step:  338   train loss:  0.09913980960845947  val loss:  0.06996378302574158\n",
      "epoch:  4   step:  339   train loss:  0.12745635211467743  val loss:  0.07039062678813934\n",
      "epoch:  4   step:  340   train loss:  0.10154833644628525  val loss:  0.07038465142250061\n",
      "epoch:  4   step:  341   train loss:  0.08213511109352112  val loss:  0.07078737765550613\n",
      "epoch:  4   step:  342   train loss:  0.0837092399597168  val loss:  0.07100687175989151\n",
      "epoch:  4   step:  343   train loss:  0.17765121161937714  val loss:  0.07159947603940964\n",
      "epoch:  5   step:  0   train loss:  0.09662535786628723  val loss:  0.07111837714910507\n",
      "epoch:  5   step:  1   train loss:  0.08010408282279968  val loss:  0.07029220461845398\n",
      "epoch:  5   step:  2   train loss:  0.09208448231220245  val loss:  0.0703752264380455\n",
      "epoch:  5   step:  3   train loss:  0.12379597127437592  val loss:  0.07044374197721481\n",
      "epoch:  5   step:  4   train loss:  0.06162867695093155  val loss:  0.07048623263835907\n",
      "epoch:  5   step:  5   train loss:  0.10213157534599304  val loss:  0.07009629905223846\n",
      "epoch:  5   step:  6   train loss:  0.06394436210393906  val loss:  0.06989282369613647\n",
      "epoch:  5   step:  7   train loss:  0.0641443058848381  val loss:  0.07079344242811203\n",
      "epoch:  5   step:  8   train loss:  0.07853353023529053  val loss:  0.07077310979366302\n",
      "epoch:  5   step:  9   train loss:  0.07167992740869522  val loss:  0.06976893544197083\n",
      "epoch:  5   step:  10   train loss:  0.06946581602096558  val loss:  0.06981722265481949\n",
      "epoch:  5   step:  11   train loss:  0.08071085810661316  val loss:  0.06959149986505508\n",
      "epoch:  5   step:  12   train loss:  0.0643606036901474  val loss:  0.06926053762435913\n",
      "epoch:  5   step:  13   train loss:  0.11973142623901367  val loss:  0.06992444396018982\n",
      "epoch:  5   step:  14   train loss:  0.08512640744447708  val loss:  0.07009154558181763\n",
      "epoch:  5   step:  15   train loss:  0.07345765084028244  val loss:  0.07057216018438339\n",
      "epoch:  5   step:  16   train loss:  0.09196798503398895  val loss:  0.07094838470220566\n",
      "epoch:  5   step:  17   train loss:  0.0809318944811821  val loss:  0.07215581834316254\n",
      "epoch:  5   step:  18   train loss:  0.05439962446689606  val loss:  0.07239899039268494\n",
      "epoch:  5   step:  19   train loss:  0.11896458268165588  val loss:  0.07252108305692673\n",
      "epoch:  5   step:  20   train loss:  0.10192713141441345  val loss:  0.07286672294139862\n",
      "epoch:  5   step:  21   train loss:  0.061767928302288055  val loss:  0.07374007999897003\n",
      "epoch:  5   step:  22   train loss:  0.11363816261291504  val loss:  0.07414744794368744\n",
      "epoch:  5   step:  23   train loss:  0.07678484916687012  val loss:  0.07412733137607574\n",
      "epoch:  5   step:  24   train loss:  0.08932749181985855  val loss:  0.07361724972724915\n",
      "epoch:  5   step:  25   train loss:  0.09737899154424667  val loss:  0.07316271215677261\n",
      "epoch:  5   step:  26   train loss:  0.0663432702422142  val loss:  0.07347027212381363\n",
      "epoch:  5   step:  27   train loss:  0.08590704202651978  val loss:  0.07411890476942062\n",
      "epoch:  5   step:  28   train loss:  0.06206663325428963  val loss:  0.07395713776350021\n",
      "epoch:  5   step:  29   train loss:  0.08428405970335007  val loss:  0.07348223775625229\n",
      "epoch:  5   step:  30   train loss:  0.07713097333908081  val loss:  0.07301683723926544\n",
      "epoch:  5   step:  31   train loss:  0.060459550470113754  val loss:  0.07366644591093063\n",
      "epoch:  5   step:  32   train loss:  0.0965840145945549  val loss:  0.07453488558530807\n",
      "epoch:  5   step:  33   train loss:  0.0791834220290184  val loss:  0.07499238103628159\n",
      "epoch:  5   step:  34   train loss:  0.0691831186413765  val loss:  0.07475439459085464\n",
      "epoch:  5   step:  35   train loss:  0.10006538778543472  val loss:  0.0751819759607315\n",
      "epoch:  5   step:  36   train loss:  0.09984391182661057  val loss:  0.07501567155122757\n",
      "epoch:  5   step:  37   train loss:  0.08283674716949463  val loss:  0.07518967241048813\n",
      "epoch:  5   step:  38   train loss:  0.062346719205379486  val loss:  0.07496532052755356\n",
      "epoch:  5   step:  39   train loss:  0.08568108826875687  val loss:  0.075396828353405\n",
      "epoch:  5   step:  40   train loss:  0.07464528828859329  val loss:  0.07558585703372955\n",
      "epoch:  5   step:  41   train loss:  0.09073800593614578  val loss:  0.07532276213169098\n",
      "epoch:  5   step:  42   train loss:  0.09639394283294678  val loss:  0.07442782074213028\n",
      "epoch:  5   step:  43   train loss:  0.04926173761487007  val loss:  0.07353625446557999\n",
      "epoch:  5   step:  44   train loss:  0.060732401907444  val loss:  0.07316361367702484\n",
      "epoch:  5   step:  45   train loss:  0.07827578485012054  val loss:  0.07386236637830734\n",
      "epoch:  5   step:  46   train loss:  0.07862777262926102  val loss:  0.07378515601158142\n",
      "epoch:  5   step:  47   train loss:  0.13353711366653442  val loss:  0.07371795177459717\n",
      "epoch:  5   step:  48   train loss:  0.08708494901657104  val loss:  0.07136455178260803\n",
      "epoch:  5   step:  49   train loss:  0.09713234752416611  val loss:  0.07088174670934677\n",
      "epoch:  5   step:  50   train loss:  0.1114516332745552  val loss:  0.07053223997354507\n",
      "epoch:  5   step:  51   train loss:  0.0777050256729126  val loss:  0.07053525000810623\n",
      "epoch:  5   step:  52   train loss:  0.08162297308444977  val loss:  0.07105961441993713\n",
      "epoch:  5   step:  53   train loss:  0.08578164875507355  val loss:  0.07137024402618408\n",
      "epoch:  5   step:  54   train loss:  0.08687558770179749  val loss:  0.07075604796409607\n",
      "epoch:  5   step:  55   train loss:  0.07216411828994751  val loss:  0.07080583274364471\n",
      "epoch:  5   step:  56   train loss:  0.06835661828517914  val loss:  0.07047630101442337\n",
      "epoch:  5   step:  57   train loss:  0.11644522845745087  val loss:  0.07069157063961029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  5   step:  58   train loss:  0.09829279780387878  val loss:  0.0693388357758522\n",
      "epoch:  5   step:  59   train loss:  0.08397836983203888  val loss:  0.07053384929895401\n",
      "epoch:  5   step:  60   train loss:  0.09921110421419144  val loss:  0.06999658048152924\n",
      "epoch:  5   step:  61   train loss:  0.07397105544805527  val loss:  0.06939331442117691\n",
      "epoch:  5   step:  62   train loss:  0.1165686771273613  val loss:  0.06968869268894196\n",
      "epoch:  5   step:  63   train loss:  0.11986425518989563  val loss:  0.07058263570070267\n",
      "epoch:  5   step:  64   train loss:  0.07401719689369202  val loss:  0.07057592272758484\n",
      "epoch:  5   step:  65   train loss:  0.09851902723312378  val loss:  0.07076389342546463\n",
      "epoch:  5   step:  66   train loss:  0.09768684208393097  val loss:  0.07109812647104263\n",
      "epoch:  5   step:  67   train loss:  0.09752019494771957  val loss:  0.07098580151796341\n",
      "epoch:  5   step:  68   train loss:  0.08416015654802322  val loss:  0.07058649510145187\n",
      "epoch:  5   step:  69   train loss:  0.0763169452548027  val loss:  0.07016536593437195\n",
      "epoch:  5   step:  70   train loss:  0.0929994136095047  val loss:  0.07047995179891586\n",
      "epoch:  5   step:  71   train loss:  0.11313239485025406  val loss:  0.07033128291368484\n",
      "epoch:  5   step:  72   train loss:  0.10795802623033524  val loss:  0.06994358450174332\n",
      "epoch:  5   step:  73   train loss:  0.04192090407013893  val loss:  0.07076232135295868\n",
      "epoch:  5   step:  74   train loss:  0.07807483524084091  val loss:  0.07152002304792404\n",
      "epoch:  5   step:  75   train loss:  0.0783524215221405  val loss:  0.07141898572444916\n",
      "epoch:  5   step:  76   train loss:  0.05827048420906067  val loss:  0.07063228636980057\n",
      "epoch:  5   step:  77   train loss:  0.08319099992513657  val loss:  0.07004974782466888\n",
      "epoch:  5   step:  78   train loss:  0.13210050761699677  val loss:  0.07092507928609848\n",
      "epoch:  5   step:  79   train loss:  0.08315584808588028  val loss:  0.07019615173339844\n",
      "epoch:  5   step:  80   train loss:  0.07469220459461212  val loss:  0.07050006091594696\n",
      "epoch:  5   step:  81   train loss:  0.06094765663146973  val loss:  0.07088673114776611\n",
      "epoch:  5   step:  82   train loss:  0.06666716933250427  val loss:  0.06837600469589233\n",
      "min_val_loss_print 0.06837600469589233\n",
      "epoch:  5   step:  83   train loss:  0.10607166588306427  val loss:  0.06913266330957413\n",
      "epoch:  5   step:  84   train loss:  0.06062697991728783  val loss:  0.06874112784862518\n",
      "epoch:  5   step:  85   train loss:  0.08264122158288956  val loss:  0.06933724880218506\n",
      "epoch:  5   step:  86   train loss:  0.06571054458618164  val loss:  0.06973970681428909\n",
      "epoch:  5   step:  87   train loss:  0.09097418189048767  val loss:  0.06998312473297119\n",
      "epoch:  5   step:  88   train loss:  0.042094286531209946  val loss:  0.06989841908216476\n",
      "epoch:  5   step:  89   train loss:  0.06035393849015236  val loss:  0.06968472898006439\n",
      "epoch:  5   step:  90   train loss:  0.07710123807191849  val loss:  0.06948305666446686\n",
      "epoch:  5   step:  91   train loss:  0.06540306657552719  val loss:  0.06932414323091507\n",
      "epoch:  5   step:  92   train loss:  0.04873012751340866  val loss:  0.06775743514299393\n",
      "min_val_loss_print 0.06775743514299393\n",
      "epoch:  5   step:  93   train loss:  0.09479416906833649  val loss:  0.06782679259777069\n",
      "epoch:  5   step:  94   train loss:  0.04499697685241699  val loss:  0.06709642708301544\n",
      "min_val_loss_print 0.06709642708301544\n",
      "epoch:  5   step:  95   train loss:  0.09224172681570053  val loss:  0.0675591379404068\n",
      "epoch:  5   step:  96   train loss:  0.088471919298172  val loss:  0.06742105633020401\n",
      "epoch:  5   step:  97   train loss:  0.06616566330194473  val loss:  0.06843069195747375\n",
      "epoch:  5   step:  98   train loss:  0.07606629282236099  val loss:  0.06873930990695953\n",
      "epoch:  5   step:  99   train loss:  0.05554746091365814  val loss:  0.06825387477874756\n",
      "epoch:  5   step:  100   train loss:  0.0639171451330185  val loss:  0.06762467324733734\n",
      "epoch:  5   step:  101   train loss:  0.05403580889105797  val loss:  0.06726425141096115\n",
      "epoch:  5   step:  102   train loss:  0.09280098974704742  val loss:  0.06627272069454193\n",
      "min_val_loss_print 0.06627272069454193\n",
      "epoch:  5   step:  103   train loss:  0.06420298665761948  val loss:  0.06780803948640823\n",
      "epoch:  5   step:  104   train loss:  0.07747417688369751  val loss:  0.06802642345428467\n",
      "epoch:  5   step:  105   train loss:  0.04813646897673607  val loss:  0.06795957684516907\n",
      "epoch:  5   step:  106   train loss:  0.052820831537246704  val loss:  0.0679374411702156\n",
      "epoch:  5   step:  107   train loss:  0.09534451365470886  val loss:  0.06750372052192688\n",
      "epoch:  5   step:  108   train loss:  0.09688475728034973  val loss:  0.06762927770614624\n",
      "epoch:  5   step:  109   train loss:  0.08321437984704971  val loss:  0.06733808666467667\n",
      "epoch:  5   step:  110   train loss:  0.07624685764312744  val loss:  0.06783149391412735\n",
      "epoch:  5   step:  111   train loss:  0.06636642664670944  val loss:  0.06876221299171448\n",
      "epoch:  5   step:  112   train loss:  0.07291454821825027  val loss:  0.06846702098846436\n",
      "epoch:  5   step:  113   train loss:  0.07135263085365295  val loss:  0.06667538732290268\n",
      "epoch:  5   step:  114   train loss:  0.06123866140842438  val loss:  0.06664396077394485\n",
      "epoch:  5   step:  115   train loss:  0.05968062952160835  val loss:  0.0657949447631836\n",
      "min_val_loss_print 0.0657949447631836\n",
      "epoch:  5   step:  116   train loss:  0.087031789124012  val loss:  0.06573338061571121\n",
      "min_val_loss_print 0.06573338061571121\n",
      "epoch:  5   step:  117   train loss:  0.08318493515253067  val loss:  0.06580150872468948\n",
      "epoch:  5   step:  118   train loss:  0.09854203462600708  val loss:  0.06521961092948914\n",
      "min_val_loss_print 0.06521961092948914\n",
      "epoch:  5   step:  119   train loss:  0.07138233631849289  val loss:  0.06543271243572235\n",
      "epoch:  5   step:  120   train loss:  0.082278311252594  val loss:  0.0651228278875351\n",
      "min_val_loss_print 0.0651228278875351\n",
      "epoch:  5   step:  121   train loss:  0.07323305308818817  val loss:  0.06437616795301437\n",
      "min_val_loss_print 0.06437616795301437\n",
      "epoch:  5   step:  122   train loss:  0.08484689146280289  val loss:  0.06458508968353271\n",
      "epoch:  5   step:  123   train loss:  0.06373707950115204  val loss:  0.06492364406585693\n",
      "epoch:  5   step:  124   train loss:  0.08121582865715027  val loss:  0.06465703248977661\n",
      "epoch:  5   step:  125   train loss:  0.0989554226398468  val loss:  0.06398942321538925\n",
      "min_val_loss_print 0.06398942321538925\n",
      "epoch:  5   step:  126   train loss:  0.0540970079600811  val loss:  0.06414098292589188\n",
      "epoch:  5   step:  127   train loss:  0.06737058609724045  val loss:  0.0627434030175209\n",
      "min_val_loss_print 0.0627434030175209\n",
      "epoch:  5   step:  128   train loss:  0.0669688805937767  val loss:  0.06309232860803604\n",
      "epoch:  5   step:  129   train loss:  0.10702836513519287  val loss:  0.06213054805994034\n",
      "min_val_loss_print 0.06213054805994034\n",
      "epoch:  5   step:  130   train loss:  0.06944166868925095  val loss:  0.0612788200378418\n",
      "min_val_loss_print 0.0612788200378418\n",
      "epoch:  5   step:  131   train loss:  0.0640634149312973  val loss:  0.06110628694295883\n",
      "min_val_loss_print 0.06110628694295883\n",
      "epoch:  5   step:  132   train loss:  0.061404451727867126  val loss:  0.06071752309799194\n",
      "min_val_loss_print 0.06071752309799194\n",
      "epoch:  5   step:  133   train loss:  0.06255161017179489  val loss:  0.059730276465415955\n",
      "min_val_loss_print 0.059730276465415955\n",
      "epoch:  5   step:  134   train loss:  0.09155593812465668  val loss:  0.05957741290330887\n",
      "min_val_loss_print 0.05957741290330887\n",
      "epoch:  5   step:  135   train loss:  0.07787258923053741  val loss:  0.059688303619623184\n",
      "epoch:  5   step:  136   train loss:  0.12392159551382065  val loss:  0.05888010188937187\n",
      "min_val_loss_print 0.05888010188937187\n",
      "epoch:  5   step:  137   train loss:  0.10529906302690506  val loss:  0.059315577149391174\n",
      "epoch:  5   step:  138   train loss:  0.05754225328564644  val loss:  0.059123795479536057\n",
      "epoch:  5   step:  139   train loss:  0.06915602833032608  val loss:  0.058843087404966354\n",
      "min_val_loss_print 0.058843087404966354\n",
      "epoch:  5   step:  140   train loss:  0.09414047002792358  val loss:  0.05918731540441513\n",
      "epoch:  5   step:  141   train loss:  0.09268277883529663  val loss:  0.06010269746184349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  5   step:  142   train loss:  0.06143008545041084  val loss:  0.06027825549244881\n",
      "epoch:  5   step:  143   train loss:  0.08268735557794571  val loss:  0.06053057685494423\n",
      "epoch:  5   step:  144   train loss:  0.07145681977272034  val loss:  0.06166717782616615\n",
      "epoch:  5   step:  145   train loss:  0.08084893971681595  val loss:  0.062258243560791016\n",
      "epoch:  5   step:  146   train loss:  0.04089831933379173  val loss:  0.06326886266469955\n",
      "epoch:  5   step:  147   train loss:  0.05301760882139206  val loss:  0.06305331736803055\n",
      "epoch:  5   step:  148   train loss:  0.08959176391363144  val loss:  0.06344091147184372\n",
      "epoch:  5   step:  149   train loss:  0.07438429445028305  val loss:  0.06393333524465561\n",
      "epoch:  5   step:  150   train loss:  0.10611969232559204  val loss:  0.06288864463567734\n",
      "epoch:  5   step:  151   train loss:  0.0720616951584816  val loss:  0.0627301037311554\n",
      "epoch:  5   step:  152   train loss:  0.11724421381950378  val loss:  0.06211443990468979\n",
      "epoch:  5   step:  153   train loss:  0.0790865421295166  val loss:  0.06187929958105087\n",
      "epoch:  5   step:  154   train loss:  0.08053839951753616  val loss:  0.06010017916560173\n",
      "epoch:  5   step:  155   train loss:  0.08372727036476135  val loss:  0.06011522188782692\n",
      "epoch:  5   step:  156   train loss:  0.11093203723430634  val loss:  0.06063127517700195\n",
      "epoch:  5   step:  157   train loss:  0.09588672965765  val loss:  0.061071351170539856\n",
      "epoch:  5   step:  158   train loss:  0.12738265097141266  val loss:  0.06045319512486458\n",
      "epoch:  5   step:  159   train loss:  0.08582651615142822  val loss:  0.06058569252490997\n",
      "epoch:  5   step:  160   train loss:  0.0875820741057396  val loss:  0.061001427471637726\n",
      "epoch:  5   step:  161   train loss:  0.06536292284727097  val loss:  0.06130463257431984\n",
      "epoch:  5   step:  162   train loss:  0.08137158304452896  val loss:  0.061737049371004105\n",
      "epoch:  5   step:  163   train loss:  0.07809402048587799  val loss:  0.06180497258901596\n",
      "epoch:  5   step:  164   train loss:  0.0894680768251419  val loss:  0.06237468868494034\n",
      "epoch:  5   step:  165   train loss:  0.05801292136311531  val loss:  0.06179099529981613\n",
      "epoch:  5   step:  166   train loss:  0.07850047200918198  val loss:  0.06207688897848129\n",
      "epoch:  5   step:  167   train loss:  0.06282316893339157  val loss:  0.061006318777799606\n",
      "epoch:  5   step:  168   train loss:  0.08434929698705673  val loss:  0.06144899129867554\n",
      "epoch:  5   step:  169   train loss:  0.08666003495454788  val loss:  0.06077367812395096\n",
      "epoch:  5   step:  170   train loss:  0.07698192447423935  val loss:  0.05936209484934807\n",
      "epoch:  5   step:  171   train loss:  0.0704437717795372  val loss:  0.05903693661093712\n",
      "epoch:  5   step:  172   train loss:  0.07977182418107986  val loss:  0.05926046893000603\n",
      "epoch:  5   step:  173   train loss:  0.08329223841428757  val loss:  0.059735383838415146\n",
      "epoch:  5   step:  174   train loss:  0.046565793454647064  val loss:  0.06011844798922539\n",
      "epoch:  5   step:  175   train loss:  0.07782087475061417  val loss:  0.05995684117078781\n",
      "epoch:  5   step:  176   train loss:  0.06417336314916611  val loss:  0.05928264185786247\n",
      "epoch:  5   step:  177   train loss:  0.10098201781511307  val loss:  0.05900556221604347\n",
      "epoch:  5   step:  178   train loss:  0.058311257511377335  val loss:  0.05898498371243477\n",
      "epoch:  5   step:  179   train loss:  0.13655203580856323  val loss:  0.05868369713425636\n",
      "min_val_loss_print 0.05868369713425636\n",
      "epoch:  5   step:  180   train loss:  0.10626892745494843  val loss:  0.058155376464128494\n",
      "min_val_loss_print 0.058155376464128494\n",
      "epoch:  5   step:  181   train loss:  0.07031119614839554  val loss:  0.058853715658187866\n",
      "epoch:  5   step:  182   train loss:  0.04981906712055206  val loss:  0.05867926403880119\n",
      "epoch:  5   step:  183   train loss:  0.06361966580152512  val loss:  0.05934610962867737\n",
      "epoch:  5   step:  184   train loss:  0.08632810413837433  val loss:  0.06011774763464928\n",
      "epoch:  5   step:  185   train loss:  0.07891497015953064  val loss:  0.06043943762779236\n",
      "epoch:  5   step:  186   train loss:  0.0507182814180851  val loss:  0.06067965179681778\n",
      "epoch:  5   step:  187   train loss:  0.07114072889089584  val loss:  0.06172821298241615\n",
      "epoch:  5   step:  188   train loss:  0.07604462653398514  val loss:  0.06283672153949738\n",
      "epoch:  5   step:  189   train loss:  0.09030024707317352  val loss:  0.06331661343574524\n",
      "epoch:  5   step:  190   train loss:  0.05813780799508095  val loss:  0.06314302980899811\n",
      "epoch:  5   step:  191   train loss:  0.08451549708843231  val loss:  0.06344295293092728\n",
      "epoch:  5   step:  192   train loss:  0.1260693073272705  val loss:  0.06331779807806015\n",
      "epoch:  5   step:  193   train loss:  0.06890677660703659  val loss:  0.06315713375806808\n",
      "epoch:  5   step:  194   train loss:  0.039442382752895355  val loss:  0.061700489372015\n",
      "epoch:  5   step:  195   train loss:  0.06863030046224594  val loss:  0.061655767261981964\n",
      "epoch:  5   step:  196   train loss:  0.06558270752429962  val loss:  0.06136122718453407\n",
      "epoch:  5   step:  197   train loss:  0.12907980382442474  val loss:  0.06167978048324585\n",
      "epoch:  5   step:  198   train loss:  0.07200358808040619  val loss:  0.06157933175563812\n",
      "epoch:  5   step:  199   train loss:  0.060279808938503265  val loss:  0.06145786866545677\n",
      "epoch:  5   step:  200   train loss:  0.06328735500574112  val loss:  0.061288271099328995\n",
      "epoch:  5   step:  201   train loss:  0.09587153047323227  val loss:  0.06142339110374451\n",
      "epoch:  5   step:  202   train loss:  0.06849584728479385  val loss:  0.06176003813743591\n",
      "epoch:  5   step:  203   train loss:  0.10072021931409836  val loss:  0.06260205060243607\n",
      "epoch:  5   step:  204   train loss:  0.09334009885787964  val loss:  0.06262341886758804\n",
      "epoch:  5   step:  205   train loss:  0.05943720415234566  val loss:  0.062028661370277405\n",
      "epoch:  5   step:  206   train loss:  0.046604808419942856  val loss:  0.061418548226356506\n",
      "epoch:  5   step:  207   train loss:  0.07609117776155472  val loss:  0.06053144112229347\n",
      "epoch:  5   step:  208   train loss:  0.06764275580644608  val loss:  0.06027214974164963\n",
      "epoch:  5   step:  209   train loss:  0.07716097682714462  val loss:  0.0599445179104805\n",
      "epoch:  5   step:  210   train loss:  0.09016302973031998  val loss:  0.05967424064874649\n",
      "epoch:  5   step:  211   train loss:  0.052702806890010834  val loss:  0.05949068441987038\n",
      "epoch:  5   step:  212   train loss:  0.12829367816448212  val loss:  0.05904523655772209\n",
      "epoch:  5   step:  213   train loss:  0.05293896421790123  val loss:  0.0602087676525116\n",
      "epoch:  5   step:  214   train loss:  0.08827156573534012  val loss:  0.0618525967001915\n",
      "epoch:  5   step:  215   train loss:  0.08304624259471893  val loss:  0.06150214001536369\n",
      "epoch:  5   step:  216   train loss:  0.06254225224256516  val loss:  0.06181327626109123\n",
      "epoch:  5   step:  217   train loss:  0.06542425602674484  val loss:  0.06340638548135757\n",
      "epoch:  5   step:  218   train loss:  0.05063750222325325  val loss:  0.06365692615509033\n",
      "epoch:  5   step:  219   train loss:  0.10460007190704346  val loss:  0.06340297311544418\n",
      "epoch:  5   step:  220   train loss:  0.04604856297373772  val loss:  0.06227815896272659\n",
      "epoch:  5   step:  221   train loss:  0.038509804755449295  val loss:  0.06132582202553749\n",
      "epoch:  5   step:  222   train loss:  0.050887104123830795  val loss:  0.06054038181900978\n",
      "epoch:  5   step:  223   train loss:  0.1034916341304779  val loss:  0.0603410005569458\n",
      "epoch:  5   step:  224   train loss:  0.0711282268166542  val loss:  0.05893486738204956\n",
      "epoch:  5   step:  225   train loss:  0.06562045961618423  val loss:  0.05813305452466011\n",
      "min_val_loss_print 0.05813305452466011\n",
      "epoch:  5   step:  226   train loss:  0.0778760090470314  val loss:  0.05735470727086067\n",
      "min_val_loss_print 0.05735470727086067\n",
      "epoch:  5   step:  227   train loss:  0.05642298609018326  val loss:  0.057512640953063965\n",
      "epoch:  5   step:  228   train loss:  0.07797323167324066  val loss:  0.058351729065179825\n",
      "epoch:  5   step:  229   train loss:  0.1268395632505417  val loss:  0.05841992422938347\n",
      "epoch:  5   step:  230   train loss:  0.0723642036318779  val loss:  0.05840807035565376\n",
      "epoch:  5   step:  231   train loss:  0.06578754633665085  val loss:  0.05871537700295448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  5   step:  232   train loss:  0.07043848186731339  val loss:  0.057963307946920395\n",
      "epoch:  5   step:  233   train loss:  0.05797212943434715  val loss:  0.05748375132679939\n",
      "epoch:  5   step:  234   train loss:  0.09825065732002258  val loss:  0.057406555861234665\n",
      "epoch:  5   step:  235   train loss:  0.09534953534603119  val loss:  0.0570257306098938\n",
      "min_val_loss_print 0.0570257306098938\n",
      "epoch:  5   step:  236   train loss:  0.09502281248569489  val loss:  0.055567920207977295\n",
      "min_val_loss_print 0.055567920207977295\n",
      "epoch:  5   step:  237   train loss:  0.07160626351833344  val loss:  0.055046070367097855\n",
      "min_val_loss_print 0.055046070367097855\n",
      "epoch:  5   step:  238   train loss:  0.05053982883691788  val loss:  0.05522885173559189\n",
      "epoch:  5   step:  239   train loss:  0.058722008019685745  val loss:  0.0541909895837307\n",
      "min_val_loss_print 0.0541909895837307\n",
      "epoch:  5   step:  240   train loss:  0.07584773004055023  val loss:  0.05368479713797569\n",
      "min_val_loss_print 0.05368479713797569\n",
      "epoch:  5   step:  241   train loss:  0.08787020295858383  val loss:  0.05448975786566734\n",
      "epoch:  5   step:  242   train loss:  0.07214178889989853  val loss:  0.05478135123848915\n",
      "epoch:  5   step:  243   train loss:  0.06662293523550034  val loss:  0.05568211153149605\n",
      "epoch:  5   step:  244   train loss:  0.06764296442270279  val loss:  0.054828669875860214\n",
      "epoch:  5   step:  245   train loss:  0.08878019452095032  val loss:  0.05454148352146149\n",
      "epoch:  5   step:  246   train loss:  0.05384064465761185  val loss:  0.054863717406988144\n",
      "epoch:  5   step:  247   train loss:  0.09177857637405396  val loss:  0.05548737570643425\n",
      "epoch:  5   step:  248   train loss:  0.07393652945756912  val loss:  0.055553216487169266\n",
      "epoch:  5   step:  249   train loss:  0.04652145132422447  val loss:  0.05574634671211243\n",
      "epoch:  5   step:  250   train loss:  0.10011989623308182  val loss:  0.05490536615252495\n",
      "epoch:  5   step:  251   train loss:  0.09113776683807373  val loss:  0.053568754345178604\n",
      "min_val_loss_print 0.053568754345178604\n",
      "epoch:  5   step:  252   train loss:  0.053491923958063126  val loss:  0.053974393755197525\n",
      "epoch:  5   step:  253   train loss:  0.0965428277850151  val loss:  0.05292126163840294\n",
      "min_val_loss_print 0.05292126163840294\n",
      "epoch:  5   step:  254   train loss:  0.07429861277341843  val loss:  0.052999693900346756\n",
      "epoch:  5   step:  255   train loss:  0.09393676370382309  val loss:  0.05360226705670357\n",
      "epoch:  5   step:  256   train loss:  0.11935550719499588  val loss:  0.05284000560641289\n",
      "min_val_loss_print 0.05284000560641289\n",
      "epoch:  5   step:  257   train loss:  0.06489743292331696  val loss:  0.05297689139842987\n",
      "epoch:  5   step:  258   train loss:  0.08065751194953918  val loss:  0.05290103703737259\n",
      "epoch:  5   step:  259   train loss:  0.08112698048353195  val loss:  0.05267196521162987\n",
      "min_val_loss_print 0.05267196521162987\n",
      "epoch:  5   step:  260   train loss:  0.07760447263717651  val loss:  0.053521376103162766\n",
      "epoch:  5   step:  261   train loss:  0.08314279466867447  val loss:  0.05284219980239868\n",
      "epoch:  5   step:  262   train loss:  0.06824052333831787  val loss:  0.05309759080410004\n",
      "epoch:  5   step:  263   train loss:  0.05935684219002724  val loss:  0.05322932451963425\n",
      "epoch:  5   step:  264   train loss:  0.061595622450113297  val loss:  0.05345319211483002\n",
      "epoch:  5   step:  265   train loss:  0.07844455540180206  val loss:  0.05363386496901512\n",
      "epoch:  5   step:  266   train loss:  0.08275321871042252  val loss:  0.05418376624584198\n",
      "epoch:  5   step:  267   train loss:  0.08394328504800797  val loss:  0.05450253561139107\n",
      "epoch:  5   step:  268   train loss:  0.05420650169253349  val loss:  0.055186569690704346\n",
      "epoch:  5   step:  269   train loss:  0.08869803696870804  val loss:  0.05475707724690437\n",
      "epoch:  5   step:  270   train loss:  0.058478426188230515  val loss:  0.05625415965914726\n",
      "epoch:  5   step:  271   train loss:  0.047564513981342316  val loss:  0.05557679757475853\n",
      "epoch:  5   step:  272   train loss:  0.09076206386089325  val loss:  0.0556207038462162\n",
      "epoch:  5   step:  273   train loss:  0.05600214749574661  val loss:  0.054348256438970566\n",
      "epoch:  5   step:  274   train loss:  0.06479804217815399  val loss:  0.053902383893728256\n",
      "epoch:  5   step:  275   train loss:  0.06905240565538406  val loss:  0.05382862314581871\n",
      "epoch:  5   step:  276   train loss:  0.07493661344051361  val loss:  0.053411439061164856\n",
      "epoch:  5   step:  277   train loss:  0.1273374855518341  val loss:  0.053283024579286575\n",
      "epoch:  5   step:  278   train loss:  0.053928956389427185  val loss:  0.05372204631567001\n",
      "epoch:  5   step:  279   train loss:  0.06424812972545624  val loss:  0.052610017359256744\n",
      "min_val_loss_print 0.052610017359256744\n",
      "epoch:  5   step:  280   train loss:  0.03964391350746155  val loss:  0.05194060504436493\n",
      "min_val_loss_print 0.05194060504436493\n",
      "epoch:  5   step:  281   train loss:  0.08482028543949127  val loss:  0.052625298500061035\n",
      "epoch:  5   step:  282   train loss:  0.05570681020617485  val loss:  0.05314035713672638\n",
      "epoch:  5   step:  283   train loss:  0.07076265662908554  val loss:  0.053300727158784866\n",
      "epoch:  5   step:  284   train loss:  0.06771105527877808  val loss:  0.0527942031621933\n",
      "epoch:  5   step:  285   train loss:  0.09249673783779144  val loss:  0.052412696182727814\n",
      "epoch:  5   step:  286   train loss:  0.05125577747821808  val loss:  0.05313100665807724\n",
      "epoch:  5   step:  287   train loss:  0.08396615087985992  val loss:  0.05276745185256004\n",
      "epoch:  5   step:  288   train loss:  0.09333034604787827  val loss:  0.05344323441386223\n",
      "epoch:  5   step:  289   train loss:  0.0939246267080307  val loss:  0.05339432880282402\n",
      "epoch:  5   step:  290   train loss:  0.03983651101589203  val loss:  0.05300551652908325\n",
      "epoch:  5   step:  291   train loss:  0.08894477784633636  val loss:  0.053562555462121964\n",
      "epoch:  5   step:  292   train loss:  0.07522187381982803  val loss:  0.053826913237571716\n",
      "epoch:  5   step:  293   train loss:  0.08090987056493759  val loss:  0.05381327494978905\n",
      "epoch:  5   step:  294   train loss:  0.10080856084823608  val loss:  0.05416387692093849\n",
      "epoch:  5   step:  295   train loss:  0.0989917442202568  val loss:  0.054703809320926666\n",
      "epoch:  5   step:  296   train loss:  0.0907856896519661  val loss:  0.05503985658288002\n",
      "epoch:  5   step:  297   train loss:  0.0658140555024147  val loss:  0.055366333574056625\n",
      "epoch:  5   step:  298   train loss:  0.06642520427703857  val loss:  0.05488915741443634\n",
      "epoch:  5   step:  299   train loss:  0.08348715305328369  val loss:  0.055640216916799545\n",
      "epoch:  5   step:  300   train loss:  0.07412277162075043  val loss:  0.055668190121650696\n",
      "epoch:  5   step:  301   train loss:  0.08910226821899414  val loss:  0.05628573149442673\n",
      "epoch:  5   step:  302   train loss:  0.044850826263427734  val loss:  0.057048242539167404\n",
      "epoch:  5   step:  303   train loss:  0.11233684420585632  val loss:  0.05802266299724579\n",
      "epoch:  5   step:  304   train loss:  0.14424286782741547  val loss:  0.057390131056308746\n",
      "epoch:  5   step:  305   train loss:  0.10664732754230499  val loss:  0.057080820202827454\n",
      "epoch:  5   step:  306   train loss:  0.07984639704227448  val loss:  0.05624423921108246\n",
      "epoch:  5   step:  307   train loss:  0.0357680507004261  val loss:  0.05484450235962868\n",
      "epoch:  5   step:  308   train loss:  0.0982804149389267  val loss:  0.05440260469913483\n",
      "epoch:  5   step:  309   train loss:  0.06687391549348831  val loss:  0.054147299379110336\n",
      "epoch:  5   step:  310   train loss:  0.056630369275808334  val loss:  0.05452708154916763\n",
      "epoch:  5   step:  311   train loss:  0.0678003802895546  val loss:  0.05423011630773544\n",
      "epoch:  5   step:  312   train loss:  0.10945103317499161  val loss:  0.05306323990225792\n",
      "epoch:  5   step:  313   train loss:  0.10468447953462601  val loss:  0.0518631711602211\n",
      "min_val_loss_print 0.0518631711602211\n",
      "epoch:  5   step:  314   train loss:  0.09399549663066864  val loss:  0.05178064480423927\n",
      "min_val_loss_print 0.05178064480423927\n",
      "epoch:  5   step:  315   train loss:  0.054481107741594315  val loss:  0.053289853036403656\n",
      "epoch:  5   step:  316   train loss:  0.06606855988502502  val loss:  0.05468222126364708\n",
      "epoch:  5   step:  317   train loss:  0.08517804741859436  val loss:  0.055017080157995224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  5   step:  318   train loss:  0.0778324156999588  val loss:  0.0555032379925251\n",
      "epoch:  5   step:  319   train loss:  0.05457951873540878  val loss:  0.055193595588207245\n",
      "epoch:  5   step:  320   train loss:  0.08736102283000946  val loss:  0.056716375052928925\n",
      "epoch:  5   step:  321   train loss:  0.048726338893175125  val loss:  0.056070517748594284\n",
      "epoch:  5   step:  322   train loss:  0.09518507122993469  val loss:  0.05666413530707359\n",
      "epoch:  5   step:  323   train loss:  0.09097661077976227  val loss:  0.056444596499204636\n",
      "epoch:  5   step:  324   train loss:  0.09758386015892029  val loss:  0.057461392134428024\n",
      "epoch:  5   step:  325   train loss:  0.07346230000257492  val loss:  0.05698047950863838\n",
      "epoch:  5   step:  326   train loss:  0.06733280420303345  val loss:  0.05688866600394249\n",
      "epoch:  5   step:  327   train loss:  0.07217951864004135  val loss:  0.05565151572227478\n",
      "epoch:  5   step:  328   train loss:  0.03746625408530235  val loss:  0.055397383868694305\n",
      "epoch:  5   step:  329   train loss:  0.07162991166114807  val loss:  0.055308885872364044\n",
      "epoch:  5   step:  330   train loss:  0.057388510555028915  val loss:  0.054798927158117294\n",
      "epoch:  5   step:  331   train loss:  0.05112290009856224  val loss:  0.05455492064356804\n",
      "epoch:  5   step:  332   train loss:  0.09755925089120865  val loss:  0.05367901921272278\n",
      "epoch:  5   step:  333   train loss:  0.05729031562805176  val loss:  0.054514892399311066\n",
      "epoch:  5   step:  334   train loss:  0.037015482783317566  val loss:  0.054079052060842514\n",
      "epoch:  5   step:  335   train loss:  0.04382767155766487  val loss:  0.053738728165626526\n",
      "epoch:  5   step:  336   train loss:  0.07627612352371216  val loss:  0.05350330099463463\n",
      "epoch:  5   step:  337   train loss:  0.06391136348247528  val loss:  0.05326974019408226\n",
      "epoch:  5   step:  338   train loss:  0.056335434317588806  val loss:  0.053780119866132736\n",
      "epoch:  5   step:  339   train loss:  0.09189622849225998  val loss:  0.05399303138256073\n",
      "epoch:  5   step:  340   train loss:  0.0738854631781578  val loss:  0.05229216814041138\n",
      "epoch:  5   step:  341   train loss:  0.08941343426704407  val loss:  0.052591223269701004\n",
      "epoch:  5   step:  342   train loss:  0.06374142318964005  val loss:  0.05300569906830788\n",
      "epoch:  5   step:  343   train loss:  0.04782360792160034  val loss:  0.0537736751139164\n",
      "epoch:  6   step:  0   train loss:  0.07006289809942245  val loss:  0.052384231239557266\n",
      "epoch:  6   step:  1   train loss:  0.055703505873680115  val loss:  0.05208529531955719\n",
      "epoch:  6   step:  2   train loss:  0.06012597307562828  val loss:  0.051340751349925995\n",
      "min_val_loss_print 0.051340751349925995\n",
      "epoch:  6   step:  3   train loss:  0.07723751664161682  val loss:  0.05156361311674118\n",
      "epoch:  6   step:  4   train loss:  0.05398676544427872  val loss:  0.05093161016702652\n",
      "min_val_loss_print 0.05093161016702652\n",
      "epoch:  6   step:  5   train loss:  0.06543724983930588  val loss:  0.05022937059402466\n",
      "min_val_loss_print 0.05022937059402466\n",
      "epoch:  6   step:  6   train loss:  0.037430837750434875  val loss:  0.05023610219359398\n",
      "epoch:  6   step:  7   train loss:  0.0674649178981781  val loss:  0.051352668553590775\n",
      "epoch:  6   step:  8   train loss:  0.07869872450828552  val loss:  0.05068935081362724\n",
      "epoch:  6   step:  9   train loss:  0.040301043540239334  val loss:  0.05132375285029411\n",
      "epoch:  6   step:  10   train loss:  0.1126377284526825  val loss:  0.052145715802907944\n",
      "epoch:  6   step:  11   train loss:  0.07322385162115097  val loss:  0.052304938435554504\n",
      "epoch:  6   step:  12   train loss:  0.05927904322743416  val loss:  0.053239788860082626\n",
      "epoch:  6   step:  13   train loss:  0.06004419922828674  val loss:  0.05162399634718895\n",
      "epoch:  6   step:  14   train loss:  0.07003769278526306  val loss:  0.051412567496299744\n",
      "epoch:  6   step:  15   train loss:  0.059824634343385696  val loss:  0.05124076455831528\n",
      "epoch:  6   step:  16   train loss:  0.052227772772312164  val loss:  0.05149555578827858\n",
      "epoch:  6   step:  17   train loss:  0.04137242212891579  val loss:  0.05136259272694588\n",
      "epoch:  6   step:  18   train loss:  0.07923974841833115  val loss:  0.049987826496362686\n",
      "min_val_loss_print 0.049987826496362686\n",
      "epoch:  6   step:  19   train loss:  0.06024390086531639  val loss:  0.04855458810925484\n",
      "min_val_loss_print 0.04855458810925484\n",
      "epoch:  6   step:  20   train loss:  0.06519769132137299  val loss:  0.0488518625497818\n",
      "epoch:  6   step:  21   train loss:  0.08329077064990997  val loss:  0.04870688170194626\n",
      "epoch:  6   step:  22   train loss:  0.07188086956739426  val loss:  0.048967935144901276\n",
      "epoch:  6   step:  23   train loss:  0.04340340942144394  val loss:  0.049019284546375275\n",
      "epoch:  6   step:  24   train loss:  0.13534948229789734  val loss:  0.0493045337498188\n",
      "epoch:  6   step:  25   train loss:  0.09411770850419998  val loss:  0.04875464364886284\n",
      "epoch:  6   step:  26   train loss:  0.07234954833984375  val loss:  0.04851992055773735\n",
      "min_val_loss_print 0.04851992055773735\n",
      "epoch:  6   step:  27   train loss:  0.05707860365509987  val loss:  0.04894912242889404\n",
      "epoch:  6   step:  28   train loss:  0.06448344141244888  val loss:  0.04871322214603424\n",
      "epoch:  6   step:  29   train loss:  0.04780566319823265  val loss:  0.04936154931783676\n",
      "epoch:  6   step:  30   train loss:  0.05908693000674248  val loss:  0.04853305220603943\n",
      "epoch:  6   step:  31   train loss:  0.07038120180368423  val loss:  0.04829521104693413\n",
      "min_val_loss_print 0.04829521104693413\n",
      "epoch:  6   step:  32   train loss:  0.069870226085186  val loss:  0.04931105300784111\n",
      "epoch:  6   step:  33   train loss:  0.07072973996400833  val loss:  0.04932713136076927\n",
      "epoch:  6   step:  34   train loss:  0.0641501396894455  val loss:  0.04996887594461441\n",
      "epoch:  6   step:  35   train loss:  0.05675840377807617  val loss:  0.04970541596412659\n",
      "epoch:  6   step:  36   train loss:  0.06512364745140076  val loss:  0.04923000931739807\n",
      "epoch:  6   step:  37   train loss:  0.043814390897750854  val loss:  0.04852294921875\n",
      "epoch:  6   step:  38   train loss:  0.06878539174795151  val loss:  0.049276258796453476\n",
      "epoch:  6   step:  39   train loss:  0.06511711329221725  val loss:  0.04947850480675697\n",
      "epoch:  6   step:  40   train loss:  0.07427739351987839  val loss:  0.048912905156612396\n",
      "epoch:  6   step:  41   train loss:  0.07219136506319046  val loss:  0.04880930483341217\n",
      "epoch:  6   step:  42   train loss:  0.04418618977069855  val loss:  0.04873087257146835\n",
      "epoch:  6   step:  43   train loss:  0.0649731382727623  val loss:  0.048410654067993164\n",
      "epoch:  6   step:  44   train loss:  0.09835832566022873  val loss:  0.04903272166848183\n",
      "epoch:  6   step:  45   train loss:  0.040827494114637375  val loss:  0.0494086891412735\n",
      "epoch:  6   step:  46   train loss:  0.08755099028348923  val loss:  0.05044952780008316\n",
      "epoch:  6   step:  47   train loss:  0.06958965212106705  val loss:  0.049963511526584625\n",
      "epoch:  6   step:  48   train loss:  0.0635232999920845  val loss:  0.05075371265411377\n",
      "epoch:  6   step:  49   train loss:  0.04074420407414436  val loss:  0.05082003027200699\n",
      "epoch:  6   step:  50   train loss:  0.0602361261844635  val loss:  0.05059560015797615\n",
      "epoch:  6   step:  51   train loss:  0.08532097935676575  val loss:  0.04985202103853226\n",
      "epoch:  6   step:  52   train loss:  0.07109066843986511  val loss:  0.049900420010089874\n",
      "epoch:  6   step:  53   train loss:  0.0625489354133606  val loss:  0.050167426466941833\n",
      "epoch:  6   step:  54   train loss:  0.053576018661260605  val loss:  0.050407525151968\n",
      "epoch:  6   step:  55   train loss:  0.05002705752849579  val loss:  0.05005738139152527\n",
      "epoch:  6   step:  56   train loss:  0.05826086178421974  val loss:  0.049554359167814255\n",
      "epoch:  6   step:  57   train loss:  0.05297766998410225  val loss:  0.050086840987205505\n",
      "epoch:  6   step:  58   train loss:  0.060277339071035385  val loss:  0.050198085606098175\n",
      "epoch:  6   step:  59   train loss:  0.0688994899392128  val loss:  0.05027974769473076\n",
      "epoch:  6   step:  60   train loss:  0.044374555349349976  val loss:  0.04939989373087883\n",
      "epoch:  6   step:  61   train loss:  0.06414160132408142  val loss:  0.04932291433215141\n",
      "epoch:  6   step:  62   train loss:  0.06301621347665787  val loss:  0.04987046867609024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  6   step:  63   train loss:  0.0634174570441246  val loss:  0.050296030938625336\n",
      "epoch:  6   step:  64   train loss:  0.07411430776119232  val loss:  0.049625277519226074\n",
      "epoch:  6   step:  65   train loss:  0.08927921205759048  val loss:  0.04995403066277504\n",
      "epoch:  6   step:  66   train loss:  0.04544072225689888  val loss:  0.05030249059200287\n",
      "epoch:  6   step:  67   train loss:  0.06246095523238182  val loss:  0.05037030577659607\n",
      "epoch:  6   step:  68   train loss:  0.05368421599268913  val loss:  0.050313469022512436\n",
      "epoch:  6   step:  69   train loss:  0.06235836446285248  val loss:  0.05094119533896446\n",
      "epoch:  6   step:  70   train loss:  0.07425630837678909  val loss:  0.051552191376686096\n",
      "epoch:  6   step:  71   train loss:  0.06446012854576111  val loss:  0.051037129014730453\n",
      "epoch:  6   step:  72   train loss:  0.07623758912086487  val loss:  0.051062554121017456\n",
      "epoch:  6   step:  73   train loss:  0.07181600481271744  val loss:  0.05072614550590515\n",
      "epoch:  6   step:  74   train loss:  0.09102565795183182  val loss:  0.0499725416302681\n",
      "epoch:  6   step:  75   train loss:  0.07186621427536011  val loss:  0.05029584839940071\n",
      "epoch:  6   step:  76   train loss:  0.07552981376647949  val loss:  0.050126705318689346\n",
      "epoch:  6   step:  77   train loss:  0.08725061267614365  val loss:  0.04897259548306465\n",
      "epoch:  6   step:  78   train loss:  0.06151202693581581  val loss:  0.04892996326088905\n",
      "epoch:  6   step:  79   train loss:  0.06560534238815308  val loss:  0.04857862368226051\n",
      "epoch:  6   step:  80   train loss:  0.08850537985563278  val loss:  0.04763243347406387\n",
      "min_val_loss_print 0.04763243347406387\n",
      "epoch:  6   step:  81   train loss:  0.10075130313634872  val loss:  0.047998640686273575\n",
      "epoch:  6   step:  82   train loss:  0.06603281944990158  val loss:  0.047844648361206055\n",
      "epoch:  6   step:  83   train loss:  0.11281747370958328  val loss:  0.04851945489645004\n",
      "epoch:  6   step:  84   train loss:  0.0632515475153923  val loss:  0.04940171167254448\n",
      "epoch:  6   step:  85   train loss:  0.06932767480611801  val loss:  0.049540434032678604\n",
      "epoch:  6   step:  86   train loss:  0.05854874849319458  val loss:  0.04960925877094269\n",
      "epoch:  6   step:  87   train loss:  0.08978271484375  val loss:  0.04953722655773163\n",
      "epoch:  6   step:  88   train loss:  0.07298851013183594  val loss:  0.048703551292419434\n",
      "epoch:  6   step:  89   train loss:  0.06084829196333885  val loss:  0.04942847788333893\n",
      "epoch:  6   step:  90   train loss:  0.09337606281042099  val loss:  0.04964740201830864\n",
      "epoch:  6   step:  91   train loss:  0.05184477940201759  val loss:  0.04932652413845062\n",
      "epoch:  6   step:  92   train loss:  0.04130418598651886  val loss:  0.048388782888650894\n",
      "epoch:  6   step:  93   train loss:  0.06276845932006836  val loss:  0.048243939876556396\n",
      "epoch:  6   step:  94   train loss:  0.06767544895410538  val loss:  0.04825592413544655\n",
      "epoch:  6   step:  95   train loss:  0.08077473938465118  val loss:  0.04799419268965721\n",
      "epoch:  6   step:  96   train loss:  0.05113012716174126  val loss:  0.047990452498197556\n",
      "epoch:  6   step:  97   train loss:  0.05340564623475075  val loss:  0.04792378842830658\n",
      "epoch:  6   step:  98   train loss:  0.05603323131799698  val loss:  0.04786242917180061\n",
      "epoch:  6   step:  99   train loss:  0.06993863731622696  val loss:  0.04846930131316185\n",
      "epoch:  6   step:  100   train loss:  0.04711569473147392  val loss:  0.04848337173461914\n",
      "epoch:  6   step:  101   train loss:  0.04575416073203087  val loss:  0.04901689291000366\n",
      "epoch:  6   step:  102   train loss:  0.07837359607219696  val loss:  0.04890095442533493\n",
      "epoch:  6   step:  103   train loss:  0.04206516966223717  val loss:  0.04894953593611717\n",
      "epoch:  6   step:  104   train loss:  0.07107608020305634  val loss:  0.048880208283662796\n",
      "epoch:  6   step:  105   train loss:  0.054613396525382996  val loss:  0.04833183437585831\n",
      "epoch:  6   step:  106   train loss:  0.06856253743171692  val loss:  0.04733250290155411\n",
      "min_val_loss_print 0.04733250290155411\n",
      "epoch:  6   step:  107   train loss:  0.08398378640413284  val loss:  0.04714571684598923\n",
      "min_val_loss_print 0.04714571684598923\n",
      "epoch:  6   step:  108   train loss:  0.03528580442070961  val loss:  0.04697022959589958\n",
      "min_val_loss_print 0.04697022959589958\n",
      "epoch:  6   step:  109   train loss:  0.06106480211019516  val loss:  0.047065652906894684\n",
      "epoch:  6   step:  110   train loss:  0.03978336602449417  val loss:  0.046759508550167084\n",
      "min_val_loss_print 0.046759508550167084\n",
      "epoch:  6   step:  111   train loss:  0.07083533704280853  val loss:  0.045935969799757004\n",
      "min_val_loss_print 0.045935969799757004\n",
      "epoch:  6   step:  112   train loss:  0.06396059691905975  val loss:  0.04553255811333656\n",
      "min_val_loss_print 0.04553255811333656\n",
      "epoch:  6   step:  113   train loss:  0.046631745994091034  val loss:  0.04547414928674698\n",
      "min_val_loss_print 0.04547414928674698\n",
      "epoch:  6   step:  114   train loss:  0.05603557080030441  val loss:  0.04508591443300247\n",
      "min_val_loss_print 0.04508591443300247\n",
      "epoch:  6   step:  115   train loss:  0.09068356454372406  val loss:  0.04544908180832863\n",
      "epoch:  6   step:  116   train loss:  0.07392967492341995  val loss:  0.04535280913114548\n",
      "epoch:  6   step:  117   train loss:  0.0460704080760479  val loss:  0.04629581421613693\n",
      "epoch:  6   step:  118   train loss:  0.055872268974781036  val loss:  0.04562082886695862\n",
      "epoch:  6   step:  119   train loss:  0.07104519009590149  val loss:  0.045387670397758484\n",
      "epoch:  6   step:  120   train loss:  0.06172660365700722  val loss:  0.045707933604717255\n",
      "epoch:  6   step:  121   train loss:  0.061111047863960266  val loss:  0.046212613582611084\n",
      "epoch:  6   step:  122   train loss:  0.06280311197042465  val loss:  0.04668072983622551\n",
      "epoch:  6   step:  123   train loss:  0.05880449339747429  val loss:  0.04763410612940788\n",
      "epoch:  6   step:  124   train loss:  0.07020347565412521  val loss:  0.04763732850551605\n",
      "epoch:  6   step:  125   train loss:  0.060568515211343765  val loss:  0.04749341681599617\n",
      "epoch:  6   step:  126   train loss:  0.11934148520231247  val loss:  0.04791127145290375\n",
      "epoch:  6   step:  127   train loss:  0.045707423239946365  val loss:  0.04772242158651352\n",
      "epoch:  6   step:  128   train loss:  0.050517529249191284  val loss:  0.047559019178152084\n",
      "epoch:  6   step:  129   train loss:  0.0793800950050354  val loss:  0.04734528809785843\n",
      "epoch:  6   step:  130   train loss:  0.05802180990576744  val loss:  0.04820052161812782\n",
      "epoch:  6   step:  131   train loss:  0.04661799594759941  val loss:  0.04850529134273529\n",
      "epoch:  6   step:  132   train loss:  0.046214960515499115  val loss:  0.04814157634973526\n",
      "epoch:  6   step:  133   train loss:  0.06003056839108467  val loss:  0.047765571624040604\n",
      "epoch:  6   step:  134   train loss:  0.07020511478185654  val loss:  0.04836715757846832\n",
      "epoch:  6   step:  135   train loss:  0.04726077616214752  val loss:  0.048160288482904434\n",
      "epoch:  6   step:  136   train loss:  0.07065491378307343  val loss:  0.048547495156526566\n",
      "epoch:  6   step:  137   train loss:  0.08040491491556168  val loss:  0.048462361097335815\n",
      "epoch:  6   step:  138   train loss:  0.06213279813528061  val loss:  0.0485684871673584\n",
      "epoch:  6   step:  139   train loss:  0.06153678148984909  val loss:  0.048045091331005096\n",
      "epoch:  6   step:  140   train loss:  0.08168558776378632  val loss:  0.04785803332924843\n",
      "epoch:  6   step:  141   train loss:  0.06041230633854866  val loss:  0.0480530820786953\n",
      "epoch:  6   step:  142   train loss:  0.04842444509267807  val loss:  0.04877404123544693\n",
      "epoch:  6   step:  143   train loss:  0.06268000602722168  val loss:  0.04945943132042885\n",
      "epoch:  6   step:  144   train loss:  0.06346997618675232  val loss:  0.049531836062669754\n",
      "epoch:  6   step:  145   train loss:  0.06194322556257248  val loss:  0.048384495079517365\n",
      "epoch:  6   step:  146   train loss:  0.04949650540947914  val loss:  0.048119571059942245\n",
      "epoch:  6   step:  147   train loss:  0.029565727338194847  val loss:  0.04732757806777954\n",
      "epoch:  6   step:  148   train loss:  0.05999068543314934  val loss:  0.04678092151880264\n",
      "epoch:  6   step:  149   train loss:  0.05226220563054085  val loss:  0.04705902189016342\n",
      "epoch:  6   step:  150   train loss:  0.06711186468601227  val loss:  0.04692346602678299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  6   step:  151   train loss:  0.04604894295334816  val loss:  0.04625255987048149\n",
      "epoch:  6   step:  152   train loss:  0.04344930499792099  val loss:  0.04692280292510986\n",
      "epoch:  6   step:  153   train loss:  0.06107117608189583  val loss:  0.047452397644519806\n",
      "epoch:  6   step:  154   train loss:  0.053102556616067886  val loss:  0.04760913923382759\n",
      "epoch:  6   step:  155   train loss:  0.059460606426000595  val loss:  0.047877199947834015\n",
      "epoch:  6   step:  156   train loss:  0.10745921730995178  val loss:  0.04794418066740036\n",
      "epoch:  6   step:  157   train loss:  0.07065243273973465  val loss:  0.04842285439372063\n",
      "epoch:  6   step:  158   train loss:  0.08670113235712051  val loss:  0.0480433814227581\n",
      "epoch:  6   step:  159   train loss:  0.039299678057432175  val loss:  0.04781264066696167\n",
      "epoch:  6   step:  160   train loss:  0.058067288249731064  val loss:  0.047013312578201294\n",
      "epoch:  6   step:  161   train loss:  0.07250107824802399  val loss:  0.04695018753409386\n",
      "epoch:  6   step:  162   train loss:  0.07549463212490082  val loss:  0.0472462959587574\n",
      "epoch:  6   step:  163   train loss:  0.07245057821273804  val loss:  0.047425057739019394\n",
      "epoch:  6   step:  164   train loss:  0.04684515297412872  val loss:  0.04752526059746742\n",
      "epoch:  6   step:  165   train loss:  0.051834579557180405  val loss:  0.047244712710380554\n",
      "epoch:  6   step:  166   train loss:  0.04851922765374184  val loss:  0.04644698277115822\n",
      "epoch:  6   step:  167   train loss:  0.06462922692298889  val loss:  0.046153392642736435\n",
      "epoch:  6   step:  168   train loss:  0.08076513558626175  val loss:  0.046995989978313446\n",
      "epoch:  6   step:  169   train loss:  0.06719160825014114  val loss:  0.0474720373749733\n",
      "epoch:  6   step:  170   train loss:  0.03639581799507141  val loss:  0.047072768211364746\n",
      "epoch:  6   step:  171   train loss:  0.04756908863782883  val loss:  0.047662027180194855\n",
      "epoch:  6   step:  172   train loss:  0.06472326070070267  val loss:  0.04675862938165665\n",
      "epoch:  6   step:  173   train loss:  0.062248505651950836  val loss:  0.04658793285489082\n",
      "epoch:  6   step:  174   train loss:  0.08228977024555206  val loss:  0.04559699073433876\n",
      "epoch:  6   step:  175   train loss:  0.052960269153118134  val loss:  0.04546191915869713\n",
      "epoch:  6   step:  176   train loss:  0.09098079800605774  val loss:  0.04553090035915375\n",
      "epoch:  6   step:  177   train loss:  0.05897962301969528  val loss:  0.045077770948410034\n",
      "min_val_loss_print 0.045077770948410034\n",
      "epoch:  6   step:  178   train loss:  0.05796351656317711  val loss:  0.04485912248492241\n",
      "min_val_loss_print 0.04485912248492241\n",
      "epoch:  6   step:  179   train loss:  0.06394129246473312  val loss:  0.04481833428144455\n",
      "min_val_loss_print 0.04481833428144455\n",
      "epoch:  6   step:  180   train loss:  0.04247191175818443  val loss:  0.044735368341207504\n",
      "min_val_loss_print 0.044735368341207504\n",
      "epoch:  6   step:  181   train loss:  0.045346278697252274  val loss:  0.04471072182059288\n",
      "min_val_loss_print 0.04471072182059288\n",
      "epoch:  6   step:  182   train loss:  0.04833529144525528  val loss:  0.043913695961236954\n",
      "min_val_loss_print 0.043913695961236954\n",
      "epoch:  6   step:  183   train loss:  0.0805327445268631  val loss:  0.04371396824717522\n",
      "min_val_loss_print 0.04371396824717522\n",
      "epoch:  6   step:  184   train loss:  0.041489943861961365  val loss:  0.04319994896650314\n",
      "min_val_loss_print 0.04319994896650314\n",
      "epoch:  6   step:  185   train loss:  0.06526273488998413  val loss:  0.04422147572040558\n",
      "epoch:  6   step:  186   train loss:  0.04982392117381096  val loss:  0.04473172500729561\n",
      "epoch:  6   step:  187   train loss:  0.05545229837298393  val loss:  0.04444633796811104\n",
      "epoch:  6   step:  188   train loss:  0.04032598063349724  val loss:  0.044605374336242676\n",
      "epoch:  6   step:  189   train loss:  0.08381681889295578  val loss:  0.04433366283774376\n",
      "epoch:  6   step:  190   train loss:  0.07886498421430588  val loss:  0.044282156974077225\n",
      "epoch:  6   step:  191   train loss:  0.061144184321165085  val loss:  0.04398070275783539\n",
      "epoch:  6   step:  192   train loss:  0.041957829147577286  val loss:  0.04428291320800781\n",
      "epoch:  6   step:  193   train loss:  0.07497381418943405  val loss:  0.04385874792933464\n",
      "epoch:  6   step:  194   train loss:  0.04123372212052345  val loss:  0.04418349638581276\n",
      "epoch:  6   step:  195   train loss:  0.060240402817726135  val loss:  0.04466034844517708\n",
      "epoch:  6   step:  196   train loss:  0.09098517149686813  val loss:  0.04464937373995781\n",
      "epoch:  6   step:  197   train loss:  0.06610463559627533  val loss:  0.044899340718984604\n",
      "epoch:  6   step:  198   train loss:  0.0649413987994194  val loss:  0.044722430408000946\n",
      "epoch:  6   step:  199   train loss:  0.0551140159368515  val loss:  0.044980358332395554\n",
      "epoch:  6   step:  200   train loss:  0.06197110190987587  val loss:  0.045383427292108536\n",
      "epoch:  6   step:  201   train loss:  0.04522847756743431  val loss:  0.046046048402786255\n",
      "epoch:  6   step:  202   train loss:  0.06497995555400848  val loss:  0.04603441432118416\n",
      "epoch:  6   step:  203   train loss:  0.0628548264503479  val loss:  0.04640188068151474\n",
      "epoch:  6   step:  204   train loss:  0.07334984093904495  val loss:  0.04747479036450386\n",
      "epoch:  6   step:  205   train loss:  0.04682174324989319  val loss:  0.047302793711423874\n",
      "epoch:  6   step:  206   train loss:  0.10094466805458069  val loss:  0.04708782956004143\n",
      "epoch:  6   step:  207   train loss:  0.053103722631931305  val loss:  0.046517498791217804\n",
      "epoch:  6   step:  208   train loss:  0.08817091584205627  val loss:  0.04624984413385391\n",
      "epoch:  6   step:  209   train loss:  0.06376735121011734  val loss:  0.04663633182644844\n",
      "epoch:  6   step:  210   train loss:  0.08650380373001099  val loss:  0.046304408460855484\n",
      "epoch:  6   step:  211   train loss:  0.049146395176649094  val loss:  0.04635954275727272\n",
      "epoch:  6   step:  212   train loss:  0.1187305673956871  val loss:  0.04684353619813919\n",
      "epoch:  6   step:  213   train loss:  0.05448683723807335  val loss:  0.04748709127306938\n",
      "epoch:  6   step:  214   train loss:  0.04330769553780556  val loss:  0.04724031314253807\n",
      "epoch:  6   step:  215   train loss:  0.05998780578374863  val loss:  0.047484081238508224\n",
      "epoch:  6   step:  216   train loss:  0.07081678509712219  val loss:  0.0481606163084507\n",
      "epoch:  6   step:  217   train loss:  0.05792442709207535  val loss:  0.04766220599412918\n",
      "epoch:  6   step:  218   train loss:  0.06456480175256729  val loss:  0.047316551208496094\n",
      "epoch:  6   step:  219   train loss:  0.04202886298298836  val loss:  0.046704068779945374\n",
      "epoch:  6   step:  220   train loss:  0.06777358055114746  val loss:  0.04622894525527954\n",
      "epoch:  6   step:  221   train loss:  0.04506111517548561  val loss:  0.04564810171723366\n",
      "epoch:  6   step:  222   train loss:  0.06287380307912827  val loss:  0.04575138911604881\n",
      "epoch:  6   step:  223   train loss:  0.06814084202051163  val loss:  0.0450785793364048\n",
      "epoch:  6   step:  224   train loss:  0.06333139538764954  val loss:  0.044086575508117676\n",
      "epoch:  6   step:  225   train loss:  0.06998420506715775  val loss:  0.04324159771203995\n",
      "epoch:  6   step:  226   train loss:  0.05373866483569145  val loss:  0.04276655986905098\n",
      "min_val_loss_print 0.04276655986905098\n",
      "epoch:  6   step:  227   train loss:  0.09544122219085693  val loss:  0.04229345545172691\n",
      "min_val_loss_print 0.04229345545172691\n",
      "epoch:  6   step:  228   train loss:  0.051145050674676895  val loss:  0.04190661013126373\n",
      "min_val_loss_print 0.04190661013126373\n",
      "epoch:  6   step:  229   train loss:  0.07029572874307632  val loss:  0.04117831960320473\n",
      "min_val_loss_print 0.04117831960320473\n",
      "epoch:  6   step:  230   train loss:  0.0587228424847126  val loss:  0.04149103909730911\n",
      "epoch:  6   step:  231   train loss:  0.050617512315511703  val loss:  0.04128478467464447\n",
      "epoch:  6   step:  232   train loss:  0.034750647842884064  val loss:  0.04140963777899742\n",
      "epoch:  6   step:  233   train loss:  0.04364242032170296  val loss:  0.04059641435742378\n",
      "min_val_loss_print 0.04059641435742378\n",
      "epoch:  6   step:  234   train loss:  0.09898011386394501  val loss:  0.04089970141649246\n",
      "epoch:  6   step:  235   train loss:  0.08560418337583542  val loss:  0.040459636598825455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_val_loss_print 0.040459636598825455\n",
      "epoch:  6   step:  236   train loss:  0.06611404567956924  val loss:  0.040090080350637436\n",
      "min_val_loss_print 0.040090080350637436\n",
      "epoch:  6   step:  237   train loss:  0.06891046464443207  val loss:  0.04006130248308182\n",
      "min_val_loss_print 0.04006130248308182\n",
      "epoch:  6   step:  238   train loss:  0.09200349450111389  val loss:  0.040471382439136505\n",
      "epoch:  6   step:  239   train loss:  0.04583170637488365  val loss:  0.04105379432439804\n",
      "epoch:  6   step:  240   train loss:  0.04615943506360054  val loss:  0.0411706268787384\n",
      "epoch:  6   step:  241   train loss:  0.0448831282556057  val loss:  0.04139265790581703\n",
      "epoch:  6   step:  242   train loss:  0.08125578612089157  val loss:  0.04259096086025238\n",
      "epoch:  6   step:  243   train loss:  0.04767322540283203  val loss:  0.04270394146442413\n",
      "epoch:  6   step:  244   train loss:  0.08183165639638901  val loss:  0.04363297298550606\n",
      "epoch:  6   step:  245   train loss:  0.06417767703533173  val loss:  0.04468763247132301\n",
      "epoch:  6   step:  246   train loss:  0.0507325679063797  val loss:  0.04619620740413666\n",
      "epoch:  6   step:  247   train loss:  0.06650587916374207  val loss:  0.04487008973956108\n",
      "epoch:  6   step:  248   train loss:  0.053694434463977814  val loss:  0.045796070247888565\n",
      "epoch:  6   step:  249   train loss:  0.06850678473711014  val loss:  0.04683735594153404\n",
      "epoch:  6   step:  250   train loss:  0.06495526432991028  val loss:  0.047174397855997086\n",
      "epoch:  6   step:  251   train loss:  0.09639303386211395  val loss:  0.047784559428691864\n",
      "epoch:  6   step:  252   train loss:  0.03771299123764038  val loss:  0.046073682606220245\n",
      "epoch:  6   step:  253   train loss:  0.05387783423066139  val loss:  0.045297347009181976\n",
      "epoch:  6   step:  254   train loss:  0.046208351850509644  val loss:  0.0450621135532856\n",
      "epoch:  6   step:  255   train loss:  0.03272578492760658  val loss:  0.04471958056092262\n",
      "epoch:  6   step:  256   train loss:  0.05862794443964958  val loss:  0.04424547404050827\n",
      "epoch:  6   step:  257   train loss:  0.08882074803113937  val loss:  0.04514356330037117\n",
      "epoch:  6   step:  258   train loss:  0.044678762555122375  val loss:  0.045151278376579285\n",
      "epoch:  6   step:  259   train loss:  0.05318206921219826  val loss:  0.044931620359420776\n",
      "epoch:  6   step:  260   train loss:  0.08481129258871078  val loss:  0.045148298144340515\n",
      "epoch:  6   step:  261   train loss:  0.05237499251961708  val loss:  0.0450005866587162\n",
      "epoch:  6   step:  262   train loss:  0.09071590006351471  val loss:  0.045048005878925323\n",
      "epoch:  6   step:  263   train loss:  0.05098257213830948  val loss:  0.04475228488445282\n",
      "epoch:  6   step:  264   train loss:  0.06903193891048431  val loss:  0.04479225352406502\n",
      "epoch:  6   step:  265   train loss:  0.05617391690611839  val loss:  0.04486958682537079\n",
      "epoch:  6   step:  266   train loss:  0.04293643310666084  val loss:  0.04314475506544113\n",
      "epoch:  6   step:  267   train loss:  0.03844407945871353  val loss:  0.043783556669950485\n",
      "epoch:  6   step:  268   train loss:  0.04657469689846039  val loss:  0.04432888329029083\n",
      "epoch:  6   step:  269   train loss:  0.07447950541973114  val loss:  0.04506460949778557\n",
      "epoch:  6   step:  270   train loss:  0.07950090616941452  val loss:  0.04480272904038429\n",
      "epoch:  6   step:  271   train loss:  0.08828787505626678  val loss:  0.04465747997164726\n",
      "epoch:  6   step:  272   train loss:  0.04756558686494827  val loss:  0.04406848922371864\n",
      "epoch:  6   step:  273   train loss:  0.03988044708967209  val loss:  0.04444641247391701\n",
      "epoch:  6   step:  274   train loss:  0.06916753947734833  val loss:  0.04469373822212219\n",
      "epoch:  6   step:  275   train loss:  0.07151491194963455  val loss:  0.044724415987730026\n",
      "epoch:  6   step:  276   train loss:  0.05187670513987541  val loss:  0.04506458714604378\n",
      "epoch:  6   step:  277   train loss:  0.05313485115766525  val loss:  0.044403307139873505\n",
      "epoch:  6   step:  278   train loss:  0.04517358914017677  val loss:  0.043816786259412766\n",
      "epoch:  6   step:  279   train loss:  0.056047920137643814  val loss:  0.04400872066617012\n",
      "epoch:  6   step:  280   train loss:  0.041370660066604614  val loss:  0.044077880680561066\n",
      "epoch:  6   step:  281   train loss:  0.03055267035961151  val loss:  0.044925570487976074\n",
      "epoch:  6   step:  282   train loss:  0.06262979656457901  val loss:  0.04505511745810509\n",
      "epoch:  6   step:  283   train loss:  0.06172667071223259  val loss:  0.045294396579265594\n",
      "epoch:  6   step:  284   train loss:  0.06384260207414627  val loss:  0.04488656669855118\n",
      "epoch:  6   step:  285   train loss:  0.040900181978940964  val loss:  0.043870847672224045\n",
      "epoch:  6   step:  286   train loss:  0.05075850710272789  val loss:  0.04408648982644081\n",
      "epoch:  6   step:  287   train loss:  0.09923003613948822  val loss:  0.043434444814920425\n",
      "epoch:  6   step:  288   train loss:  0.05124245956540108  val loss:  0.04244996979832649\n",
      "epoch:  6   step:  289   train loss:  0.07600491493940353  val loss:  0.042183224111795425\n",
      "epoch:  6   step:  290   train loss:  0.04298124834895134  val loss:  0.042225487530231476\n",
      "epoch:  6   step:  291   train loss:  0.07316873222589493  val loss:  0.04222977161407471\n",
      "epoch:  6   step:  292   train loss:  0.05426514148712158  val loss:  0.041959792375564575\n",
      "epoch:  6   step:  293   train loss:  0.09873739629983902  val loss:  0.04258985444903374\n",
      "epoch:  6   step:  294   train loss:  0.045334529131650925  val loss:  0.042653366923332214\n",
      "epoch:  6   step:  295   train loss:  0.08019151538610458  val loss:  0.04366370663046837\n",
      "epoch:  6   step:  296   train loss:  0.04781750217080116  val loss:  0.044383466243743896\n",
      "epoch:  6   step:  297   train loss:  0.04224435240030289  val loss:  0.045375365763902664\n",
      "epoch:  6   step:  298   train loss:  0.10015320032835007  val loss:  0.04670204594731331\n",
      "epoch:  6   step:  299   train loss:  0.08429132401943207  val loss:  0.04729831963777542\n",
      "epoch:  6   step:  300   train loss:  0.044950805604457855  val loss:  0.047926198691129684\n",
      "epoch:  6   step:  301   train loss:  0.06321652233600616  val loss:  0.04842362180352211\n",
      "epoch:  6   step:  302   train loss:  0.05229690298438072  val loss:  0.047064948827028275\n",
      "epoch:  6   step:  303   train loss:  0.05917777121067047  val loss:  0.04673720896244049\n",
      "epoch:  6   step:  304   train loss:  0.07040628790855408  val loss:  0.045786961913108826\n",
      "epoch:  6   step:  305   train loss:  0.05102739855647087  val loss:  0.04522283002734184\n",
      "epoch:  6   step:  306   train loss:  0.06795868277549744  val loss:  0.04425175115466118\n",
      "epoch:  6   step:  307   train loss:  0.07801137119531631  val loss:  0.04514254629611969\n",
      "epoch:  6   step:  308   train loss:  0.09483819454908371  val loss:  0.04479335620999336\n",
      "epoch:  6   step:  309   train loss:  0.0496097169816494  val loss:  0.04557427018880844\n",
      "epoch:  6   step:  310   train loss:  0.07047132402658463  val loss:  0.044405221939086914\n",
      "epoch:  6   step:  311   train loss:  0.0817350447177887  val loss:  0.0431930311024189\n",
      "epoch:  6   step:  312   train loss:  0.06151265278458595  val loss:  0.04268964007496834\n",
      "epoch:  6   step:  313   train loss:  0.08582327514886856  val loss:  0.04280884563922882\n",
      "epoch:  6   step:  314   train loss:  0.04706196114420891  val loss:  0.042525798082351685\n",
      "epoch:  6   step:  315   train loss:  0.03844176232814789  val loss:  0.04267667233943939\n",
      "epoch:  6   step:  316   train loss:  0.056011609733104706  val loss:  0.04224932938814163\n",
      "epoch:  6   step:  317   train loss:  0.08908520638942719  val loss:  0.0430169515311718\n",
      "epoch:  6   step:  318   train loss:  0.05509223788976669  val loss:  0.043017104268074036\n",
      "epoch:  6   step:  319   train loss:  0.0641496330499649  val loss:  0.04245525225996971\n",
      "epoch:  6   step:  320   train loss:  0.0653899610042572  val loss:  0.043535489588975906\n",
      "epoch:  6   step:  321   train loss:  0.03246335685253143  val loss:  0.042935535311698914\n",
      "epoch:  6   step:  322   train loss:  0.0645335242152214  val loss:  0.04281157627701759\n",
      "epoch:  6   step:  323   train loss:  0.06501715630292892  val loss:  0.0442194864153862\n",
      "epoch:  6   step:  324   train loss:  0.05524226278066635  val loss:  0.043956208974123\n",
      "epoch:  6   step:  325   train loss:  0.06364662945270538  val loss:  0.044152505695819855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  6   step:  326   train loss:  0.03723679110407829  val loss:  0.04399598017334938\n",
      "epoch:  6   step:  327   train loss:  0.037064097821712494  val loss:  0.0436319075524807\n",
      "epoch:  6   step:  328   train loss:  0.049557942897081375  val loss:  0.0431823693215847\n",
      "epoch:  6   step:  329   train loss:  0.042620837688446045  val loss:  0.04318255931138992\n",
      "epoch:  6   step:  330   train loss:  0.07272948324680328  val loss:  0.0427691787481308\n",
      "epoch:  6   step:  331   train loss:  0.05472281202673912  val loss:  0.04250738024711609\n",
      "epoch:  6   step:  332   train loss:  0.08137322962284088  val loss:  0.04268273338675499\n",
      "epoch:  6   step:  333   train loss:  0.0560738667845726  val loss:  0.04264065623283386\n",
      "epoch:  6   step:  334   train loss:  0.04733094573020935  val loss:  0.04260718822479248\n",
      "epoch:  6   step:  335   train loss:  0.07053950428962708  val loss:  0.043151479214429855\n",
      "epoch:  6   step:  336   train loss:  0.12168783694505692  val loss:  0.043907493352890015\n",
      "epoch:  6   step:  337   train loss:  0.05546938255429268  val loss:  0.04427448660135269\n",
      "epoch:  6   step:  338   train loss:  0.08222553879022598  val loss:  0.04444529116153717\n",
      "epoch:  6   step:  339   train loss:  0.07922768592834473  val loss:  0.045147184282541275\n",
      "epoch:  6   step:  340   train loss:  0.05170290917158127  val loss:  0.044584717601537704\n",
      "epoch:  6   step:  341   train loss:  0.05164352431893349  val loss:  0.04346992447972298\n",
      "epoch:  6   step:  342   train loss:  0.0615852028131485  val loss:  0.042602311819791794\n",
      "epoch:  6   step:  343   train loss:  0.05201338976621628  val loss:  0.04280715063214302\n",
      "epoch:  7   step:  0   train loss:  0.04499528184533119  val loss:  0.04268817976117134\n",
      "epoch:  7   step:  1   train loss:  0.05179690197110176  val loss:  0.042459629476070404\n",
      "epoch:  7   step:  2   train loss:  0.059284597635269165  val loss:  0.0420243963599205\n",
      "epoch:  7   step:  3   train loss:  0.036339253187179565  val loss:  0.0412282831966877\n",
      "epoch:  7   step:  4   train loss:  0.05006904527544975  val loss:  0.040450319647789\n",
      "epoch:  7   step:  5   train loss:  0.08991477638483047  val loss:  0.04044913873076439\n",
      "epoch:  7   step:  6   train loss:  0.039424870163202286  val loss:  0.04013691842556\n",
      "epoch:  7   step:  7   train loss:  0.06745176017284393  val loss:  0.04001613333821297\n",
      "min_val_loss_print 0.04001613333821297\n",
      "epoch:  7   step:  8   train loss:  0.08777393400669098  val loss:  0.03996618464589119\n",
      "min_val_loss_print 0.03996618464589119\n",
      "epoch:  7   step:  9   train loss:  0.0594584085047245  val loss:  0.039459116756916046\n",
      "min_val_loss_print 0.039459116756916046\n",
      "epoch:  7   step:  10   train loss:  0.07762091606855392  val loss:  0.03932041674852371\n",
      "min_val_loss_print 0.03932041674852371\n",
      "epoch:  7   step:  11   train loss:  0.04121001809835434  val loss:  0.03887507691979408\n",
      "min_val_loss_print 0.03887507691979408\n",
      "epoch:  7   step:  12   train loss:  0.0515052005648613  val loss:  0.03922659531235695\n",
      "epoch:  7   step:  13   train loss:  0.036879561841487885  val loss:  0.038887280970811844\n",
      "epoch:  7   step:  14   train loss:  0.043205201625823975  val loss:  0.038868892937898636\n",
      "min_val_loss_print 0.038868892937898636\n",
      "epoch:  7   step:  15   train loss:  0.043538499623537064  val loss:  0.0393710695207119\n",
      "epoch:  7   step:  16   train loss:  0.05344207212328911  val loss:  0.03956605866551399\n",
      "epoch:  7   step:  17   train loss:  0.07318770885467529  val loss:  0.039580900222063065\n",
      "epoch:  7   step:  18   train loss:  0.07769348472356796  val loss:  0.039748840034008026\n",
      "epoch:  7   step:  19   train loss:  0.0695495679974556  val loss:  0.03965997323393822\n",
      "epoch:  7   step:  20   train loss:  0.053793732076883316  val loss:  0.04043208435177803\n",
      "epoch:  7   step:  21   train loss:  0.03604450449347496  val loss:  0.04043601453304291\n",
      "epoch:  7   step:  22   train loss:  0.06570575386285782  val loss:  0.04040706157684326\n",
      "epoch:  7   step:  23   train loss:  0.05981086939573288  val loss:  0.04005374386906624\n",
      "epoch:  7   step:  24   train loss:  0.05985460430383682  val loss:  0.04041147232055664\n",
      "epoch:  7   step:  25   train loss:  0.0421629436314106  val loss:  0.04039248079061508\n",
      "epoch:  7   step:  26   train loss:  0.039008431136608124  val loss:  0.04013935104012489\n",
      "epoch:  7   step:  27   train loss:  0.04254445061087608  val loss:  0.04008054733276367\n",
      "epoch:  7   step:  28   train loss:  0.04251664876937866  val loss:  0.03885722905397415\n",
      "min_val_loss_print 0.03885722905397415\n",
      "epoch:  7   step:  29   train loss:  0.09448756277561188  val loss:  0.03912784531712532\n",
      "epoch:  7   step:  30   train loss:  0.05760228633880615  val loss:  0.03998095914721489\n",
      "epoch:  7   step:  31   train loss:  0.07566390931606293  val loss:  0.03983870521187782\n",
      "epoch:  7   step:  32   train loss:  0.05198727920651436  val loss:  0.039140861481428146\n",
      "epoch:  7   step:  33   train loss:  0.06693131476640701  val loss:  0.03925487399101257\n",
      "epoch:  7   step:  34   train loss:  0.056550394743680954  val loss:  0.03901594504714012\n",
      "epoch:  7   step:  35   train loss:  0.02768082544207573  val loss:  0.038581181317567825\n",
      "min_val_loss_print 0.038581181317567825\n",
      "epoch:  7   step:  36   train loss:  0.05435268208384514  val loss:  0.039151713252067566\n",
      "epoch:  7   step:  37   train loss:  0.04310396686196327  val loss:  0.03926056623458862\n",
      "epoch:  7   step:  38   train loss:  0.054784197360277176  val loss:  0.039176493883132935\n",
      "epoch:  7   step:  39   train loss:  0.07112418860197067  val loss:  0.03905283287167549\n",
      "epoch:  7   step:  40   train loss:  0.04090702906250954  val loss:  0.038970254361629486\n",
      "epoch:  7   step:  41   train loss:  0.04767126217484474  val loss:  0.03930620476603508\n",
      "epoch:  7   step:  42   train loss:  0.05130026489496231  val loss:  0.03957647085189819\n",
      "epoch:  7   step:  43   train loss:  0.055197086185216904  val loss:  0.039303455501794815\n",
      "epoch:  7   step:  44   train loss:  0.040972545742988586  val loss:  0.03965781629085541\n",
      "epoch:  7   step:  45   train loss:  0.0630243793129921  val loss:  0.04019716754555702\n",
      "epoch:  7   step:  46   train loss:  0.06127244979143143  val loss:  0.04032333567738533\n",
      "epoch:  7   step:  47   train loss:  0.06788622587919235  val loss:  0.04005267843604088\n",
      "epoch:  7   step:  48   train loss:  0.05293240770697594  val loss:  0.04071192443370819\n",
      "epoch:  7   step:  49   train loss:  0.05504124239087105  val loss:  0.040919993072748184\n",
      "epoch:  7   step:  50   train loss:  0.06801459193229675  val loss:  0.04038412868976593\n",
      "epoch:  7   step:  51   train loss:  0.05014371871948242  val loss:  0.040507782250642776\n",
      "epoch:  7   step:  52   train loss:  0.06850136071443558  val loss:  0.040992747992277145\n",
      "epoch:  7   step:  53   train loss:  0.06254135072231293  val loss:  0.041114479303359985\n",
      "epoch:  7   step:  54   train loss:  0.07979407906532288  val loss:  0.04089493304491043\n",
      "epoch:  7   step:  55   train loss:  0.039186302572488785  val loss:  0.040868766605854034\n",
      "epoch:  7   step:  56   train loss:  0.04880537465214729  val loss:  0.04025464132428169\n",
      "epoch:  7   step:  57   train loss:  0.047705501317977905  val loss:  0.04111456498503685\n",
      "epoch:  7   step:  58   train loss:  0.05050795152783394  val loss:  0.04114013537764549\n",
      "epoch:  7   step:  59   train loss:  0.05242552608251572  val loss:  0.04137067869305611\n",
      "epoch:  7   step:  60   train loss:  0.03756335750222206  val loss:  0.040987130254507065\n",
      "epoch:  7   step:  61   train loss:  0.034862417727708817  val loss:  0.04051666334271431\n",
      "epoch:  7   step:  62   train loss:  0.047462668269872665  val loss:  0.040090881288051605\n",
      "epoch:  7   step:  63   train loss:  0.04911132901906967  val loss:  0.04082682356238365\n",
      "epoch:  7   step:  64   train loss:  0.045856282114982605  val loss:  0.04070751741528511\n",
      "epoch:  7   step:  65   train loss:  0.03252742812037468  val loss:  0.04171496257185936\n",
      "epoch:  7   step:  66   train loss:  0.05288580432534218  val loss:  0.041718244552612305\n",
      "epoch:  7   step:  67   train loss:  0.04940745234489441  val loss:  0.04239550232887268\n",
      "epoch:  7   step:  68   train loss:  0.0862746313214302  val loss:  0.042234502732753754\n",
      "epoch:  7   step:  69   train loss:  0.07249655574560165  val loss:  0.0426558218896389\n",
      "epoch:  7   step:  70   train loss:  0.07356655597686768  val loss:  0.04267548769712448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  7   step:  71   train loss:  0.03593362122774124  val loss:  0.042977042496204376\n",
      "epoch:  7   step:  72   train loss:  0.06331426650285721  val loss:  0.04171537607908249\n",
      "epoch:  7   step:  73   train loss:  0.04770972207188606  val loss:  0.042053136974573135\n",
      "epoch:  7   step:  74   train loss:  0.06772823631763458  val loss:  0.040682751685380936\n",
      "epoch:  7   step:  75   train loss:  0.0723605528473854  val loss:  0.04090564697980881\n",
      "epoch:  7   step:  76   train loss:  0.03732771798968315  val loss:  0.041352640837430954\n",
      "epoch:  7   step:  77   train loss:  0.06640566140413284  val loss:  0.0416591502726078\n",
      "epoch:  7   step:  78   train loss:  0.05326865613460541  val loss:  0.04099198803305626\n",
      "epoch:  7   step:  79   train loss:  0.0376226045191288  val loss:  0.040328990668058395\n",
      "epoch:  7   step:  80   train loss:  0.062221623957157135  val loss:  0.03997901827096939\n",
      "epoch:  7   step:  81   train loss:  0.05898839980363846  val loss:  0.04035022482275963\n",
      "epoch:  7   step:  82   train loss:  0.03726814314723015  val loss:  0.04081594944000244\n",
      "epoch:  7   step:  83   train loss:  0.0442836619913578  val loss:  0.04080863669514656\n",
      "epoch:  7   step:  84   train loss:  0.04754573106765747  val loss:  0.040746480226516724\n",
      "epoch:  7   step:  85   train loss:  0.047669027000665665  val loss:  0.03958113491535187\n",
      "epoch:  7   step:  86   train loss:  0.05616195499897003  val loss:  0.0395706370472908\n",
      "epoch:  7   step:  87   train loss:  0.03558393567800522  val loss:  0.039608340710401535\n",
      "epoch:  7   step:  88   train loss:  0.06940796971321106  val loss:  0.04023483768105507\n",
      "epoch:  7   step:  89   train loss:  0.027893410995602608  val loss:  0.040152519941329956\n",
      "epoch:  7   step:  90   train loss:  0.0789269208908081  val loss:  0.0392494797706604\n",
      "epoch:  7   step:  91   train loss:  0.053409092128276825  val loss:  0.03946984186768532\n",
      "epoch:  7   step:  92   train loss:  0.032079074531793594  val loss:  0.03947054594755173\n",
      "epoch:  7   step:  93   train loss:  0.04983626678586006  val loss:  0.03932260721921921\n",
      "epoch:  7   step:  94   train loss:  0.08091767132282257  val loss:  0.04001697152853012\n",
      "epoch:  7   step:  95   train loss:  0.05523877963423729  val loss:  0.04053908959031105\n",
      "epoch:  7   step:  96   train loss:  0.057397931814193726  val loss:  0.040077343583106995\n",
      "epoch:  7   step:  97   train loss:  0.03378579765558243  val loss:  0.04028473421931267\n",
      "epoch:  7   step:  98   train loss:  0.08472554385662079  val loss:  0.04013115540146828\n",
      "epoch:  7   step:  99   train loss:  0.05703548341989517  val loss:  0.03977305814623833\n",
      "epoch:  7   step:  100   train loss:  0.04159798473119736  val loss:  0.03984212875366211\n",
      "epoch:  7   step:  101   train loss:  0.051975317299366  val loss:  0.0389842763543129\n",
      "epoch:  7   step:  102   train loss:  0.0616561584174633  val loss:  0.03855578601360321\n",
      "min_val_loss_print 0.03855578601360321\n",
      "epoch:  7   step:  103   train loss:  0.03597371652722359  val loss:  0.03823301941156387\n",
      "min_val_loss_print 0.03823301941156387\n",
      "epoch:  7   step:  104   train loss:  0.0649447813630104  val loss:  0.038413070142269135\n",
      "epoch:  7   step:  105   train loss:  0.06332378834486008  val loss:  0.03861264884471893\n",
      "epoch:  7   step:  106   train loss:  0.05113866180181503  val loss:  0.03861641883850098\n",
      "epoch:  7   step:  107   train loss:  0.07250892370939255  val loss:  0.039363786578178406\n",
      "epoch:  7   step:  108   train loss:  0.02420440874993801  val loss:  0.03964875265955925\n",
      "epoch:  7   step:  109   train loss:  0.043879687786102295  val loss:  0.040043484419584274\n",
      "epoch:  7   step:  110   train loss:  0.031031548976898193  val loss:  0.040027718991041183\n",
      "epoch:  7   step:  111   train loss:  0.08258412033319473  val loss:  0.03991832956671715\n",
      "epoch:  7   step:  112   train loss:  0.03296183794736862  val loss:  0.04025760293006897\n",
      "epoch:  7   step:  113   train loss:  0.04697897285223007  val loss:  0.040962714701890945\n",
      "epoch:  7   step:  114   train loss:  0.06883774697780609  val loss:  0.04051336273550987\n",
      "epoch:  7   step:  115   train loss:  0.06004152074456215  val loss:  0.040396418422460556\n",
      "epoch:  7   step:  116   train loss:  0.04275139421224594  val loss:  0.04089927673339844\n",
      "epoch:  7   step:  117   train loss:  0.036004845052957535  val loss:  0.039953067898750305\n",
      "epoch:  7   step:  118   train loss:  0.04648025706410408  val loss:  0.040033455938100815\n",
      "epoch:  7   step:  119   train loss:  0.04496750980615616  val loss:  0.03982974588871002\n",
      "epoch:  7   step:  120   train loss:  0.04878987371921539  val loss:  0.03972535952925682\n",
      "epoch:  7   step:  121   train loss:  0.06149383634328842  val loss:  0.0386107936501503\n",
      "epoch:  7   step:  122   train loss:  0.08197943866252899  val loss:  0.03932133689522743\n",
      "epoch:  7   step:  123   train loss:  0.07718975096940994  val loss:  0.04000268504023552\n",
      "epoch:  7   step:  124   train loss:  0.0755336806178093  val loss:  0.03934060409665108\n",
      "epoch:  7   step:  125   train loss:  0.06110142171382904  val loss:  0.04000677168369293\n",
      "epoch:  7   step:  126   train loss:  0.057344790548086166  val loss:  0.04043329879641533\n",
      "epoch:  7   step:  127   train loss:  0.042318567633628845  val loss:  0.04008527472615242\n",
      "epoch:  7   step:  128   train loss:  0.04382619634270668  val loss:  0.039581600576639175\n",
      "epoch:  7   step:  129   train loss:  0.060034580528736115  val loss:  0.03852516785264015\n",
      "epoch:  7   step:  130   train loss:  0.037232521921396255  val loss:  0.03863056004047394\n",
      "epoch:  7   step:  131   train loss:  0.06111828610301018  val loss:  0.039037544280290604\n",
      "epoch:  7   step:  132   train loss:  0.044728994369506836  val loss:  0.03921909257769585\n",
      "epoch:  7   step:  133   train loss:  0.040839046239852905  val loss:  0.038635604083538055\n",
      "epoch:  7   step:  134   train loss:  0.08582619577646255  val loss:  0.038984015583992004\n",
      "epoch:  7   step:  135   train loss:  0.044951118528842926  val loss:  0.03903311491012573\n",
      "epoch:  7   step:  136   train loss:  0.050894107669591904  val loss:  0.03878774866461754\n",
      "epoch:  7   step:  137   train loss:  0.05364859849214554  val loss:  0.03853866457939148\n",
      "epoch:  7   step:  138   train loss:  0.048218999058008194  val loss:  0.03922110050916672\n",
      "epoch:  7   step:  139   train loss:  0.051298920065164566  val loss:  0.03895820677280426\n",
      "epoch:  7   step:  140   train loss:  0.04273419827222824  val loss:  0.03835621476173401\n",
      "epoch:  7   step:  141   train loss:  0.0389404334127903  val loss:  0.038522884249687195\n",
      "epoch:  7   step:  142   train loss:  0.039741694927215576  val loss:  0.037823133170604706\n",
      "min_val_loss_print 0.037823133170604706\n",
      "epoch:  7   step:  143   train loss:  0.05010860040783882  val loss:  0.03829873353242874\n",
      "epoch:  7   step:  144   train loss:  0.04795188829302788  val loss:  0.03807291388511658\n",
      "epoch:  7   step:  145   train loss:  0.0320236012339592  val loss:  0.03837050125002861\n",
      "epoch:  7   step:  146   train loss:  0.042220573872327805  val loss:  0.0385114848613739\n",
      "epoch:  7   step:  147   train loss:  0.06836795806884766  val loss:  0.03784657269716263\n",
      "epoch:  7   step:  148   train loss:  0.10807041078805923  val loss:  0.03791140764951706\n",
      "epoch:  7   step:  149   train loss:  0.0778830498456955  val loss:  0.037756893783807755\n",
      "min_val_loss_print 0.037756893783807755\n",
      "epoch:  7   step:  150   train loss:  0.04800419509410858  val loss:  0.03773221746087074\n",
      "min_val_loss_print 0.03773221746087074\n",
      "epoch:  7   step:  151   train loss:  0.03903847932815552  val loss:  0.03688187152147293\n",
      "min_val_loss_print 0.03688187152147293\n",
      "epoch:  7   step:  152   train loss:  0.039572425186634064  val loss:  0.036214686930179596\n",
      "min_val_loss_print 0.036214686930179596\n",
      "epoch:  7   step:  153   train loss:  0.059239622205495834  val loss:  0.03649400547146797\n",
      "epoch:  7   step:  154   train loss:  0.03829224407672882  val loss:  0.036397408694028854\n",
      "epoch:  7   step:  155   train loss:  0.04836716130375862  val loss:  0.036959562450647354\n",
      "epoch:  7   step:  156   train loss:  0.052152156829833984  val loss:  0.03629636391997337\n",
      "epoch:  7   step:  157   train loss:  0.05070529133081436  val loss:  0.03724793717265129\n",
      "epoch:  7   step:  158   train loss:  0.06548341363668442  val loss:  0.0372287891805172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  7   step:  159   train loss:  0.06991362571716309  val loss:  0.03844786062836647\n",
      "epoch:  7   step:  160   train loss:  0.09108766913414001  val loss:  0.03704952076077461\n",
      "epoch:  7   step:  161   train loss:  0.06424100697040558  val loss:  0.03660242632031441\n",
      "epoch:  7   step:  162   train loss:  0.0503060445189476  val loss:  0.03750978037714958\n",
      "epoch:  7   step:  163   train loss:  0.03648299723863602  val loss:  0.03691144287586212\n",
      "epoch:  7   step:  164   train loss:  0.050493448972702026  val loss:  0.03692314028739929\n",
      "epoch:  7   step:  165   train loss:  0.03221597522497177  val loss:  0.03635631129145622\n",
      "epoch:  7   step:  166   train loss:  0.04781761020421982  val loss:  0.0365455225110054\n",
      "epoch:  7   step:  167   train loss:  0.07968896627426147  val loss:  0.03666739910840988\n",
      "epoch:  7   step:  168   train loss:  0.04573754593729973  val loss:  0.0371207520365715\n",
      "epoch:  7   step:  169   train loss:  0.03748277574777603  val loss:  0.03693084791302681\n",
      "epoch:  7   step:  170   train loss:  0.056611526757478714  val loss:  0.03640947863459587\n",
      "epoch:  7   step:  171   train loss:  0.0470934733748436  val loss:  0.03613876551389694\n",
      "min_val_loss_print 0.03613876551389694\n",
      "epoch:  7   step:  172   train loss:  0.04904184117913246  val loss:  0.03596607223153114\n",
      "min_val_loss_print 0.03596607223153114\n",
      "epoch:  7   step:  173   train loss:  0.06038212403655052  val loss:  0.03643607720732689\n",
      "epoch:  7   step:  174   train loss:  0.05688019096851349  val loss:  0.036245573312044144\n",
      "epoch:  7   step:  175   train loss:  0.031356800347566605  val loss:  0.03625784069299698\n",
      "epoch:  7   step:  176   train loss:  0.08633534610271454  val loss:  0.03679797425866127\n",
      "epoch:  7   step:  177   train loss:  0.06139716878533363  val loss:  0.03701869398355484\n",
      "epoch:  7   step:  178   train loss:  0.039105676114559174  val loss:  0.03686907887458801\n",
      "epoch:  7   step:  179   train loss:  0.09585953503847122  val loss:  0.03708883747458458\n",
      "epoch:  7   step:  180   train loss:  0.03697076812386513  val loss:  0.0372040718793869\n",
      "epoch:  7   step:  181   train loss:  0.03484033793210983  val loss:  0.03767376020550728\n",
      "epoch:  7   step:  182   train loss:  0.04951713606715202  val loss:  0.03770271688699722\n",
      "epoch:  7   step:  183   train loss:  0.03798436000943184  val loss:  0.03794116526842117\n",
      "epoch:  7   step:  184   train loss:  0.09411806613206863  val loss:  0.03782148286700249\n",
      "epoch:  7   step:  185   train loss:  0.03730817139148712  val loss:  0.03801581636071205\n",
      "epoch:  7   step:  186   train loss:  0.030304865911602974  val loss:  0.038177769631147385\n",
      "epoch:  7   step:  187   train loss:  0.06423066556453705  val loss:  0.038262128829956055\n",
      "epoch:  7   step:  188   train loss:  0.05587160959839821  val loss:  0.03893236815929413\n",
      "epoch:  7   step:  189   train loss:  0.05496053770184517  val loss:  0.038420502096414566\n",
      "epoch:  7   step:  190   train loss:  0.05684014409780502  val loss:  0.03789081051945686\n",
      "epoch:  7   step:  191   train loss:  0.04477555677294731  val loss:  0.037455156445503235\n",
      "epoch:  7   step:  192   train loss:  0.04628575220704079  val loss:  0.03768894821405411\n",
      "epoch:  7   step:  193   train loss:  0.07733451575040817  val loss:  0.03788718581199646\n",
      "epoch:  7   step:  194   train loss:  0.04575377702713013  val loss:  0.037965863943099976\n",
      "epoch:  7   step:  195   train loss:  0.058238059282302856  val loss:  0.03824761509895325\n",
      "epoch:  7   step:  196   train loss:  0.03949978947639465  val loss:  0.038522712886333466\n",
      "epoch:  7   step:  197   train loss:  0.049051959067583084  val loss:  0.03854416683316231\n",
      "epoch:  7   step:  198   train loss:  0.03966046869754791  val loss:  0.03904568776488304\n",
      "epoch:  7   step:  199   train loss:  0.051384393125772476  val loss:  0.03989782556891441\n",
      "epoch:  7   step:  200   train loss:  0.0453164242208004  val loss:  0.04053306579589844\n",
      "epoch:  7   step:  201   train loss:  0.07178190350532532  val loss:  0.03991411253809929\n",
      "epoch:  7   step:  202   train loss:  0.05659007653594017  val loss:  0.0400693453848362\n",
      "epoch:  7   step:  203   train loss:  0.041672930121421814  val loss:  0.039756204932928085\n",
      "epoch:  7   step:  204   train loss:  0.049930818378925323  val loss:  0.039788201451301575\n",
      "epoch:  7   step:  205   train loss:  0.038877639919519424  val loss:  0.039852436631917953\n",
      "epoch:  7   step:  206   train loss:  0.05400087311863899  val loss:  0.03938688710331917\n",
      "epoch:  7   step:  207   train loss:  0.027226202189922333  val loss:  0.039625752717256546\n",
      "epoch:  7   step:  208   train loss:  0.037057265639305115  val loss:  0.04002745822072029\n",
      "epoch:  7   step:  209   train loss:  0.054693516343832016  val loss:  0.04004877060651779\n",
      "epoch:  7   step:  210   train loss:  0.037233684211969376  val loss:  0.03969979286193848\n",
      "epoch:  7   step:  211   train loss:  0.03155846148729324  val loss:  0.03961588442325592\n",
      "epoch:  7   step:  212   train loss:  0.08164409548044205  val loss:  0.040027547627687454\n",
      "epoch:  7   step:  213   train loss:  0.044516004621982574  val loss:  0.039289042353630066\n",
      "epoch:  7   step:  214   train loss:  0.05022963136434555  val loss:  0.039032213389873505\n",
      "epoch:  7   step:  215   train loss:  0.043099042028188705  val loss:  0.03875255584716797\n",
      "epoch:  7   step:  216   train loss:  0.05761949345469475  val loss:  0.038389869034290314\n",
      "epoch:  7   step:  217   train loss:  0.04500119388103485  val loss:  0.037183668464422226\n",
      "epoch:  7   step:  218   train loss:  0.03829345852136612  val loss:  0.036327093839645386\n",
      "epoch:  7   step:  219   train loss:  0.09293888509273529  val loss:  0.036463309079408646\n",
      "epoch:  7   step:  220   train loss:  0.0401105135679245  val loss:  0.03605133295059204\n",
      "epoch:  7   step:  221   train loss:  0.04666278883814812  val loss:  0.03603831306099892\n",
      "epoch:  7   step:  222   train loss:  0.03472642973065376  val loss:  0.03618711233139038\n",
      "epoch:  7   step:  223   train loss:  0.04921238496899605  val loss:  0.036559272557497025\n",
      "epoch:  7   step:  224   train loss:  0.05147373303771019  val loss:  0.03617842122912407\n",
      "epoch:  7   step:  225   train loss:  0.031557485461235046  val loss:  0.035710256546735764\n",
      "min_val_loss_print 0.035710256546735764\n",
      "epoch:  7   step:  226   train loss:  0.05361638963222504  val loss:  0.03599647060036659\n",
      "epoch:  7   step:  227   train loss:  0.03015837073326111  val loss:  0.036664366722106934\n",
      "epoch:  7   step:  228   train loss:  0.039776794612407684  val loss:  0.035783398896455765\n",
      "epoch:  7   step:  229   train loss:  0.051117848604917526  val loss:  0.03443700447678566\n",
      "min_val_loss_print 0.03443700447678566\n",
      "epoch:  7   step:  230   train loss:  0.05863040313124657  val loss:  0.034617047756910324\n",
      "epoch:  7   step:  231   train loss:  0.027840450406074524  val loss:  0.03464304655790329\n",
      "epoch:  7   step:  232   train loss:  0.06818284094333649  val loss:  0.03466358408331871\n",
      "epoch:  7   step:  233   train loss:  0.04209648817777634  val loss:  0.03603613004088402\n",
      "epoch:  7   step:  234   train loss:  0.053309548646211624  val loss:  0.03594481199979782\n",
      "epoch:  7   step:  235   train loss:  0.025870582088828087  val loss:  0.034602679312229156\n",
      "epoch:  7   step:  236   train loss:  0.07513674348592758  val loss:  0.034429192543029785\n",
      "min_val_loss_print 0.034429192543029785\n",
      "epoch:  7   step:  237   train loss:  0.05054951086640358  val loss:  0.034164346754550934\n",
      "min_val_loss_print 0.034164346754550934\n",
      "epoch:  7   step:  238   train loss:  0.073872871696949  val loss:  0.03418881446123123\n",
      "epoch:  7   step:  239   train loss:  0.03177165612578392  val loss:  0.03388707712292671\n",
      "min_val_loss_print 0.03388707712292671\n",
      "epoch:  7   step:  240   train loss:  0.04228492081165314  val loss:  0.034494996070861816\n",
      "epoch:  7   step:  241   train loss:  0.06236424297094345  val loss:  0.03400737792253494\n",
      "epoch:  7   step:  242   train loss:  0.04824759066104889  val loss:  0.03508676588535309\n",
      "epoch:  7   step:  243   train loss:  0.06193196028470993  val loss:  0.03571572154760361\n",
      "epoch:  7   step:  244   train loss:  0.057322777807712555  val loss:  0.03616166114807129\n",
      "epoch:  7   step:  245   train loss:  0.06045706942677498  val loss:  0.03595275059342384\n",
      "epoch:  7   step:  246   train loss:  0.034512221813201904  val loss:  0.035212066024541855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  7   step:  247   train loss:  0.04947285354137421  val loss:  0.0353233776986599\n",
      "epoch:  7   step:  248   train loss:  0.07559102773666382  val loss:  0.0347462072968483\n",
      "epoch:  7   step:  249   train loss:  0.034637339413166046  val loss:  0.03468811511993408\n",
      "epoch:  7   step:  250   train loss:  0.040033772587776184  val loss:  0.03514869511127472\n",
      "epoch:  7   step:  251   train loss:  0.050758007913827896  val loss:  0.0347263365983963\n",
      "epoch:  7   step:  252   train loss:  0.03980089724063873  val loss:  0.03515903279185295\n",
      "epoch:  7   step:  253   train loss:  0.05233615264296532  val loss:  0.03513311222195625\n",
      "epoch:  7   step:  254   train loss:  0.06879478693008423  val loss:  0.03478385508060455\n",
      "epoch:  7   step:  255   train loss:  0.05408300831913948  val loss:  0.03512948378920555\n",
      "epoch:  7   step:  256   train loss:  0.051424723118543625  val loss:  0.035631757229566574\n",
      "epoch:  7   step:  257   train loss:  0.04036872088909149  val loss:  0.035091832280159\n",
      "epoch:  7   step:  258   train loss:  0.03630441799759865  val loss:  0.03492743894457817\n",
      "epoch:  7   step:  259   train loss:  0.042216844856739044  val loss:  0.03452053293585777\n",
      "epoch:  7   step:  260   train loss:  0.02635014057159424  val loss:  0.03391428291797638\n",
      "epoch:  7   step:  261   train loss:  0.028024205937981606  val loss:  0.03390391170978546\n",
      "epoch:  7   step:  262   train loss:  0.06083333492279053  val loss:  0.033877767622470856\n",
      "min_val_loss_print 0.033877767622470856\n",
      "epoch:  7   step:  263   train loss:  0.05041489377617836  val loss:  0.034171029925346375\n",
      "epoch:  7   step:  264   train loss:  0.061967067420482635  val loss:  0.035680774599313736\n",
      "epoch:  7   step:  265   train loss:  0.07774975150823593  val loss:  0.03607531264424324\n",
      "epoch:  7   step:  266   train loss:  0.07945346087217331  val loss:  0.03502616658806801\n",
      "epoch:  7   step:  267   train loss:  0.05765482783317566  val loss:  0.034733884036540985\n",
      "epoch:  7   step:  268   train loss:  0.054038431495428085  val loss:  0.03264176845550537\n",
      "min_val_loss_print 0.03264176845550537\n",
      "epoch:  7   step:  269   train loss:  0.03512469306588173  val loss:  0.032331280410289764\n",
      "min_val_loss_print 0.032331280410289764\n",
      "epoch:  7   step:  270   train loss:  0.0400870256125927  val loss:  0.032040879130363464\n",
      "min_val_loss_print 0.032040879130363464\n",
      "epoch:  7   step:  271   train loss:  0.05914686992764473  val loss:  0.031669676303863525\n",
      "min_val_loss_print 0.031669676303863525\n",
      "epoch:  7   step:  272   train loss:  0.03923780098557472  val loss:  0.031663842499256134\n",
      "min_val_loss_print 0.031663842499256134\n",
      "epoch:  7   step:  273   train loss:  0.042205389589071274  val loss:  0.031594421714544296\n",
      "min_val_loss_print 0.031594421714544296\n",
      "epoch:  7   step:  274   train loss:  0.028452059254050255  val loss:  0.031634557992219925\n",
      "epoch:  7   step:  275   train loss:  0.04763565585017204  val loss:  0.031993355602025986\n",
      "epoch:  7   step:  276   train loss:  0.07138442993164062  val loss:  0.03223393112421036\n",
      "epoch:  7   step:  277   train loss:  0.057608384639024734  val loss:  0.031190240755677223\n",
      "min_val_loss_print 0.031190240755677223\n",
      "epoch:  7   step:  278   train loss:  0.07049024105072021  val loss:  0.03244132921099663\n",
      "epoch:  7   step:  279   train loss:  0.05978875979781151  val loss:  0.03202143684029579\n",
      "epoch:  7   step:  280   train loss:  0.0713934600353241  val loss:  0.03165212273597717\n",
      "epoch:  7   step:  281   train loss:  0.07214807718992233  val loss:  0.03269636631011963\n",
      "epoch:  7   step:  282   train loss:  0.04099435731768608  val loss:  0.03277561813592911\n",
      "epoch:  7   step:  283   train loss:  0.05957459285855293  val loss:  0.032978467643260956\n",
      "epoch:  7   step:  284   train loss:  0.04541631042957306  val loss:  0.03260823339223862\n",
      "epoch:  7   step:  285   train loss:  0.04433436691761017  val loss:  0.03243422135710716\n",
      "epoch:  7   step:  286   train loss:  0.03031206876039505  val loss:  0.03311966732144356\n",
      "epoch:  7   step:  287   train loss:  0.06378134340047836  val loss:  0.03396117314696312\n",
      "epoch:  7   step:  288   train loss:  0.06869474053382874  val loss:  0.03298048675060272\n",
      "epoch:  7   step:  289   train loss:  0.06137291342020035  val loss:  0.03323919326066971\n",
      "epoch:  7   step:  290   train loss:  0.035477425903081894  val loss:  0.03298504278063774\n",
      "epoch:  7   step:  291   train loss:  0.05156831815838814  val loss:  0.03333473950624466\n",
      "epoch:  7   step:  292   train loss:  0.0710136890411377  val loss:  0.03313577547669411\n",
      "epoch:  7   step:  293   train loss:  0.08452393859624863  val loss:  0.0320621095597744\n",
      "epoch:  7   step:  294   train loss:  0.05049252510070801  val loss:  0.03255989030003548\n",
      "epoch:  7   step:  295   train loss:  0.055319637060165405  val loss:  0.03215683624148369\n",
      "epoch:  7   step:  296   train loss:  0.05934128165245056  val loss:  0.03266537934541702\n",
      "epoch:  7   step:  297   train loss:  0.052079107612371445  val loss:  0.03350406512618065\n",
      "epoch:  7   step:  298   train loss:  0.06336777657270432  val loss:  0.034455183893442154\n",
      "epoch:  7   step:  299   train loss:  0.027799304574728012  val loss:  0.03374782204627991\n",
      "epoch:  7   step:  300   train loss:  0.06047416478395462  val loss:  0.03417597711086273\n",
      "epoch:  7   step:  301   train loss:  0.042434826493263245  val loss:  0.034384340047836304\n",
      "epoch:  7   step:  302   train loss:  0.07665900141000748  val loss:  0.03518900275230408\n",
      "epoch:  7   step:  303   train loss:  0.052460331469774246  val loss:  0.03576386347413063\n",
      "epoch:  7   step:  304   train loss:  0.06678678840398788  val loss:  0.036164700984954834\n",
      "epoch:  7   step:  305   train loss:  0.03880658000707626  val loss:  0.035262349992990494\n",
      "epoch:  7   step:  306   train loss:  0.03090488351881504  val loss:  0.03557053953409195\n",
      "epoch:  7   step:  307   train loss:  0.04985831305384636  val loss:  0.03606246039271355\n",
      "epoch:  7   step:  308   train loss:  0.056046370416879654  val loss:  0.03602180629968643\n",
      "epoch:  7   step:  309   train loss:  0.0643947497010231  val loss:  0.036770883947610855\n",
      "epoch:  7   step:  310   train loss:  0.06411851942539215  val loss:  0.03665114566683769\n",
      "epoch:  7   step:  311   train loss:  0.061137136071920395  val loss:  0.03659597039222717\n",
      "epoch:  7   step:  312   train loss:  0.06296378374099731  val loss:  0.036883022636175156\n",
      "epoch:  7   step:  313   train loss:  0.03958027809858322  val loss:  0.03623969107866287\n",
      "epoch:  7   step:  314   train loss:  0.04384193569421768  val loss:  0.03560536727309227\n",
      "epoch:  7   step:  315   train loss:  0.05416405200958252  val loss:  0.03619125112891197\n",
      "epoch:  7   step:  316   train loss:  0.040530070662498474  val loss:  0.03631959483027458\n",
      "epoch:  7   step:  317   train loss:  0.03762277588248253  val loss:  0.036709606647491455\n",
      "epoch:  7   step:  318   train loss:  0.049384456127882004  val loss:  0.03653974086046219\n",
      "epoch:  7   step:  319   train loss:  0.03781356289982796  val loss:  0.03673767298460007\n",
      "epoch:  7   step:  320   train loss:  0.04190106689929962  val loss:  0.03671315684914589\n",
      "epoch:  7   step:  321   train loss:  0.054594215005636215  val loss:  0.035887498408555984\n",
      "epoch:  7   step:  322   train loss:  0.05988585948944092  val loss:  0.03527374193072319\n",
      "epoch:  7   step:  323   train loss:  0.024639584124088287  val loss:  0.03520800918340683\n",
      "epoch:  7   step:  324   train loss:  0.049595776945352554  val loss:  0.0348518043756485\n",
      "epoch:  7   step:  325   train loss:  0.06834481656551361  val loss:  0.035230305045843124\n",
      "epoch:  7   step:  326   train loss:  0.03815464675426483  val loss:  0.036156363785266876\n",
      "epoch:  7   step:  327   train loss:  0.03026977740228176  val loss:  0.03516346588730812\n",
      "epoch:  7   step:  328   train loss:  0.07056991010904312  val loss:  0.034869104623794556\n",
      "epoch:  7   step:  329   train loss:  0.05688298121094704  val loss:  0.03483760356903076\n",
      "epoch:  7   step:  330   train loss:  0.0476510263979435  val loss:  0.03514047712087631\n",
      "epoch:  7   step:  331   train loss:  0.04927029088139534  val loss:  0.03473815321922302\n",
      "epoch:  7   step:  332   train loss:  0.05889061838388443  val loss:  0.03410862013697624\n",
      "epoch:  7   step:  333   train loss:  0.041277188807725906  val loss:  0.03427712991833687\n",
      "epoch:  7   step:  334   train loss:  0.059212155640125275  val loss:  0.03375750407576561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  7   step:  335   train loss:  0.049071237444877625  val loss:  0.033724892884492874\n",
      "epoch:  7   step:  336   train loss:  0.04626290500164032  val loss:  0.033679328858852386\n",
      "epoch:  7   step:  337   train loss:  0.03243747353553772  val loss:  0.03329065814614296\n",
      "epoch:  7   step:  338   train loss:  0.052107781171798706  val loss:  0.03348580747842789\n",
      "epoch:  7   step:  339   train loss:  0.03394101560115814  val loss:  0.033812668174505234\n",
      "epoch:  7   step:  340   train loss:  0.032998379319906235  val loss:  0.03404657915234566\n",
      "epoch:  7   step:  341   train loss:  0.04696320369839668  val loss:  0.034073445945978165\n",
      "epoch:  7   step:  342   train loss:  0.04544970020651817  val loss:  0.03355639800429344\n",
      "epoch:  7   step:  343   train loss:  0.025132890790700912  val loss:  0.03400105610489845\n",
      "epoch:  8   step:  0   train loss:  0.033916033804416656  val loss:  0.03411753103137016\n",
      "epoch:  8   step:  1   train loss:  0.06622219830751419  val loss:  0.034030865877866745\n",
      "epoch:  8   step:  2   train loss:  0.05446474999189377  val loss:  0.03338461369276047\n",
      "epoch:  8   step:  3   train loss:  0.04098852351307869  val loss:  0.03338242694735527\n",
      "epoch:  8   step:  4   train loss:  0.04794622212648392  val loss:  0.03388792648911476\n",
      "epoch:  8   step:  5   train loss:  0.07158184796571732  val loss:  0.03452890366315842\n",
      "epoch:  8   step:  6   train loss:  0.05862177163362503  val loss:  0.033297423273324966\n",
      "epoch:  8   step:  7   train loss:  0.027954963967204094  val loss:  0.033164165914058685\n",
      "epoch:  8   step:  8   train loss:  0.04727346450090408  val loss:  0.03351742774248123\n",
      "epoch:  8   step:  9   train loss:  0.05666390433907509  val loss:  0.03358176350593567\n",
      "epoch:  8   step:  10   train loss:  0.050073977559804916  val loss:  0.03375765681266785\n",
      "epoch:  8   step:  11   train loss:  0.04622799903154373  val loss:  0.033814843744039536\n",
      "epoch:  8   step:  12   train loss:  0.06233373284339905  val loss:  0.03458559140563011\n",
      "epoch:  8   step:  13   train loss:  0.04369885474443436  val loss:  0.03380359709262848\n",
      "epoch:  8   step:  14   train loss:  0.04419839754700661  val loss:  0.034565702080726624\n",
      "epoch:  8   step:  15   train loss:  0.06284970790147781  val loss:  0.03433109074831009\n",
      "epoch:  8   step:  16   train loss:  0.06220661848783493  val loss:  0.033735305070877075\n",
      "epoch:  8   step:  17   train loss:  0.0542808398604393  val loss:  0.03429697826504707\n",
      "epoch:  8   step:  18   train loss:  0.042890582233667374  val loss:  0.03373672440648079\n",
      "epoch:  8   step:  19   train loss:  0.06105053797364235  val loss:  0.033719658851623535\n",
      "epoch:  8   step:  20   train loss:  0.046644534915685654  val loss:  0.03478047251701355\n",
      "epoch:  8   step:  21   train loss:  0.054008129984140396  val loss:  0.03496929258108139\n",
      "epoch:  8   step:  22   train loss:  0.06308791041374207  val loss:  0.03504074737429619\n",
      "epoch:  8   step:  23   train loss:  0.042322393506765366  val loss:  0.035253677517175674\n",
      "epoch:  8   step:  24   train loss:  0.04576185345649719  val loss:  0.03507518023252487\n",
      "epoch:  8   step:  25   train loss:  0.032479435205459595  val loss:  0.03534860908985138\n",
      "epoch:  8   step:  26   train loss:  0.04710134491324425  val loss:  0.035219401121139526\n",
      "epoch:  8   step:  27   train loss:  0.05170638486742973  val loss:  0.03590506315231323\n",
      "epoch:  8   step:  28   train loss:  0.04539445787668228  val loss:  0.03589101508259773\n",
      "epoch:  8   step:  29   train loss:  0.03559454157948494  val loss:  0.0354810506105423\n",
      "epoch:  8   step:  30   train loss:  0.08866800367832184  val loss:  0.03671620413661003\n",
      "epoch:  8   step:  31   train loss:  0.05432657524943352  val loss:  0.03738608956336975\n",
      "epoch:  8   step:  32   train loss:  0.04932381212711334  val loss:  0.03734874352812767\n",
      "epoch:  8   step:  33   train loss:  0.03967820480465889  val loss:  0.037348534911870956\n",
      "epoch:  8   step:  34   train loss:  0.058470554649829865  val loss:  0.037599384784698486\n",
      "epoch:  8   step:  35   train loss:  0.07022088021039963  val loss:  0.037759363651275635\n",
      "epoch:  8   step:  36   train loss:  0.058452632278203964  val loss:  0.037794750183820724\n",
      "epoch:  8   step:  37   train loss:  0.03545910865068436  val loss:  0.03760823979973793\n",
      "epoch:  8   step:  38   train loss:  0.06290684640407562  val loss:  0.037545546889305115\n",
      "epoch:  8   step:  39   train loss:  0.043352045118808746  val loss:  0.03701252117753029\n",
      "epoch:  8   step:  40   train loss:  0.051838554441928864  val loss:  0.03642268478870392\n",
      "epoch:  8   step:  41   train loss:  0.048256225883960724  val loss:  0.03661444038152695\n",
      "epoch:  8   step:  42   train loss:  0.04027547314763069  val loss:  0.035892173647880554\n",
      "epoch:  8   step:  43   train loss:  0.03412533923983574  val loss:  0.03561427816748619\n",
      "epoch:  8   step:  44   train loss:  0.04630928114056587  val loss:  0.03572595864534378\n",
      "epoch:  8   step:  45   train loss:  0.04300196096301079  val loss:  0.03486429899930954\n",
      "epoch:  8   step:  46   train loss:  0.041680339723825455  val loss:  0.03530237078666687\n",
      "epoch:  8   step:  47   train loss:  0.051441825926303864  val loss:  0.03484119474887848\n",
      "epoch:  8   step:  48   train loss:  0.028453413397073746  val loss:  0.034843966364860535\n",
      "epoch:  8   step:  49   train loss:  0.08701029419898987  val loss:  0.03581437095999718\n",
      "epoch:  8   step:  50   train loss:  0.03683732450008392  val loss:  0.036656640470027924\n",
      "epoch:  8   step:  51   train loss:  0.04711408540606499  val loss:  0.03678767383098602\n",
      "epoch:  8   step:  52   train loss:  0.040974926203489304  val loss:  0.03706388175487518\n",
      "epoch:  8   step:  53   train loss:  0.034555044025182724  val loss:  0.037047695368528366\n",
      "epoch:  8   step:  54   train loss:  0.0537494532763958  val loss:  0.03681345656514168\n",
      "epoch:  8   step:  55   train loss:  0.043498676270246506  val loss:  0.03643091395497322\n",
      "epoch:  8   step:  56   train loss:  0.0438108816742897  val loss:  0.03622518852353096\n",
      "epoch:  8   step:  57   train loss:  0.04220086336135864  val loss:  0.035529837012290955\n",
      "epoch:  8   step:  58   train loss:  0.050815943628549576  val loss:  0.035021502524614334\n",
      "epoch:  8   step:  59   train loss:  0.050123319029808044  val loss:  0.034358806908130646\n",
      "epoch:  8   step:  60   train loss:  0.0600631944835186  val loss:  0.03376727178692818\n",
      "epoch:  8   step:  61   train loss:  0.05417802557349205  val loss:  0.03354352340102196\n",
      "epoch:  8   step:  62   train loss:  0.03455204516649246  val loss:  0.03364067152142525\n",
      "epoch:  8   step:  63   train loss:  0.03857811167836189  val loss:  0.03308524936437607\n",
      "epoch:  8   step:  64   train loss:  0.03327932581305504  val loss:  0.03322407603263855\n",
      "epoch:  8   step:  65   train loss:  0.059339795261621475  val loss:  0.0341833122074604\n",
      "epoch:  8   step:  66   train loss:  0.0523531474173069  val loss:  0.03290242701768875\n",
      "epoch:  8   step:  67   train loss:  0.03592216596007347  val loss:  0.03248895704746246\n",
      "epoch:  8   step:  68   train loss:  0.050279226154088974  val loss:  0.032801054418087006\n",
      "epoch:  8   step:  69   train loss:  0.05733245238661766  val loss:  0.03288579359650612\n",
      "epoch:  8   step:  70   train loss:  0.04301745072007179  val loss:  0.03239703178405762\n",
      "epoch:  8   step:  71   train loss:  0.03789797052741051  val loss:  0.03176182135939598\n",
      "epoch:  8   step:  72   train loss:  0.05645813047885895  val loss:  0.03203164041042328\n",
      "epoch:  8   step:  73   train loss:  0.060986898839473724  val loss:  0.03214067220687866\n",
      "epoch:  8   step:  74   train loss:  0.0343131348490715  val loss:  0.03261233866214752\n",
      "epoch:  8   step:  75   train loss:  0.056242406368255615  val loss:  0.03256125748157501\n",
      "epoch:  8   step:  76   train loss:  0.03331500664353371  val loss:  0.032467711716890335\n",
      "epoch:  8   step:  77   train loss:  0.060390472412109375  val loss:  0.0323513001203537\n",
      "epoch:  8   step:  78   train loss:  0.06925292313098907  val loss:  0.0325235053896904\n",
      "epoch:  8   step:  79   train loss:  0.03071691282093525  val loss:  0.03349723294377327\n",
      "epoch:  8   step:  80   train loss:  0.06493543088436127  val loss:  0.033161479979753494\n",
      "epoch:  8   step:  81   train loss:  0.041132379323244095  val loss:  0.03299030661582947\n",
      "epoch:  8   step:  82   train loss:  0.08009662479162216  val loss:  0.03308727219700813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  8   step:  83   train loss:  0.038666579872369766  val loss:  0.033242058008909225\n",
      "epoch:  8   step:  84   train loss:  0.03568654879927635  val loss:  0.03347013145685196\n",
      "epoch:  8   step:  85   train loss:  0.03447709232568741  val loss:  0.03363111615180969\n",
      "epoch:  8   step:  86   train loss:  0.034234948456287384  val loss:  0.033225297927856445\n",
      "epoch:  8   step:  87   train loss:  0.05166591703891754  val loss:  0.03319653496146202\n",
      "epoch:  8   step:  88   train loss:  0.05087379366159439  val loss:  0.03321142867207527\n",
      "epoch:  8   step:  89   train loss:  0.031061137095093727  val loss:  0.03271019831299782\n",
      "epoch:  8   step:  90   train loss:  0.03612615540623665  val loss:  0.03296421840786934\n",
      "epoch:  8   step:  91   train loss:  0.03171190619468689  val loss:  0.03321363031864166\n",
      "epoch:  8   step:  92   train loss:  0.053983256220817566  val loss:  0.03350977599620819\n",
      "epoch:  8   step:  93   train loss:  0.062067724764347076  val loss:  0.034265413880348206\n",
      "epoch:  8   step:  94   train loss:  0.09936869144439697  val loss:  0.03326933830976486\n",
      "epoch:  8   step:  95   train loss:  0.04053212329745293  val loss:  0.032588884234428406\n",
      "epoch:  8   step:  96   train loss:  0.03945014253258705  val loss:  0.032443974167108536\n",
      "epoch:  8   step:  97   train loss:  0.05469775199890137  val loss:  0.031945835798978806\n",
      "epoch:  8   step:  98   train loss:  0.03752512484788895  val loss:  0.031931016594171524\n",
      "epoch:  8   step:  99   train loss:  0.04693656787276268  val loss:  0.0314924493432045\n",
      "epoch:  8   step:  100   train loss:  0.0511539950966835  val loss:  0.03166019544005394\n",
      "epoch:  8   step:  101   train loss:  0.03054281696677208  val loss:  0.0318015031516552\n",
      "epoch:  8   step:  102   train loss:  0.05796859413385391  val loss:  0.03162476792931557\n",
      "epoch:  8   step:  103   train loss:  0.04607640206813812  val loss:  0.03213949501514435\n",
      "epoch:  8   step:  104   train loss:  0.06733265519142151  val loss:  0.03211803734302521\n",
      "epoch:  8   step:  105   train loss:  0.04758420214056969  val loss:  0.03228956460952759\n",
      "epoch:  8   step:  106   train loss:  0.05474654957652092  val loss:  0.03213014081120491\n",
      "epoch:  8   step:  107   train loss:  0.045093607157468796  val loss:  0.0321633443236351\n",
      "epoch:  8   step:  108   train loss:  0.05177685618400574  val loss:  0.03226513788104057\n",
      "epoch:  8   step:  109   train loss:  0.041082922369241714  val loss:  0.03156721219420433\n",
      "epoch:  8   step:  110   train loss:  0.05126554146409035  val loss:  0.03197779133915901\n",
      "epoch:  8   step:  111   train loss:  0.04230119287967682  val loss:  0.03216930851340294\n",
      "epoch:  8   step:  112   train loss:  0.037798840552568436  val loss:  0.03228956460952759\n",
      "epoch:  8   step:  113   train loss:  0.03166951611638069  val loss:  0.03229615092277527\n",
      "epoch:  8   step:  114   train loss:  0.03480767458677292  val loss:  0.0322396457195282\n",
      "epoch:  8   step:  115   train loss:  0.03606700152158737  val loss:  0.03202993795275688\n",
      "epoch:  8   step:  116   train loss:  0.0470886304974556  val loss:  0.03180313855409622\n",
      "epoch:  8   step:  117   train loss:  0.03364667668938637  val loss:  0.03202764317393303\n",
      "epoch:  8   step:  118   train loss:  0.05164671689271927  val loss:  0.03196591138839722\n",
      "epoch:  8   step:  119   train loss:  0.04062008857727051  val loss:  0.03172250837087631\n",
      "epoch:  8   step:  120   train loss:  0.03407806158065796  val loss:  0.031953051686286926\n",
      "epoch:  8   step:  121   train loss:  0.029891055077314377  val loss:  0.03175584226846695\n",
      "epoch:  8   step:  122   train loss:  0.07548821717500687  val loss:  0.03078458458185196\n",
      "min_val_loss_print 0.03078458458185196\n",
      "epoch:  8   step:  123   train loss:  0.04109782353043556  val loss:  0.030624568462371826\n",
      "min_val_loss_print 0.030624568462371826\n",
      "epoch:  8   step:  124   train loss:  0.08020856976509094  val loss:  0.03098350204527378\n",
      "epoch:  8   step:  125   train loss:  0.049581609666347504  val loss:  0.03126959875226021\n",
      "epoch:  8   step:  126   train loss:  0.031030850484967232  val loss:  0.030945265665650368\n",
      "epoch:  8   step:  127   train loss:  0.057691797614097595  val loss:  0.030854538083076477\n",
      "epoch:  8   step:  128   train loss:  0.03926153481006622  val loss:  0.029876047745347023\n",
      "min_val_loss_print 0.029876047745347023\n",
      "epoch:  8   step:  129   train loss:  0.047594763338565826  val loss:  0.02986832894384861\n",
      "min_val_loss_print 0.02986832894384861\n",
      "epoch:  8   step:  130   train loss:  0.047872625291347504  val loss:  0.030095160007476807\n",
      "epoch:  8   step:  131   train loss:  0.03903768211603165  val loss:  0.030036678537726402\n",
      "epoch:  8   step:  132   train loss:  0.04256902262568474  val loss:  0.030059436336159706\n",
      "epoch:  8   step:  133   train loss:  0.03516697138547897  val loss:  0.030774671584367752\n",
      "epoch:  8   step:  134   train loss:  0.03364744782447815  val loss:  0.0305859986692667\n",
      "epoch:  8   step:  135   train loss:  0.041345637291669846  val loss:  0.03085319697856903\n",
      "epoch:  8   step:  136   train loss:  0.044009506702423096  val loss:  0.030800800770521164\n",
      "epoch:  8   step:  137   train loss:  0.054313741624355316  val loss:  0.031134596094489098\n",
      "epoch:  8   step:  138   train loss:  0.05222687870264053  val loss:  0.031972263008356094\n",
      "epoch:  8   step:  139   train loss:  0.034548718482255936  val loss:  0.03241454064846039\n",
      "epoch:  8   step:  140   train loss:  0.06088385730981827  val loss:  0.032862987369298935\n",
      "epoch:  8   step:  141   train loss:  0.04780879244208336  val loss:  0.03293518349528313\n",
      "epoch:  8   step:  142   train loss:  0.030129795894026756  val loss:  0.0320984348654747\n",
      "epoch:  8   step:  143   train loss:  0.04382866993546486  val loss:  0.032333854585886\n",
      "epoch:  8   step:  144   train loss:  0.03131796047091484  val loss:  0.03252098709344864\n",
      "epoch:  8   step:  145   train loss:  0.06364654004573822  val loss:  0.03314103186130524\n",
      "epoch:  8   step:  146   train loss:  0.031080113723874092  val loss:  0.03243575245141983\n",
      "epoch:  8   step:  147   train loss:  0.03982704505324364  val loss:  0.03238511085510254\n",
      "epoch:  8   step:  148   train loss:  0.036488160490989685  val loss:  0.032833948731422424\n",
      "epoch:  8   step:  149   train loss:  0.033483002334833145  val loss:  0.032806284725666046\n",
      "epoch:  8   step:  150   train loss:  0.04244033992290497  val loss:  0.03269784152507782\n",
      "epoch:  8   step:  151   train loss:  0.034618228673934937  val loss:  0.03196069970726967\n",
      "epoch:  8   step:  152   train loss:  0.033927883952856064  val loss:  0.03150197118520737\n",
      "epoch:  8   step:  153   train loss:  0.03368237614631653  val loss:  0.031135551631450653\n",
      "epoch:  8   step:  154   train loss:  0.04830705001950264  val loss:  0.03117610700428486\n",
      "epoch:  8   step:  155   train loss:  0.050827495753765106  val loss:  0.03117109276354313\n",
      "epoch:  8   step:  156   train loss:  0.04608448967337608  val loss:  0.03144386038184166\n",
      "epoch:  8   step:  157   train loss:  0.04985962063074112  val loss:  0.03108837641775608\n",
      "epoch:  8   step:  158   train loss:  0.03095330111682415  val loss:  0.030993090942502022\n",
      "epoch:  8   step:  159   train loss:  0.0304403118789196  val loss:  0.03047226183116436\n",
      "epoch:  8   step:  160   train loss:  0.05154937505722046  val loss:  0.030289100483059883\n",
      "epoch:  8   step:  161   train loss:  0.034355636686086655  val loss:  0.030650760978460312\n",
      "epoch:  8   step:  162   train loss:  0.04333211109042168  val loss:  0.03023342229425907\n",
      "epoch:  8   step:  163   train loss:  0.026255009695887566  val loss:  0.0302494578063488\n",
      "epoch:  8   step:  164   train loss:  0.055542245507240295  val loss:  0.030411554500460625\n",
      "epoch:  8   step:  165   train loss:  0.04270624369382858  val loss:  0.030120201408863068\n",
      "epoch:  8   step:  166   train loss:  0.05335669219493866  val loss:  0.03005269169807434\n",
      "epoch:  8   step:  167   train loss:  0.03609360009431839  val loss:  0.0299572441726923\n",
      "epoch:  8   step:  168   train loss:  0.042228128761053085  val loss:  0.029967380687594414\n",
      "epoch:  8   step:  169   train loss:  0.03382907062768936  val loss:  0.0300875473767519\n",
      "epoch:  8   step:  170   train loss:  0.03824467211961746  val loss:  0.029669802635908127\n",
      "min_val_loss_print 0.029669802635908127\n",
      "epoch:  8   step:  171   train loss:  0.05474087968468666  val loss:  0.02962087094783783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_val_loss_print 0.02962087094783783\n",
      "epoch:  8   step:  172   train loss:  0.038931913673877716  val loss:  0.03019968792796135\n",
      "epoch:  8   step:  173   train loss:  0.06417682766914368  val loss:  0.030153067782521248\n",
      "epoch:  8   step:  174   train loss:  0.04071856662631035  val loss:  0.030238285660743713\n",
      "epoch:  8   step:  175   train loss:  0.05082356557250023  val loss:  0.030431652441620827\n",
      "epoch:  8   step:  176   train loss:  0.07165123522281647  val loss:  0.030466614291071892\n",
      "epoch:  8   step:  177   train loss:  0.04195554181933403  val loss:  0.030465880408883095\n",
      "epoch:  8   step:  178   train loss:  0.06270197033882141  val loss:  0.030181700363755226\n",
      "epoch:  8   step:  179   train loss:  0.041685979813337326  val loss:  0.03005632758140564\n",
      "epoch:  8   step:  180   train loss:  0.0458398312330246  val loss:  0.030034581199288368\n",
      "epoch:  8   step:  181   train loss:  0.03135836496949196  val loss:  0.02947331964969635\n",
      "min_val_loss_print 0.02947331964969635\n",
      "epoch:  8   step:  182   train loss:  0.037952639162540436  val loss:  0.029590236023068428\n",
      "epoch:  8   step:  183   train loss:  0.03447744622826576  val loss:  0.030157672241330147\n",
      "epoch:  8   step:  184   train loss:  0.04979101940989494  val loss:  0.030797887593507767\n",
      "epoch:  8   step:  185   train loss:  0.0451684333384037  val loss:  0.030878113582730293\n",
      "epoch:  8   step:  186   train loss:  0.06124076992273331  val loss:  0.03102821297943592\n",
      "epoch:  8   step:  187   train loss:  0.06884559988975525  val loss:  0.03163647651672363\n",
      "epoch:  8   step:  188   train loss:  0.03426368534564972  val loss:  0.03208532556891441\n",
      "epoch:  8   step:  189   train loss:  0.03357183188199997  val loss:  0.03175831958651543\n",
      "epoch:  8   step:  190   train loss:  0.03286798670887947  val loss:  0.03184376657009125\n",
      "epoch:  8   step:  191   train loss:  0.054235655814409256  val loss:  0.031535107642412186\n",
      "epoch:  8   step:  192   train loss:  0.026632197201251984  val loss:  0.03152783587574959\n",
      "epoch:  8   step:  193   train loss:  0.02636527642607689  val loss:  0.0316418819129467\n",
      "epoch:  8   step:  194   train loss:  0.054730869829654694  val loss:  0.031535081565380096\n",
      "epoch:  8   step:  195   train loss:  0.05900563299655914  val loss:  0.03130696341395378\n",
      "epoch:  8   step:  196   train loss:  0.06370960175991058  val loss:  0.030810585245490074\n",
      "epoch:  8   step:  197   train loss:  0.043443046510219574  val loss:  0.03137066960334778\n",
      "epoch:  8   step:  198   train loss:  0.037405695766210556  val loss:  0.031066128984093666\n",
      "epoch:  8   step:  199   train loss:  0.07336069643497467  val loss:  0.031169407069683075\n",
      "epoch:  8   step:  200   train loss:  0.035086896270513535  val loss:  0.031018825247883797\n",
      "epoch:  8   step:  201   train loss:  0.037777192890644073  val loss:  0.030879899859428406\n",
      "epoch:  8   step:  202   train loss:  0.021300118416547775  val loss:  0.030808668583631516\n",
      "epoch:  8   step:  203   train loss:  0.04500393941998482  val loss:  0.030510686337947845\n",
      "epoch:  8   step:  204   train loss:  0.03671693429350853  val loss:  0.030632929876446724\n",
      "epoch:  8   step:  205   train loss:  0.041220370680093765  val loss:  0.03101877123117447\n",
      "epoch:  8   step:  206   train loss:  0.04545766860246658  val loss:  0.03132060915231705\n",
      "epoch:  8   step:  207   train loss:  0.05033469572663307  val loss:  0.03149963915348053\n",
      "epoch:  8   step:  208   train loss:  0.0405595563352108  val loss:  0.03166182339191437\n",
      "epoch:  8   step:  209   train loss:  0.04646090418100357  val loss:  0.03124512918293476\n",
      "epoch:  8   step:  210   train loss:  0.048574235290288925  val loss:  0.031309980899095535\n",
      "epoch:  8   step:  211   train loss:  0.05675898492336273  val loss:  0.031348660588264465\n",
      "epoch:  8   step:  212   train loss:  0.0333169586956501  val loss:  0.0312158465385437\n",
      "epoch:  8   step:  213   train loss:  0.053894124925136566  val loss:  0.03126107156276703\n",
      "epoch:  8   step:  214   train loss:  0.053295768797397614  val loss:  0.031091012060642242\n",
      "epoch:  8   step:  215   train loss:  0.05116387456655502  val loss:  0.030798451974987984\n",
      "epoch:  8   step:  216   train loss:  0.03460684046149254  val loss:  0.031167149543762207\n",
      "epoch:  8   step:  217   train loss:  0.07125286757946014  val loss:  0.03103257715702057\n",
      "epoch:  8   step:  218   train loss:  0.030926870182156563  val loss:  0.03162354230880737\n",
      "epoch:  8   step:  219   train loss:  0.03510681167244911  val loss:  0.03170842304825783\n",
      "epoch:  8   step:  220   train loss:  0.034281786531209946  val loss:  0.03164259344339371\n",
      "epoch:  8   step:  221   train loss:  0.05909030884504318  val loss:  0.030992930755019188\n",
      "epoch:  8   step:  222   train loss:  0.02329103834927082  val loss:  0.030746612697839737\n",
      "epoch:  8   step:  223   train loss:  0.03159727901220322  val loss:  0.031009435653686523\n",
      "epoch:  8   step:  224   train loss:  0.05009071156382561  val loss:  0.031200788915157318\n",
      "epoch:  8   step:  225   train loss:  0.0426134392619133  val loss:  0.03125368431210518\n",
      "epoch:  8   step:  226   train loss:  0.026488715782761574  val loss:  0.03134136646986008\n",
      "epoch:  8   step:  227   train loss:  0.03147268667817116  val loss:  0.03077896684408188\n",
      "epoch:  8   step:  228   train loss:  0.04407747462391853  val loss:  0.03077894076704979\n",
      "epoch:  8   step:  229   train loss:  0.022918159142136574  val loss:  0.030768338590860367\n",
      "epoch:  8   step:  230   train loss:  0.0472579300403595  val loss:  0.03073434717953205\n",
      "epoch:  8   step:  231   train loss:  0.05151746794581413  val loss:  0.030975311994552612\n",
      "epoch:  8   step:  232   train loss:  0.04658391326665878  val loss:  0.03017035312950611\n",
      "epoch:  8   step:  233   train loss:  0.042450226843357086  val loss:  0.029911624267697334\n",
      "epoch:  8   step:  234   train loss:  0.03748825937509537  val loss:  0.030220534652471542\n",
      "epoch:  8   step:  235   train loss:  0.04988718032836914  val loss:  0.02965807169675827\n",
      "epoch:  8   step:  236   train loss:  0.036241721361875534  val loss:  0.030269891023635864\n",
      "epoch:  8   step:  237   train loss:  0.04753212258219719  val loss:  0.030501505360007286\n",
      "epoch:  8   step:  238   train loss:  0.027725839987397194  val loss:  0.030398016795516014\n",
      "epoch:  8   step:  239   train loss:  0.0459449365735054  val loss:  0.03055054135620594\n",
      "epoch:  8   step:  240   train loss:  0.0508669950067997  val loss:  0.030376631766557693\n",
      "epoch:  8   step:  241   train loss:  0.05058325082063675  val loss:  0.030146656557917595\n",
      "epoch:  8   step:  242   train loss:  0.031778816133737564  val loss:  0.029827190563082695\n",
      "epoch:  8   step:  243   train loss:  0.05057702958583832  val loss:  0.029404234141111374\n",
      "min_val_loss_print 0.029404234141111374\n",
      "epoch:  8   step:  244   train loss:  0.030721014365553856  val loss:  0.029333466663956642\n",
      "min_val_loss_print 0.029333466663956642\n",
      "epoch:  8   step:  245   train loss:  0.03580758720636368  val loss:  0.029754290357232094\n",
      "epoch:  8   step:  246   train loss:  0.04326985031366348  val loss:  0.030010970309376717\n",
      "epoch:  8   step:  247   train loss:  0.034627191722393036  val loss:  0.030113859102129936\n",
      "epoch:  8   step:  248   train loss:  0.02547428198158741  val loss:  0.02996276132762432\n",
      "epoch:  8   step:  249   train loss:  0.028456667438149452  val loss:  0.029860898852348328\n",
      "epoch:  8   step:  250   train loss:  0.03422991931438446  val loss:  0.0300203375518322\n",
      "epoch:  8   step:  251   train loss:  0.030399342998862267  val loss:  0.03003815934062004\n",
      "epoch:  8   step:  252   train loss:  0.02844615839421749  val loss:  0.03028874658048153\n",
      "epoch:  8   step:  253   train loss:  0.05875372886657715  val loss:  0.03038022294640541\n",
      "epoch:  8   step:  254   train loss:  0.028320498764514923  val loss:  0.029984312132000923\n",
      "epoch:  8   step:  255   train loss:  0.03315982595086098  val loss:  0.030015192925930023\n",
      "epoch:  8   step:  256   train loss:  0.023322826251387596  val loss:  0.030451709404587746\n",
      "epoch:  8   step:  257   train loss:  0.037520043551921844  val loss:  0.029857372865080833\n",
      "epoch:  8   step:  258   train loss:  0.03294619545340538  val loss:  0.029987961053848267\n",
      "epoch:  8   step:  259   train loss:  0.04165247455239296  val loss:  0.030116457492113113\n",
      "epoch:  8   step:  260   train loss:  0.04099399968981743  val loss:  0.030484141781926155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  8   step:  261   train loss:  0.04477726295590401  val loss:  0.030463648959994316\n",
      "epoch:  8   step:  262   train loss:  0.05881686508655548  val loss:  0.030442550778388977\n",
      "epoch:  8   step:  263   train loss:  0.034638192504644394  val loss:  0.03000953420996666\n",
      "epoch:  8   step:  264   train loss:  0.023409726098179817  val loss:  0.029733024537563324\n",
      "epoch:  8   step:  265   train loss:  0.050259582698345184  val loss:  0.02985353395342827\n",
      "epoch:  8   step:  266   train loss:  0.03690379112958908  val loss:  0.029713191092014313\n",
      "epoch:  8   step:  267   train loss:  0.033754877746105194  val loss:  0.02980927936732769\n",
      "epoch:  8   step:  268   train loss:  0.0694594606757164  val loss:  0.02978198044002056\n",
      "epoch:  8   step:  269   train loss:  0.04942821338772774  val loss:  0.030542204156517982\n",
      "epoch:  8   step:  270   train loss:  0.03298717737197876  val loss:  0.030381113290786743\n",
      "epoch:  8   step:  271   train loss:  0.04731149598956108  val loss:  0.030486147850751877\n",
      "epoch:  8   step:  272   train loss:  0.06604903936386108  val loss:  0.030009448528289795\n",
      "epoch:  8   step:  273   train loss:  0.03663061186671257  val loss:  0.030003622174263\n",
      "epoch:  8   step:  274   train loss:  0.02076098322868347  val loss:  0.02959391102194786\n",
      "epoch:  8   step:  275   train loss:  0.02928261272609234  val loss:  0.02943263202905655\n",
      "epoch:  8   step:  276   train loss:  0.07912728190422058  val loss:  0.0291014201939106\n",
      "min_val_loss_print 0.0291014201939106\n",
      "epoch:  8   step:  277   train loss:  0.05518870800733566  val loss:  0.028492141515016556\n",
      "min_val_loss_print 0.028492141515016556\n",
      "epoch:  8   step:  278   train loss:  0.020338842645287514  val loss:  0.028365328907966614\n",
      "min_val_loss_print 0.028365328907966614\n",
      "epoch:  8   step:  279   train loss:  0.052608560770750046  val loss:  0.02828751690685749\n",
      "min_val_loss_print 0.02828751690685749\n",
      "epoch:  8   step:  280   train loss:  0.05143987387418747  val loss:  0.02879282645881176\n",
      "epoch:  8   step:  281   train loss:  0.016090769320726395  val loss:  0.029006635770201683\n",
      "epoch:  8   step:  282   train loss:  0.05215729773044586  val loss:  0.02897793985903263\n",
      "epoch:  8   step:  283   train loss:  0.037847016006708145  val loss:  0.029079904779791832\n",
      "epoch:  8   step:  284   train loss:  0.0432436466217041  val loss:  0.028626900166273117\n",
      "epoch:  8   step:  285   train loss:  0.051698535680770874  val loss:  0.029166365042328835\n",
      "epoch:  8   step:  286   train loss:  0.035502150654792786  val loss:  0.029435116797685623\n",
      "epoch:  8   step:  287   train loss:  0.061442550271749496  val loss:  0.029766537249088287\n",
      "epoch:  8   step:  288   train loss:  0.03331766277551651  val loss:  0.030136557295918465\n",
      "epoch:  8   step:  289   train loss:  0.051484815776348114  val loss:  0.02953963167965412\n",
      "epoch:  8   step:  290   train loss:  0.043473534286022186  val loss:  0.030675891786813736\n",
      "epoch:  8   step:  291   train loss:  0.02868570201098919  val loss:  0.029554981738328934\n",
      "epoch:  8   step:  292   train loss:  0.03874151036143303  val loss:  0.029192011803388596\n",
      "epoch:  8   step:  293   train loss:  0.035409171134233475  val loss:  0.029108501970767975\n",
      "epoch:  8   step:  294   train loss:  0.052790120244026184  val loss:  0.02894924208521843\n",
      "epoch:  8   step:  295   train loss:  0.037012822926044464  val loss:  0.028884710744023323\n",
      "epoch:  8   step:  296   train loss:  0.03865750879049301  val loss:  0.02876034565269947\n",
      "epoch:  8   step:  297   train loss:  0.04450179263949394  val loss:  0.02873481623828411\n",
      "epoch:  8   step:  298   train loss:  0.03760385140776634  val loss:  0.02867119386792183\n",
      "epoch:  8   step:  299   train loss:  0.03367183730006218  val loss:  0.028379859402775764\n",
      "epoch:  8   step:  300   train loss:  0.04333638772368431  val loss:  0.027450941503047943\n",
      "min_val_loss_print 0.027450941503047943\n",
      "epoch:  8   step:  301   train loss:  0.03155776858329773  val loss:  0.027795838192105293\n",
      "epoch:  8   step:  302   train loss:  0.0523611344397068  val loss:  0.028076063841581345\n",
      "epoch:  8   step:  303   train loss:  0.05067860335111618  val loss:  0.028074005618691444\n",
      "epoch:  8   step:  304   train loss:  0.03354563191533089  val loss:  0.02811727300286293\n",
      "epoch:  8   step:  305   train loss:  0.05348719283938408  val loss:  0.02788763865828514\n",
      "epoch:  8   step:  306   train loss:  0.058858562260866165  val loss:  0.028206966817378998\n",
      "epoch:  8   step:  307   train loss:  0.03849950060248375  val loss:  0.027904612943530083\n",
      "epoch:  8   step:  308   train loss:  0.054166875779628754  val loss:  0.027845267206430435\n",
      "epoch:  8   step:  309   train loss:  0.06170286610722542  val loss:  0.027606993913650513\n",
      "epoch:  8   step:  310   train loss:  0.03645174950361252  val loss:  0.02758408896625042\n",
      "epoch:  8   step:  311   train loss:  0.06416415423154831  val loss:  0.027513427659869194\n",
      "epoch:  8   step:  312   train loss:  0.03562868386507034  val loss:  0.027617841958999634\n",
      "epoch:  8   step:  313   train loss:  0.056135743856430054  val loss:  0.02791711688041687\n",
      "epoch:  8   step:  314   train loss:  0.03821263462305069  val loss:  0.027989137917757034\n",
      "epoch:  8   step:  315   train loss:  0.034427426755428314  val loss:  0.027936995029449463\n",
      "epoch:  8   step:  316   train loss:  0.04440000280737877  val loss:  0.027737099677324295\n",
      "epoch:  8   step:  317   train loss:  0.03778038173913956  val loss:  0.028179628774523735\n",
      "epoch:  8   step:  318   train loss:  0.04096290096640587  val loss:  0.028477653861045837\n",
      "epoch:  8   step:  319   train loss:  0.05347145348787308  val loss:  0.028595851734280586\n",
      "epoch:  8   step:  320   train loss:  0.03851936385035515  val loss:  0.028449658304452896\n",
      "epoch:  8   step:  321   train loss:  0.045575398951768875  val loss:  0.028457002714276314\n",
      "epoch:  8   step:  322   train loss:  0.046044230461120605  val loss:  0.02841958776116371\n",
      "epoch:  8   step:  323   train loss:  0.03572231903672218  val loss:  0.027845239266753197\n",
      "epoch:  8   step:  324   train loss:  0.05293158441781998  val loss:  0.028532350435853004\n",
      "epoch:  8   step:  325   train loss:  0.026518292725086212  val loss:  0.028347959741950035\n",
      "epoch:  8   step:  326   train loss:  0.06832126528024673  val loss:  0.028587235137820244\n",
      "epoch:  8   step:  327   train loss:  0.02302577532827854  val loss:  0.02851931005716324\n",
      "epoch:  8   step:  328   train loss:  0.05383506044745445  val loss:  0.02830970287322998\n",
      "epoch:  8   step:  329   train loss:  0.04124344512820244  val loss:  0.028382692486047745\n",
      "epoch:  8   step:  330   train loss:  0.06526579707860947  val loss:  0.0284418985247612\n",
      "epoch:  8   step:  331   train loss:  0.04724594205617905  val loss:  0.028602980077266693\n",
      "epoch:  8   step:  332   train loss:  0.05967981740832329  val loss:  0.028571011498570442\n",
      "epoch:  8   step:  333   train loss:  0.05075850337743759  val loss:  0.02895822376012802\n",
      "epoch:  8   step:  334   train loss:  0.028987577185034752  val loss:  0.02848486229777336\n",
      "epoch:  8   step:  335   train loss:  0.04687010124325752  val loss:  0.029051639139652252\n",
      "epoch:  8   step:  336   train loss:  0.03093676082789898  val loss:  0.02867373451590538\n",
      "epoch:  8   step:  337   train loss:  0.05233980342745781  val loss:  0.02886238507926464\n",
      "epoch:  8   step:  338   train loss:  0.04667338728904724  val loss:  0.02827385440468788\n",
      "epoch:  8   step:  339   train loss:  0.05966746434569359  val loss:  0.028485912829637527\n",
      "epoch:  8   step:  340   train loss:  0.04599132016301155  val loss:  0.02853180468082428\n",
      "epoch:  8   step:  341   train loss:  0.029816830530762672  val loss:  0.028687993064522743\n",
      "epoch:  8   step:  342   train loss:  0.04772527888417244  val loss:  0.028727974742650986\n",
      "epoch:  8   step:  343   train loss:  0.026046080514788628  val loss:  0.027650868520140648\n",
      "epoch:  9   step:  0   train loss:  0.04311919957399368  val loss:  0.02779879793524742\n",
      "epoch:  9   step:  1   train loss:  0.044208355247974396  val loss:  0.02757401578128338\n",
      "epoch:  9   step:  2   train loss:  0.02484903670847416  val loss:  0.02799876406788826\n",
      "epoch:  9   step:  3   train loss:  0.03632291778922081  val loss:  0.02790844812989235\n",
      "epoch:  9   step:  4   train loss:  0.0367947556078434  val loss:  0.027859138324856758\n",
      "epoch:  9   step:  5   train loss:  0.03988612815737724  val loss:  0.02819126285612583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  9   step:  6   train loss:  0.03150039166212082  val loss:  0.028397705405950546\n",
      "epoch:  9   step:  7   train loss:  0.03273657336831093  val loss:  0.02846316620707512\n",
      "epoch:  9   step:  8   train loss:  0.058020103722810745  val loss:  0.028647325932979584\n",
      "epoch:  9   step:  9   train loss:  0.022871963679790497  val loss:  0.028493551537394524\n",
      "epoch:  9   step:  10   train loss:  0.03883220627903938  val loss:  0.028473995625972748\n",
      "epoch:  9   step:  11   train loss:  0.05497824028134346  val loss:  0.028768805786967278\n",
      "epoch:  9   step:  12   train loss:  0.0382566899061203  val loss:  0.029218684881925583\n",
      "epoch:  9   step:  13   train loss:  0.031995613127946854  val loss:  0.029009899124503136\n",
      "epoch:  9   step:  14   train loss:  0.03468232601881027  val loss:  0.02915680967271328\n",
      "epoch:  9   step:  15   train loss:  0.045559532940387726  val loss:  0.02998427301645279\n",
      "epoch:  9   step:  16   train loss:  0.05306950956583023  val loss:  0.0302444901317358\n",
      "epoch:  9   step:  17   train loss:  0.033496733754873276  val loss:  0.029908066615462303\n",
      "epoch:  9   step:  18   train loss:  0.020050959661602974  val loss:  0.029104480519890785\n",
      "epoch:  9   step:  19   train loss:  0.02163616009056568  val loss:  0.02934439666569233\n",
      "epoch:  9   step:  20   train loss:  0.03170561045408249  val loss:  0.028027288615703583\n",
      "epoch:  9   step:  21   train loss:  0.03801097348332405  val loss:  0.028070304542779922\n",
      "epoch:  9   step:  22   train loss:  0.05096622556447983  val loss:  0.028203772380948067\n",
      "epoch:  9   step:  23   train loss:  0.04876469820737839  val loss:  0.02834474854171276\n",
      "epoch:  9   step:  24   train loss:  0.055476099252700806  val loss:  0.027780795469880104\n",
      "epoch:  9   step:  25   train loss:  0.019457846879959106  val loss:  0.027826128527522087\n",
      "epoch:  9   step:  26   train loss:  0.0334024615585804  val loss:  0.02784797176718712\n",
      "epoch:  9   step:  27   train loss:  0.03909384831786156  val loss:  0.027614712715148926\n",
      "epoch:  9   step:  28   train loss:  0.03933844342827797  val loss:  0.027546925470232964\n",
      "epoch:  9   step:  29   train loss:  0.05816616863012314  val loss:  0.027646075934171677\n",
      "epoch:  9   step:  30   train loss:  0.05854704976081848  val loss:  0.026915766298770905\n",
      "min_val_loss_print 0.026915766298770905\n",
      "epoch:  9   step:  31   train loss:  0.03404252603650093  val loss:  0.026538493111729622\n",
      "min_val_loss_print 0.026538493111729622\n",
      "epoch:  9   step:  32   train loss:  0.03302619606256485  val loss:  0.026707837358117104\n",
      "epoch:  9   step:  33   train loss:  0.03518177196383476  val loss:  0.026977738365530968\n",
      "epoch:  9   step:  34   train loss:  0.06943747401237488  val loss:  0.026564089581370354\n",
      "epoch:  9   step:  35   train loss:  0.05585567280650139  val loss:  0.02645413763821125\n",
      "min_val_loss_print 0.02645413763821125\n",
      "epoch:  9   step:  36   train loss:  0.033772002905607224  val loss:  0.026667704805731773\n",
      "epoch:  9   step:  37   train loss:  0.03696988895535469  val loss:  0.026421424001455307\n",
      "min_val_loss_print 0.026421424001455307\n",
      "epoch:  9   step:  38   train loss:  0.025028107687830925  val loss:  0.026281647384166718\n",
      "min_val_loss_print 0.026281647384166718\n",
      "epoch:  9   step:  39   train loss:  0.03949457406997681  val loss:  0.025977451354265213\n",
      "min_val_loss_print 0.025977451354265213\n",
      "epoch:  9   step:  40   train loss:  0.02502709999680519  val loss:  0.02638828009366989\n",
      "epoch:  9   step:  41   train loss:  0.038236018270254135  val loss:  0.02605821006000042\n",
      "epoch:  9   step:  42   train loss:  0.027925346046686172  val loss:  0.025751762092113495\n",
      "min_val_loss_print 0.025751762092113495\n",
      "epoch:  9   step:  43   train loss:  0.030999531969428062  val loss:  0.02591099962592125\n",
      "epoch:  9   step:  44   train loss:  0.030651085078716278  val loss:  0.026027511805295944\n",
      "epoch:  9   step:  45   train loss:  0.045678477734327316  val loss:  0.026271922513842583\n",
      "epoch:  9   step:  46   train loss:  0.044827815145254135  val loss:  0.026560425758361816\n",
      "epoch:  9   step:  47   train loss:  0.04449407383799553  val loss:  0.026118280366063118\n",
      "epoch:  9   step:  48   train loss:  0.026919294148683548  val loss:  0.02659590356051922\n",
      "epoch:  9   step:  49   train loss:  0.03730003535747528  val loss:  0.026928186416625977\n",
      "epoch:  9   step:  50   train loss:  0.07776158303022385  val loss:  0.027033833786845207\n",
      "epoch:  9   step:  51   train loss:  0.047509998083114624  val loss:  0.027818873524665833\n",
      "epoch:  9   step:  52   train loss:  0.04946513846516609  val loss:  0.027350060641765594\n",
      "epoch:  9   step:  53   train loss:  0.053280360996723175  val loss:  0.026935264468193054\n",
      "epoch:  9   step:  54   train loss:  0.0319414883852005  val loss:  0.02694346383213997\n",
      "epoch:  9   step:  55   train loss:  0.02730061672627926  val loss:  0.02753126248717308\n",
      "epoch:  9   step:  56   train loss:  0.04525936394929886  val loss:  0.027872124686837196\n",
      "epoch:  9   step:  57   train loss:  0.05078733712434769  val loss:  0.027610598132014275\n",
      "epoch:  9   step:  58   train loss:  0.02769828587770462  val loss:  0.02807900682091713\n",
      "epoch:  9   step:  59   train loss:  0.0313609354197979  val loss:  0.028505882248282433\n",
      "epoch:  9   step:  60   train loss:  0.05321073904633522  val loss:  0.0288565531373024\n",
      "epoch:  9   step:  61   train loss:  0.04625290632247925  val loss:  0.029031911864876747\n",
      "epoch:  9   step:  62   train loss:  0.035869453102350235  val loss:  0.029276885092258453\n",
      "epoch:  9   step:  63   train loss:  0.0374678336083889  val loss:  0.029228663071990013\n",
      "epoch:  9   step:  64   train loss:  0.04791707918047905  val loss:  0.02913045696914196\n",
      "epoch:  9   step:  65   train loss:  0.043545354157686234  val loss:  0.029257502406835556\n",
      "epoch:  9   step:  66   train loss:  0.03496482968330383  val loss:  0.028970839455723763\n",
      "epoch:  9   step:  67   train loss:  0.028573375195264816  val loss:  0.028851360082626343\n",
      "epoch:  9   step:  68   train loss:  0.0650971457362175  val loss:  0.028833355754613876\n",
      "epoch:  9   step:  69   train loss:  0.04351537674665451  val loss:  0.0288368072360754\n",
      "epoch:  9   step:  70   train loss:  0.036423902958631516  val loss:  0.028047481551766396\n",
      "epoch:  9   step:  71   train loss:  0.03249422088265419  val loss:  0.027721062302589417\n",
      "epoch:  9   step:  72   train loss:  0.024263858795166016  val loss:  0.027887968346476555\n",
      "epoch:  9   step:  73   train loss:  0.04791548103094101  val loss:  0.027043459936976433\n",
      "epoch:  9   step:  74   train loss:  0.08307518064975739  val loss:  0.027017217129468918\n",
      "epoch:  9   step:  75   train loss:  0.04624130576848984  val loss:  0.0270376019179821\n",
      "epoch:  9   step:  76   train loss:  0.025644732639193535  val loss:  0.026857595890760422\n",
      "epoch:  9   step:  77   train loss:  0.03496244549751282  val loss:  0.02728600800037384\n",
      "epoch:  9   step:  78   train loss:  0.04205356538295746  val loss:  0.027404261752963066\n",
      "epoch:  9   step:  79   train loss:  0.044133510440588  val loss:  0.027466995641589165\n",
      "epoch:  9   step:  80   train loss:  0.052008457481861115  val loss:  0.027904720976948738\n",
      "epoch:  9   step:  81   train loss:  0.04119284823536873  val loss:  0.027485068887472153\n",
      "epoch:  9   step:  82   train loss:  0.055489033460617065  val loss:  0.027449021115899086\n",
      "epoch:  9   step:  83   train loss:  0.06285879760980606  val loss:  0.027544710785150528\n",
      "epoch:  9   step:  84   train loss:  0.034522224217653275  val loss:  0.027729718014597893\n",
      "epoch:  9   step:  85   train loss:  0.0392354279756546  val loss:  0.027494411915540695\n",
      "epoch:  9   step:  86   train loss:  0.02705417014658451  val loss:  0.0276387557387352\n",
      "epoch:  9   step:  87   train loss:  0.0330972857773304  val loss:  0.02729286253452301\n",
      "epoch:  9   step:  88   train loss:  0.03863866999745369  val loss:  0.027242496609687805\n",
      "epoch:  9   step:  89   train loss:  0.05907075107097626  val loss:  0.02748844213783741\n",
      "epoch:  9   step:  90   train loss:  0.02811732143163681  val loss:  0.027175113558769226\n",
      "epoch:  9   step:  91   train loss:  0.04447877034544945  val loss:  0.027930689975619316\n",
      "epoch:  9   step:  92   train loss:  0.028914082795381546  val loss:  0.02753741294145584\n",
      "epoch:  9   step:  93   train loss:  0.030710794031620026  val loss:  0.02722380869090557\n",
      "epoch:  9   step:  94   train loss:  0.06273309886455536  val loss:  0.02725829742848873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  9   step:  95   train loss:  0.055277012288570404  val loss:  0.027530919760465622\n",
      "epoch:  9   step:  96   train loss:  0.04224427789449692  val loss:  0.02745032124221325\n",
      "epoch:  9   step:  97   train loss:  0.0262350644916296  val loss:  0.027801774442195892\n",
      "epoch:  9   step:  98   train loss:  0.026379650458693504  val loss:  0.0282753873616457\n",
      "epoch:  9   step:  99   train loss:  0.03299205005168915  val loss:  0.02796196937561035\n",
      "epoch:  9   step:  100   train loss:  0.043387819081544876  val loss:  0.028148125857114792\n",
      "epoch:  9   step:  101   train loss:  0.035019807517528534  val loss:  0.028879305347800255\n",
      "epoch:  9   step:  102   train loss:  0.03031117655336857  val loss:  0.02957061491906643\n",
      "epoch:  9   step:  103   train loss:  0.04667291417717934  val loss:  0.030060576274991035\n",
      "epoch:  9   step:  104   train loss:  0.03805580362677574  val loss:  0.029848165810108185\n",
      "epoch:  9   step:  105   train loss:  0.03522319719195366  val loss:  0.028801439329981804\n",
      "epoch:  9   step:  106   train loss:  0.02768554352223873  val loss:  0.02883208356797695\n",
      "epoch:  9   step:  107   train loss:  0.04034481197595596  val loss:  0.02922431007027626\n",
      "epoch:  9   step:  108   train loss:  0.01774716190993786  val loss:  0.029284076765179634\n",
      "epoch:  9   step:  109   train loss:  0.046126432716846466  val loss:  0.029165687039494514\n",
      "epoch:  9   step:  110   train loss:  0.07309511303901672  val loss:  0.029314186424016953\n",
      "epoch:  9   step:  111   train loss:  0.031088318675756454  val loss:  0.029278600588440895\n",
      "epoch:  9   step:  112   train loss:  0.061159707605838776  val loss:  0.029044797644019127\n",
      "epoch:  9   step:  113   train loss:  0.03534052148461342  val loss:  0.02880535088479519\n",
      "epoch:  9   step:  114   train loss:  0.023167388513684273  val loss:  0.028850387781858444\n",
      "epoch:  9   step:  115   train loss:  0.038876693695783615  val loss:  0.029014021158218384\n",
      "epoch:  9   step:  116   train loss:  0.019291462376713753  val loss:  0.02883858047425747\n",
      "epoch:  9   step:  117   train loss:  0.04204663634300232  val loss:  0.02821422927081585\n",
      "epoch:  9   step:  118   train loss:  0.029534325003623962  val loss:  0.02758975885808468\n",
      "epoch:  9   step:  119   train loss:  0.02685718797147274  val loss:  0.027529584243893623\n",
      "epoch:  9   step:  120   train loss:  0.04478751868009567  val loss:  0.02820264920592308\n",
      "epoch:  9   step:  121   train loss:  0.038312528282403946  val loss:  0.028717676177620888\n",
      "epoch:  9   step:  122   train loss:  0.03553388640284538  val loss:  0.02908896468579769\n",
      "epoch:  9   step:  123   train loss:  0.043153420090675354  val loss:  0.028812527656555176\n",
      "epoch:  9   step:  124   train loss:  0.038699157536029816  val loss:  0.028416337445378304\n",
      "epoch:  9   step:  125   train loss:  0.03497886285185814  val loss:  0.027918625622987747\n",
      "epoch:  9   step:  126   train loss:  0.02860897034406662  val loss:  0.027680477127432823\n",
      "epoch:  9   step:  127   train loss:  0.029307998716831207  val loss:  0.027682414278388023\n",
      "epoch:  9   step:  128   train loss:  0.07179851830005646  val loss:  0.028065672144293785\n",
      "epoch:  9   step:  129   train loss:  0.05331091582775116  val loss:  0.027851687744259834\n",
      "epoch:  9   step:  130   train loss:  0.02496456354856491  val loss:  0.02797211892902851\n",
      "epoch:  9   step:  131   train loss:  0.04360121861100197  val loss:  0.02773800678551197\n",
      "epoch:  9   step:  132   train loss:  0.04191681370139122  val loss:  0.027089720591902733\n",
      "epoch:  9   step:  133   train loss:  0.037443798035383224  val loss:  0.026749376207590103\n",
      "epoch:  9   step:  134   train loss:  0.024833302944898605  val loss:  0.02647312916815281\n",
      "epoch:  9   step:  135   train loss:  0.04582519829273224  val loss:  0.026155399158596992\n",
      "epoch:  9   step:  136   train loss:  0.045020416378974915  val loss:  0.025995125994086266\n",
      "epoch:  9   step:  137   train loss:  0.05808500945568085  val loss:  0.026147866621613503\n",
      "epoch:  9   step:  138   train loss:  0.03487056866288185  val loss:  0.0263837743550539\n",
      "epoch:  9   step:  139   train loss:  0.028520656749606133  val loss:  0.027075115591287613\n",
      "epoch:  9   step:  140   train loss:  0.03029327094554901  val loss:  0.027194393798708916\n",
      "epoch:  9   step:  141   train loss:  0.04030155390501022  val loss:  0.027225187048316002\n",
      "epoch:  9   step:  142   train loss:  0.026858139783143997  val loss:  0.027195772156119347\n",
      "epoch:  9   step:  143   train loss:  0.02601921744644642  val loss:  0.02778875268995762\n",
      "epoch:  9   step:  144   train loss:  0.03199537843465805  val loss:  0.027699658647179604\n",
      "epoch:  9   step:  145   train loss:  0.037149857729673386  val loss:  0.027916433289647102\n",
      "epoch:  9   step:  146   train loss:  0.03090849705040455  val loss:  0.027864554896950722\n",
      "epoch:  9   step:  147   train loss:  0.0529668927192688  val loss:  0.028311392292380333\n",
      "epoch:  9   step:  148   train loss:  0.025341879576444626  val loss:  0.028375010937452316\n",
      "epoch:  9   step:  149   train loss:  0.03624472767114639  val loss:  0.028561130166053772\n",
      "epoch:  9   step:  150   train loss:  0.018973657861351967  val loss:  0.028315730392932892\n",
      "epoch:  9   step:  151   train loss:  0.025793978944420815  val loss:  0.028225211426615715\n",
      "epoch:  9   step:  152   train loss:  0.03636757284402847  val loss:  0.02774694189429283\n",
      "epoch:  9   step:  153   train loss:  0.043140023946762085  val loss:  0.02787189558148384\n",
      "epoch:  9   step:  154   train loss:  0.042443931102752686  val loss:  0.02808164618909359\n",
      "epoch:  9   step:  155   train loss:  0.03295103833079338  val loss:  0.028122834861278534\n",
      "epoch:  9   step:  156   train loss:  0.02386952005326748  val loss:  0.02825816534459591\n",
      "epoch:  9   step:  157   train loss:  0.06511979550123215  val loss:  0.028397640213370323\n",
      "epoch:  9   step:  158   train loss:  0.03515777364373207  val loss:  0.028330789878964424\n",
      "epoch:  9   step:  159   train loss:  0.07552412152290344  val loss:  0.02797696553170681\n",
      "epoch:  9   step:  160   train loss:  0.0459393709897995  val loss:  0.027800343930721283\n",
      "epoch:  9   step:  161   train loss:  0.048151224851608276  val loss:  0.028380557894706726\n",
      "epoch:  9   step:  162   train loss:  0.056782204657793045  val loss:  0.028851749375462532\n",
      "epoch:  9   step:  163   train loss:  0.036927223205566406  val loss:  0.02812182530760765\n",
      "epoch:  9   step:  164   train loss:  0.04316270351409912  val loss:  0.027351014316082\n",
      "epoch:  9   step:  165   train loss:  0.04533468931913376  val loss:  0.027094759047031403\n",
      "epoch:  9   step:  166   train loss:  0.028993915766477585  val loss:  0.02718363329768181\n",
      "epoch:  9   step:  167   train loss:  0.03337214142084122  val loss:  0.026837820187211037\n",
      "epoch:  9   step:  168   train loss:  0.023897824808955193  val loss:  0.026560578495264053\n",
      "epoch:  9   step:  169   train loss:  0.0630827397108078  val loss:  0.026261257007718086\n",
      "epoch:  9   step:  170   train loss:  0.02780856564640999  val loss:  0.026349663734436035\n",
      "epoch:  9   step:  171   train loss:  0.02764490805566311  val loss:  0.026104507967829704\n",
      "epoch:  9   step:  172   train loss:  0.0353727713227272  val loss:  0.026188047602772713\n",
      "epoch:  9   step:  173   train loss:  0.06032757833600044  val loss:  0.02599894255399704\n",
      "epoch:  9   step:  174   train loss:  0.059335727244615555  val loss:  0.025454258546233177\n",
      "min_val_loss_print 0.025454258546233177\n",
      "epoch:  9   step:  175   train loss:  0.041284848004579544  val loss:  0.025857051834464073\n",
      "epoch:  9   step:  176   train loss:  0.025500573217868805  val loss:  0.02572125568985939\n",
      "epoch:  9   step:  177   train loss:  0.04791068658232689  val loss:  0.025962965562939644\n",
      "epoch:  9   step:  178   train loss:  0.029085692018270493  val loss:  0.026134630665183067\n",
      "epoch:  9   step:  179   train loss:  0.03616451099514961  val loss:  0.025982001796364784\n",
      "epoch:  9   step:  180   train loss:  0.03658748418092728  val loss:  0.02631949447095394\n",
      "epoch:  9   step:  181   train loss:  0.029303163290023804  val loss:  0.026062117889523506\n",
      "epoch:  9   step:  182   train loss:  0.02944008633494377  val loss:  0.026615140959620476\n",
      "epoch:  9   step:  183   train loss:  0.027857523411512375  val loss:  0.026335017755627632\n",
      "epoch:  9   step:  184   train loss:  0.04109673947095871  val loss:  0.025896597653627396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  9   step:  185   train loss:  0.04117072373628616  val loss:  0.025951365008950233\n",
      "epoch:  9   step:  186   train loss:  0.06749486923217773  val loss:  0.026889337226748466\n",
      "epoch:  9   step:  187   train loss:  0.03249368444085121  val loss:  0.02750297263264656\n",
      "epoch:  9   step:  188   train loss:  0.050824690610170364  val loss:  0.02716265618801117\n",
      "epoch:  9   step:  189   train loss:  0.026138177141547203  val loss:  0.026862191036343575\n",
      "epoch:  9   step:  190   train loss:  0.03508634865283966  val loss:  0.02703734114766121\n",
      "epoch:  9   step:  191   train loss:  0.04319467768073082  val loss:  0.027197590097784996\n",
      "epoch:  9   step:  192   train loss:  0.06343276053667068  val loss:  0.027348896488547325\n",
      "epoch:  9   step:  193   train loss:  0.049524497240781784  val loss:  0.02692512422800064\n",
      "epoch:  9   step:  194   train loss:  0.025074036791920662  val loss:  0.026879165321588516\n",
      "epoch:  9   step:  195   train loss:  0.03360287472605705  val loss:  0.026554860174655914\n",
      "epoch:  9   step:  196   train loss:  0.02910657599568367  val loss:  0.02651945687830448\n",
      "epoch:  9   step:  197   train loss:  0.0439029224216938  val loss:  0.02631097286939621\n",
      "epoch:  9   step:  198   train loss:  0.025141460821032524  val loss:  0.02662750892341137\n",
      "epoch:  9   step:  199   train loss:  0.022754384204745293  val loss:  0.026399413123726845\n",
      "epoch:  9   step:  200   train loss:  0.06445527076721191  val loss:  0.027413953095674515\n",
      "epoch:  9   step:  201   train loss:  0.052753061056137085  val loss:  0.026756523177027702\n",
      "epoch:  9   step:  202   train loss:  0.03849554806947708  val loss:  0.026747986674308777\n",
      "epoch:  9   step:  203   train loss:  0.04412815347313881  val loss:  0.026920653879642487\n",
      "epoch:  9   step:  204   train loss:  0.03246978670358658  val loss:  0.02743329107761383\n",
      "epoch:  9   step:  205   train loss:  0.03219926357269287  val loss:  0.027948400005698204\n",
      "epoch:  9   step:  206   train loss:  0.042915403842926025  val loss:  0.02818932756781578\n",
      "epoch:  9   step:  207   train loss:  0.03522991016507149  val loss:  0.028692977502942085\n",
      "epoch:  9   step:  208   train loss:  0.03192779794335365  val loss:  0.027831638231873512\n",
      "epoch:  9   step:  209   train loss:  0.03845226764678955  val loss:  0.02823677845299244\n",
      "epoch:  9   step:  210   train loss:  0.030997291207313538  val loss:  0.028748823329806328\n",
      "epoch:  9   step:  211   train loss:  0.048906758427619934  val loss:  0.028596017509698868\n",
      "epoch:  9   step:  212   train loss:  0.045949988067150116  val loss:  0.028424879536032677\n",
      "epoch:  9   step:  213   train loss:  0.03088126890361309  val loss:  0.028015002608299255\n",
      "epoch:  9   step:  214   train loss:  0.04402940347790718  val loss:  0.028666697442531586\n",
      "epoch:  9   step:  215   train loss:  0.0236796997487545  val loss:  0.028413180261850357\n",
      "epoch:  9   step:  216   train loss:  0.025930481031537056  val loss:  0.0284190084785223\n",
      "epoch:  9   step:  217   train loss:  0.04241570830345154  val loss:  0.028454992920160294\n",
      "epoch:  9   step:  218   train loss:  0.032350506633520126  val loss:  0.027949713170528412\n",
      "epoch:  9   step:  219   train loss:  0.024186547845602036  val loss:  0.02781425043940544\n",
      "epoch:  9   step:  220   train loss:  0.03697659447789192  val loss:  0.028076671063899994\n",
      "epoch:  9   step:  221   train loss:  0.03739885613322258  val loss:  0.02839815616607666\n",
      "epoch:  9   step:  222   train loss:  0.025332896038889885  val loss:  0.028124408796429634\n",
      "epoch:  9   step:  223   train loss:  0.04176643490791321  val loss:  0.028280802071094513\n",
      "epoch:  9   step:  224   train loss:  0.04751134291291237  val loss:  0.028608636930584908\n",
      "epoch:  9   step:  225   train loss:  0.0571097657084465  val loss:  0.029652435332536697\n",
      "epoch:  9   step:  226   train loss:  0.03369052708148956  val loss:  0.0292892474681139\n",
      "epoch:  9   step:  227   train loss:  0.034713827073574066  val loss:  0.029691116884350777\n",
      "epoch:  9   step:  228   train loss:  0.04116133973002434  val loss:  0.029196249321103096\n",
      "epoch:  9   step:  229   train loss:  0.03776171803474426  val loss:  0.029398130252957344\n",
      "epoch:  9   step:  230   train loss:  0.04218136891722679  val loss:  0.030232921242713928\n",
      "epoch:  9   step:  231   train loss:  0.036763258278369904  val loss:  0.029959633946418762\n",
      "epoch:  9   step:  232   train loss:  0.02992762066423893  val loss:  0.02892436273396015\n",
      "epoch:  9   step:  233   train loss:  0.05966566503047943  val loss:  0.02830606885254383\n",
      "epoch:  9   step:  234   train loss:  0.04333188012242317  val loss:  0.028025245293974876\n",
      "epoch:  9   step:  235   train loss:  0.03647632896900177  val loss:  0.02753218077123165\n",
      "epoch:  9   step:  236   train loss:  0.03576301038265228  val loss:  0.027769334614276886\n",
      "epoch:  9   step:  237   train loss:  0.04153992608189583  val loss:  0.027741659432649612\n",
      "epoch:  9   step:  238   train loss:  0.04775754734873772  val loss:  0.027797484770417213\n",
      "epoch:  9   step:  239   train loss:  0.017946019768714905  val loss:  0.02775455266237259\n",
      "epoch:  9   step:  240   train loss:  0.03783747926354408  val loss:  0.028507133945822716\n",
      "epoch:  9   step:  241   train loss:  0.03182835504412651  val loss:  0.02808808535337448\n",
      "epoch:  9   step:  242   train loss:  0.0456320121884346  val loss:  0.0283187385648489\n",
      "epoch:  9   step:  243   train loss:  0.0320112518966198  val loss:  0.028245985507965088\n",
      "epoch:  9   step:  244   train loss:  0.03463979437947273  val loss:  0.02788320556282997\n",
      "epoch:  9   step:  245   train loss:  0.043672457337379456  val loss:  0.027586352080106735\n",
      "epoch:  9   step:  246   train loss:  0.030379876494407654  val loss:  0.027011148631572723\n",
      "epoch:  9   step:  247   train loss:  0.04199439659714699  val loss:  0.026702243834733963\n",
      "epoch:  9   step:  248   train loss:  0.03131338208913803  val loss:  0.026956060901284218\n",
      "epoch:  9   step:  249   train loss:  0.03616364300251007  val loss:  0.02619975619018078\n",
      "epoch:  9   step:  250   train loss:  0.02951117977499962  val loss:  0.02634790539741516\n",
      "epoch:  9   step:  251   train loss:  0.02988237887620926  val loss:  0.02609003335237503\n",
      "epoch:  9   step:  252   train loss:  0.06294605880975723  val loss:  0.02579946257174015\n",
      "epoch:  9   step:  253   train loss:  0.02727801539003849  val loss:  0.02563411369919777\n",
      "epoch:  9   step:  254   train loss:  0.03429846093058586  val loss:  0.025061318650841713\n",
      "min_val_loss_print 0.025061318650841713\n",
      "epoch:  9   step:  255   train loss:  0.02996068261563778  val loss:  0.025182178243994713\n",
      "epoch:  9   step:  256   train loss:  0.043315209448337555  val loss:  0.025503410026431084\n",
      "epoch:  9   step:  257   train loss:  0.0592169389128685  val loss:  0.025681864470243454\n",
      "epoch:  9   step:  258   train loss:  0.03398200497031212  val loss:  0.02556430734694004\n",
      "epoch:  9   step:  259   train loss:  0.04672469198703766  val loss:  0.02591128461062908\n",
      "epoch:  9   step:  260   train loss:  0.04240834712982178  val loss:  0.026014642789959908\n",
      "epoch:  9   step:  261   train loss:  0.02732352539896965  val loss:  0.026111740618944168\n",
      "epoch:  9   step:  262   train loss:  0.038738202303647995  val loss:  0.0260857455432415\n",
      "epoch:  9   step:  263   train loss:  0.040209442377090454  val loss:  0.026539480313658714\n",
      "epoch:  9   step:  264   train loss:  0.03566202148795128  val loss:  0.02561730146408081\n",
      "epoch:  9   step:  265   train loss:  0.031906601041555405  val loss:  0.025495905429124832\n",
      "epoch:  9   step:  266   train loss:  0.030840100720524788  val loss:  0.025363706052303314\n",
      "epoch:  9   step:  267   train loss:  0.034309230744838715  val loss:  0.025497255846858025\n",
      "epoch:  9   step:  268   train loss:  0.03786524385213852  val loss:  0.025666618719697\n",
      "epoch:  9   step:  269   train loss:  0.03836888074874878  val loss:  0.02549976296722889\n",
      "epoch:  9   step:  270   train loss:  0.030557731166481972  val loss:  0.025689169764518738\n",
      "epoch:  9   step:  271   train loss:  0.05245250090956688  val loss:  0.02620747685432434\n",
      "epoch:  9   step:  272   train loss:  0.031367797404527664  val loss:  0.026909010484814644\n",
      "epoch:  9   step:  273   train loss:  0.035902395844459534  val loss:  0.02682431787252426\n",
      "epoch:  9   step:  274   train loss:  0.030713947489857674  val loss:  0.026867631822824478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  9   step:  275   train loss:  0.05566263943910599  val loss:  0.026064284145832062\n",
      "epoch:  9   step:  276   train loss:  0.04611630365252495  val loss:  0.02597927115857601\n",
      "epoch:  9   step:  277   train loss:  0.03184005618095398  val loss:  0.026226375252008438\n",
      "epoch:  9   step:  278   train loss:  0.0818895474076271  val loss:  0.02579089254140854\n",
      "epoch:  9   step:  279   train loss:  0.025512363761663437  val loss:  0.026339495554566383\n",
      "epoch:  9   step:  280   train loss:  0.024088652804493904  val loss:  0.02637186087667942\n",
      "epoch:  9   step:  281   train loss:  0.04076111689209938  val loss:  0.026988284662365913\n",
      "epoch:  9   step:  282   train loss:  0.0429520346224308  val loss:  0.026550106704235077\n",
      "epoch:  9   step:  283   train loss:  0.0330626554787159  val loss:  0.026614191010594368\n",
      "epoch:  9   step:  284   train loss:  0.051544807851314545  val loss:  0.02603119984269142\n",
      "epoch:  9   step:  285   train loss:  0.030926654115319252  val loss:  0.026875760406255722\n",
      "epoch:  9   step:  286   train loss:  0.03135313466191292  val loss:  0.027792373672127724\n",
      "epoch:  9   step:  287   train loss:  0.04558204114437103  val loss:  0.02686733938753605\n",
      "epoch:  9   step:  288   train loss:  0.04845833033323288  val loss:  0.02678753063082695\n",
      "epoch:  9   step:  289   train loss:  0.02606506086885929  val loss:  0.02738543041050434\n",
      "epoch:  9   step:  290   train loss:  0.04171028360724449  val loss:  0.026596268638968468\n",
      "epoch:  9   step:  291   train loss:  0.03731072321534157  val loss:  0.025701060891151428\n",
      "epoch:  9   step:  292   train loss:  0.03224532678723335  val loss:  0.025391696020960808\n",
      "epoch:  9   step:  293   train loss:  0.04104013368487358  val loss:  0.02560391277074814\n",
      "epoch:  9   step:  294   train loss:  0.055320851504802704  val loss:  0.025355804711580276\n",
      "epoch:  9   step:  295   train loss:  0.052959781140089035  val loss:  0.02508610300719738\n",
      "epoch:  9   step:  296   train loss:  0.05173667147755623  val loss:  0.024733243510127068\n",
      "min_val_loss_print 0.024733243510127068\n",
      "epoch:  9   step:  297   train loss:  0.02009209245443344  val loss:  0.02478712797164917\n",
      "epoch:  9   step:  298   train loss:  0.0316183902323246  val loss:  0.02490120194852352\n",
      "epoch:  9   step:  299   train loss:  0.04517635703086853  val loss:  0.024851512163877487\n",
      "epoch:  9   step:  300   train loss:  0.06241750717163086  val loss:  0.024669045582413673\n",
      "min_val_loss_print 0.024669045582413673\n",
      "epoch:  9   step:  301   train loss:  0.020657354965806007  val loss:  0.024315519258379936\n",
      "min_val_loss_print 0.024315519258379936\n",
      "epoch:  9   step:  302   train loss:  0.0879327654838562  val loss:  0.024183306843042374\n",
      "min_val_loss_print 0.024183306843042374\n",
      "epoch:  9   step:  303   train loss:  0.022552799433469772  val loss:  0.023944243788719177\n",
      "min_val_loss_print 0.023944243788719177\n",
      "epoch:  9   step:  304   train loss:  0.038293417543172836  val loss:  0.023879483342170715\n",
      "min_val_loss_print 0.023879483342170715\n",
      "epoch:  9   step:  305   train loss:  0.03536013141274452  val loss:  0.023807469755411148\n",
      "min_val_loss_print 0.023807469755411148\n",
      "epoch:  9   step:  306   train loss:  0.02451244741678238  val loss:  0.023915112018585205\n",
      "epoch:  9   step:  307   train loss:  0.03337830677628517  val loss:  0.024199318140745163\n",
      "epoch:  9   step:  308   train loss:  0.03449708968400955  val loss:  0.024122808128595352\n",
      "epoch:  9   step:  309   train loss:  0.028721796348690987  val loss:  0.023505503311753273\n",
      "min_val_loss_print 0.023505503311753273\n",
      "epoch:  9   step:  310   train loss:  0.035562921315431595  val loss:  0.023544149473309517\n",
      "epoch:  9   step:  311   train loss:  0.02795741893351078  val loss:  0.02351919561624527\n",
      "epoch:  9   step:  312   train loss:  0.029348965734243393  val loss:  0.02317514270544052\n",
      "min_val_loss_print 0.02317514270544052\n",
      "epoch:  9   step:  313   train loss:  0.05444905161857605  val loss:  0.023281516507267952\n",
      "epoch:  9   step:  314   train loss:  0.026791831478476524  val loss:  0.023305002599954605\n",
      "epoch:  9   step:  315   train loss:  0.0284466240555048  val loss:  0.023757411167025566\n",
      "epoch:  9   step:  316   train loss:  0.03128855302929878  val loss:  0.023770593106746674\n",
      "epoch:  9   step:  317   train loss:  0.04732852056622505  val loss:  0.024028230458498\n",
      "epoch:  9   step:  318   train loss:  0.02622227557003498  val loss:  0.02390841580927372\n",
      "epoch:  9   step:  319   train loss:  0.03861606866121292  val loss:  0.02367524802684784\n",
      "epoch:  9   step:  320   train loss:  0.0325322300195694  val loss:  0.024022839963436127\n",
      "epoch:  9   step:  321   train loss:  0.04994305968284607  val loss:  0.023841693997383118\n",
      "epoch:  9   step:  322   train loss:  0.028150642290711403  val loss:  0.02426210418343544\n",
      "epoch:  9   step:  323   train loss:  0.031376853585243225  val loss:  0.02404421754181385\n",
      "epoch:  9   step:  324   train loss:  0.02842368558049202  val loss:  0.023710524663329124\n",
      "epoch:  9   step:  325   train loss:  0.02275928296148777  val loss:  0.02343044988811016\n",
      "epoch:  9   step:  326   train loss:  0.03983369097113609  val loss:  0.023268885910511017\n",
      "epoch:  9   step:  327   train loss:  0.04771367460489273  val loss:  0.023189539089798927\n",
      "epoch:  9   step:  328   train loss:  0.04919122904539108  val loss:  0.022950749844312668\n",
      "min_val_loss_print 0.022950749844312668\n",
      "epoch:  9   step:  329   train loss:  0.04356422275304794  val loss:  0.022916141897439957\n",
      "min_val_loss_print 0.022916141897439957\n",
      "epoch:  9   step:  330   train loss:  0.027417784556746483  val loss:  0.023153696209192276\n",
      "epoch:  9   step:  331   train loss:  0.03811676427721977  val loss:  0.0235437024384737\n",
      "epoch:  9   step:  332   train loss:  0.04262127727270126  val loss:  0.023422788828611374\n",
      "epoch:  9   step:  333   train loss:  0.019218655303120613  val loss:  0.023466806858778\n",
      "epoch:  9   step:  334   train loss:  0.023846201598644257  val loss:  0.023412080481648445\n",
      "epoch:  9   step:  335   train loss:  0.03447296842932701  val loss:  0.023531131446361542\n",
      "epoch:  9   step:  336   train loss:  0.04795442894101143  val loss:  0.02408863604068756\n",
      "epoch:  9   step:  337   train loss:  0.027982816100120544  val loss:  0.0238061361014843\n",
      "epoch:  9   step:  338   train loss:  0.028944365680217743  val loss:  0.024055637419223785\n",
      "epoch:  9   step:  339   train loss:  0.027353299781680107  val loss:  0.024028318002820015\n",
      "epoch:  9   step:  340   train loss:  0.03895268961787224  val loss:  0.024076776579022408\n",
      "epoch:  9   step:  341   train loss:  0.026192557066679  val loss:  0.024452758952975273\n",
      "epoch:  9   step:  342   train loss:  0.0452282577753067  val loss:  0.024572884663939476\n",
      "epoch:  9   step:  343   train loss:  0.036491699516773224  val loss:  0.024683216586709023\n",
      "epoch:  10   step:  0   train loss:  0.028371095657348633  val loss:  0.024845970794558525\n",
      "epoch:  10   step:  1   train loss:  0.05029842630028725  val loss:  0.02496251091361046\n",
      "epoch:  10   step:  2   train loss:  0.029593823477625847  val loss:  0.0247410386800766\n",
      "epoch:  10   step:  3   train loss:  0.026906555518507957  val loss:  0.025318382307887077\n",
      "epoch:  10   step:  4   train loss:  0.03395871818065643  val loss:  0.025328250601887703\n",
      "epoch:  10   step:  5   train loss:  0.028285285457968712  val loss:  0.02504759281873703\n",
      "epoch:  10   step:  6   train loss:  0.036027826368808746  val loss:  0.02522599697113037\n",
      "epoch:  10   step:  7   train loss:  0.0661800429224968  val loss:  0.024844903498888016\n",
      "epoch:  10   step:  8   train loss:  0.025788752362132072  val loss:  0.024593906477093697\n",
      "epoch:  10   step:  9   train loss:  0.050303347408771515  val loss:  0.024300916120409966\n",
      "epoch:  10   step:  10   train loss:  0.02116423100233078  val loss:  0.023858239874243736\n",
      "epoch:  10   step:  11   train loss:  0.03717608004808426  val loss:  0.02432934194803238\n",
      "epoch:  10   step:  12   train loss:  0.02398599684238434  val loss:  0.024026136845350266\n",
      "epoch:  10   step:  13   train loss:  0.031328678131103516  val loss:  0.023570135235786438\n",
      "epoch:  10   step:  14   train loss:  0.04777837544679642  val loss:  0.023338183760643005\n",
      "epoch:  10   step:  15   train loss:  0.03431219607591629  val loss:  0.02327822707593441\n",
      "epoch:  10   step:  16   train loss:  0.021480698138475418  val loss:  0.023084731772542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  10   step:  17   train loss:  0.028060704469680786  val loss:  0.023062357679009438\n",
      "epoch:  10   step:  18   train loss:  0.031692877411842346  val loss:  0.023020485416054726\n",
      "epoch:  10   step:  19   train loss:  0.03905355557799339  val loss:  0.023071633651852608\n",
      "epoch:  10   step:  20   train loss:  0.023641686886548996  val loss:  0.023230720311403275\n",
      "epoch:  10   step:  21   train loss:  0.03539809212088585  val loss:  0.0230450127273798\n",
      "epoch:  10   step:  22   train loss:  0.043920502066612244  val loss:  0.02283017337322235\n",
      "min_val_loss_print 0.02283017337322235\n",
      "epoch:  10   step:  23   train loss:  0.04038505628705025  val loss:  0.022906873375177383\n",
      "epoch:  10   step:  24   train loss:  0.02592543698847294  val loss:  0.022573351860046387\n",
      "min_val_loss_print 0.022573351860046387\n",
      "epoch:  10   step:  25   train loss:  0.0439407043159008  val loss:  0.02217765711247921\n",
      "min_val_loss_print 0.02217765711247921\n",
      "epoch:  10   step:  26   train loss:  0.03742659091949463  val loss:  0.022332603111863136\n",
      "epoch:  10   step:  27   train loss:  0.01856296882033348  val loss:  0.022617189213633537\n",
      "epoch:  10   step:  28   train loss:  0.03949044272303581  val loss:  0.02244502305984497\n",
      "epoch:  10   step:  29   train loss:  0.04058511555194855  val loss:  0.02270592749118805\n",
      "epoch:  10   step:  30   train loss:  0.045567117631435394  val loss:  0.022714896127581596\n",
      "epoch:  10   step:  31   train loss:  0.037328239530324936  val loss:  0.02299419417977333\n",
      "epoch:  10   step:  32   train loss:  0.0326438806951046  val loss:  0.02322399616241455\n",
      "epoch:  10   step:  33   train loss:  0.021639378741383553  val loss:  0.023069187998771667\n",
      "epoch:  10   step:  34   train loss:  0.025725098326802254  val loss:  0.023068659007549286\n",
      "epoch:  10   step:  35   train loss:  0.055406104773283005  val loss:  0.023090463131666183\n",
      "epoch:  10   step:  36   train loss:  0.03307850658893585  val loss:  0.02293194830417633\n",
      "epoch:  10   step:  37   train loss:  0.03913690894842148  val loss:  0.022320393472909927\n",
      "epoch:  10   step:  38   train loss:  0.020576873794198036  val loss:  0.022088101133704185\n",
      "min_val_loss_print 0.022088101133704185\n",
      "epoch:  10   step:  39   train loss:  0.044404760003089905  val loss:  0.02212131954729557\n",
      "epoch:  10   step:  40   train loss:  0.033704113215208054  val loss:  0.022435670718550682\n",
      "epoch:  10   step:  41   train loss:  0.02242944948375225  val loss:  0.022463856264948845\n",
      "epoch:  10   step:  42   train loss:  0.03332176432013512  val loss:  0.02235729619860649\n",
      "epoch:  10   step:  43   train loss:  0.023470263928174973  val loss:  0.02225598692893982\n",
      "epoch:  10   step:  44   train loss:  0.04214368760585785  val loss:  0.022690419107675552\n",
      "epoch:  10   step:  45   train loss:  0.040836431086063385  val loss:  0.02312605082988739\n",
      "epoch:  10   step:  46   train loss:  0.026371197775006294  val loss:  0.02320975810289383\n",
      "epoch:  10   step:  47   train loss:  0.03402908518910408  val loss:  0.023030828684568405\n",
      "epoch:  10   step:  48   train loss:  0.044233258813619614  val loss:  0.023138975724577904\n",
      "epoch:  10   step:  49   train loss:  0.04622393846511841  val loss:  0.022908970713615417\n",
      "epoch:  10   step:  50   train loss:  0.048782046884298325  val loss:  0.02286933735013008\n",
      "epoch:  10   step:  51   train loss:  0.03095540590584278  val loss:  0.023055601865053177\n",
      "epoch:  10   step:  52   train loss:  0.04235069826245308  val loss:  0.02294938638806343\n",
      "epoch:  10   step:  53   train loss:  0.03170641139149666  val loss:  0.0232619047164917\n",
      "epoch:  10   step:  54   train loss:  0.05153794586658478  val loss:  0.02379181608557701\n",
      "epoch:  10   step:  55   train loss:  0.04217708855867386  val loss:  0.023725584149360657\n",
      "epoch:  10   step:  56   train loss:  0.03637510538101196  val loss:  0.024165457114577293\n",
      "epoch:  10   step:  57   train loss:  0.02900051698088646  val loss:  0.02400190196931362\n",
      "epoch:  10   step:  58   train loss:  0.048252422362565994  val loss:  0.0238177627325058\n",
      "epoch:  10   step:  59   train loss:  0.036652158945798874  val loss:  0.02402818575501442\n",
      "epoch:  10   step:  60   train loss:  0.03354652598500252  val loss:  0.023537227883934975\n",
      "epoch:  10   step:  61   train loss:  0.040428586304187775  val loss:  0.024645933881402016\n",
      "epoch:  10   step:  62   train loss:  0.02959175407886505  val loss:  0.024307070299983025\n",
      "epoch:  10   step:  63   train loss:  0.054927676916122437  val loss:  0.024018149822950363\n",
      "epoch:  10   step:  64   train loss:  0.035875558853149414  val loss:  0.023369580507278442\n",
      "epoch:  10   step:  65   train loss:  0.021399499848484993  val loss:  0.023510027676820755\n",
      "epoch:  10   step:  66   train loss:  0.023546362295746803  val loss:  0.023712512105703354\n",
      "epoch:  10   step:  67   train loss:  0.04607326164841652  val loss:  0.023068875074386597\n",
      "epoch:  10   step:  68   train loss:  0.031168874353170395  val loss:  0.02300375886261463\n",
      "epoch:  10   step:  69   train loss:  0.048936668783426285  val loss:  0.0229532141238451\n",
      "epoch:  10   step:  70   train loss:  0.021649543195962906  val loss:  0.02344387210905552\n",
      "epoch:  10   step:  71   train loss:  0.02857847698032856  val loss:  0.023928243666887283\n",
      "epoch:  10   step:  72   train loss:  0.02828865684568882  val loss:  0.02397962287068367\n",
      "epoch:  10   step:  73   train loss:  0.02174932323396206  val loss:  0.023375416174530983\n",
      "epoch:  10   step:  74   train loss:  0.02579888142645359  val loss:  0.023893028497695923\n",
      "epoch:  10   step:  75   train loss:  0.02279622107744217  val loss:  0.02359634079039097\n",
      "epoch:  10   step:  76   train loss:  0.03974639251828194  val loss:  0.023592637851834297\n",
      "epoch:  10   step:  77   train loss:  0.026624877005815506  val loss:  0.02345040626823902\n",
      "epoch:  10   step:  78   train loss:  0.029110057279467583  val loss:  0.023134421557188034\n",
      "epoch:  10   step:  79   train loss:  0.03304409235715866  val loss:  0.02293199487030506\n",
      "epoch:  10   step:  80   train loss:  0.03970522806048393  val loss:  0.02301817759871483\n",
      "epoch:  10   step:  81   train loss:  0.0393173024058342  val loss:  0.02330298162996769\n",
      "epoch:  10   step:  82   train loss:  0.032425470650196075  val loss:  0.0240996852517128\n",
      "epoch:  10   step:  83   train loss:  0.07755301147699356  val loss:  0.024941643700003624\n",
      "epoch:  10   step:  84   train loss:  0.033984918147325516  val loss:  0.02507452480494976\n",
      "epoch:  10   step:  85   train loss:  0.02826562710106373  val loss:  0.024653121829032898\n",
      "epoch:  10   step:  86   train loss:  0.02020454593002796  val loss:  0.02427518740296364\n",
      "epoch:  10   step:  87   train loss:  0.03896138444542885  val loss:  0.023524263873696327\n",
      "epoch:  10   step:  88   train loss:  0.031740203499794006  val loss:  0.023294098675251007\n",
      "epoch:  10   step:  89   train loss:  0.02667585201561451  val loss:  0.023199666291475296\n",
      "epoch:  10   step:  90   train loss:  0.03559819981455803  val loss:  0.02300954796373844\n",
      "epoch:  10   step:  91   train loss:  0.02090531960129738  val loss:  0.02291598916053772\n",
      "epoch:  10   step:  92   train loss:  0.024048468098044395  val loss:  0.022543083876371384\n",
      "epoch:  10   step:  93   train loss:  0.03348328173160553  val loss:  0.02261543646454811\n",
      "epoch:  10   step:  94   train loss:  0.0434330478310585  val loss:  0.02290395461022854\n",
      "epoch:  10   step:  95   train loss:  0.042798031121492386  val loss:  0.022473055869340897\n",
      "epoch:  10   step:  96   train loss:  0.03371281176805496  val loss:  0.022611567750573158\n",
      "epoch:  10   step:  97   train loss:  0.02904476225376129  val loss:  0.0226742010563612\n",
      "epoch:  10   step:  98   train loss:  0.024765990674495697  val loss:  0.022509844973683357\n",
      "epoch:  10   step:  99   train loss:  0.061906080693006516  val loss:  0.022699372842907906\n",
      "epoch:  10   step:  100   train loss:  0.024164889007806778  val loss:  0.022496316581964493\n",
      "epoch:  10   step:  101   train loss:  0.0372786745429039  val loss:  0.02265217714011669\n",
      "epoch:  10   step:  102   train loss:  0.03161701187491417  val loss:  0.022564511746168137\n",
      "epoch:  10   step:  103   train loss:  0.029200412333011627  val loss:  0.02244006097316742\n",
      "epoch:  10   step:  104   train loss:  0.034776218235492706  val loss:  0.022577328607439995\n",
      "epoch:  10   step:  105   train loss:  0.024983514100313187  val loss:  0.023222986608743668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  10   step:  106   train loss:  0.02646762691438198  val loss:  0.022913966327905655\n",
      "epoch:  10   step:  107   train loss:  0.0306903924793005  val loss:  0.023015214130282402\n",
      "epoch:  10   step:  108   train loss:  0.03497777134180069  val loss:  0.022968314588069916\n",
      "epoch:  10   step:  109   train loss:  0.032583851367235184  val loss:  0.02319459803402424\n",
      "epoch:  10   step:  110   train loss:  0.042595066130161285  val loss:  0.023050514981150627\n",
      "epoch:  10   step:  111   train loss:  0.028441008180379868  val loss:  0.02367568388581276\n",
      "epoch:  10   step:  112   train loss:  0.026520373299717903  val loss:  0.023761536926031113\n",
      "epoch:  10   step:  113   train loss:  0.03717393800616264  val loss:  0.023617921397089958\n",
      "epoch:  10   step:  114   train loss:  0.02859548106789589  val loss:  0.023855017498135567\n",
      "epoch:  10   step:  115   train loss:  0.04672041907906532  val loss:  0.024680450558662415\n",
      "epoch:  10   step:  116   train loss:  0.024190295487642288  val loss:  0.02521425485610962\n",
      "epoch:  10   step:  117   train loss:  0.02608257345855236  val loss:  0.02513236179947853\n",
      "epoch:  10   step:  118   train loss:  0.026072440668940544  val loss:  0.025072680786252022\n",
      "epoch:  10   step:  119   train loss:  0.06339168548583984  val loss:  0.025062166154384613\n",
      "epoch:  10   step:  120   train loss:  0.025608880445361137  val loss:  0.024960869923233986\n",
      "epoch:  10   step:  121   train loss:  0.031600221991539  val loss:  0.024626003578305244\n",
      "epoch:  10   step:  122   train loss:  0.04392138123512268  val loss:  0.024800285696983337\n",
      "epoch:  10   step:  123   train loss:  0.020555540919303894  val loss:  0.024537313729524612\n",
      "epoch:  10   step:  124   train loss:  0.04009455814957619  val loss:  0.02516990900039673\n",
      "epoch:  10   step:  125   train loss:  0.021749787032604218  val loss:  0.02501695230603218\n",
      "epoch:  10   step:  126   train loss:  0.026522528380155563  val loss:  0.024567600339651108\n",
      "epoch:  10   step:  127   train loss:  0.018452974036335945  val loss:  0.024515079334378242\n",
      "epoch:  10   step:  128   train loss:  0.019491048529744148  val loss:  0.024464206770062447\n",
      "epoch:  10   step:  129   train loss:  0.02185298502445221  val loss:  0.02436099573969841\n",
      "epoch:  10   step:  130   train loss:  0.022807886824011803  val loss:  0.02426587976515293\n",
      "epoch:  10   step:  131   train loss:  0.054417625069618225  val loss:  0.024000344797968864\n",
      "epoch:  10   step:  132   train loss:  0.04296121001243591  val loss:  0.02411261573433876\n",
      "epoch:  10   step:  133   train loss:  0.0256760586053133  val loss:  0.023916641250252724\n",
      "epoch:  10   step:  134   train loss:  0.03323788195848465  val loss:  0.02427963726222515\n",
      "epoch:  10   step:  135   train loss:  0.04114104434847832  val loss:  0.02447444200515747\n",
      "epoch:  10   step:  136   train loss:  0.028763635084033012  val loss:  0.024501550942659378\n",
      "epoch:  10   step:  137   train loss:  0.022079069167375565  val loss:  0.024220788851380348\n",
      "epoch:  10   step:  138   train loss:  0.02822829596698284  val loss:  0.024561217054724693\n",
      "epoch:  10   step:  139   train loss:  0.030898505821824074  val loss:  0.024503488093614578\n",
      "epoch:  10   step:  140   train loss:  0.026539046317338943  val loss:  0.024077046662569046\n",
      "epoch:  10   step:  141   train loss:  0.03725467994809151  val loss:  0.023961611092090607\n",
      "epoch:  10   step:  142   train loss:  0.02358725666999817  val loss:  0.02392665110528469\n",
      "epoch:  10   step:  143   train loss:  0.03056482784450054  val loss:  0.023499753326177597\n",
      "epoch:  10   step:  144   train loss:  0.06520167738199234  val loss:  0.023387586697936058\n",
      "epoch:  10   step:  145   train loss:  0.017621101811528206  val loss:  0.023615658283233643\n",
      "epoch:  10   step:  146   train loss:  0.01691306196153164  val loss:  0.02315908670425415\n",
      "epoch:  10   step:  147   train loss:  0.04244016110897064  val loss:  0.023622389882802963\n",
      "epoch:  10   step:  148   train loss:  0.030355306342244148  val loss:  0.02365666814148426\n",
      "epoch:  10   step:  149   train loss:  0.034560348838567734  val loss:  0.02394646219909191\n",
      "epoch:  10   step:  150   train loss:  0.03573702648282051  val loss:  0.02356628142297268\n",
      "epoch:  10   step:  151   train loss:  0.05947235971689224  val loss:  0.02317892014980316\n",
      "epoch:  10   step:  152   train loss:  0.025707514956593513  val loss:  0.023754432797431946\n",
      "epoch:  10   step:  153   train loss:  0.04493711143732071  val loss:  0.02409261465072632\n",
      "epoch:  10   step:  154   train loss:  0.026865623891353607  val loss:  0.024714019149541855\n",
      "epoch:  10   step:  155   train loss:  0.031027674674987793  val loss:  0.024172816425561905\n",
      "epoch:  10   step:  156   train loss:  0.037801679223775864  val loss:  0.023347925394773483\n",
      "epoch:  10   step:  157   train loss:  0.04070398584008217  val loss:  0.023147929459810257\n",
      "epoch:  10   step:  158   train loss:  0.039574213325977325  val loss:  0.022709226235747337\n",
      "epoch:  10   step:  159   train loss:  0.033971671015024185  val loss:  0.022927697747945786\n",
      "epoch:  10   step:  160   train loss:  0.03388427570462227  val loss:  0.022531144320964813\n",
      "epoch:  10   step:  161   train loss:  0.0308625977486372  val loss:  0.022920895367860794\n",
      "epoch:  10   step:  162   train loss:  0.024624910205602646  val loss:  0.02290242724120617\n",
      "epoch:  10   step:  163   train loss:  0.025540145114064217  val loss:  0.023017657920718193\n",
      "epoch:  10   step:  164   train loss:  0.03118271939456463  val loss:  0.023727530613541603\n",
      "epoch:  10   step:  165   train loss:  0.026678547263145447  val loss:  0.023565175011754036\n",
      "epoch:  10   step:  166   train loss:  0.03664848953485489  val loss:  0.02424716204404831\n",
      "epoch:  10   step:  167   train loss:  0.03357141837477684  val loss:  0.023916160687804222\n",
      "epoch:  10   step:  168   train loss:  0.034403447061777115  val loss:  0.024438822641968727\n",
      "epoch:  10   step:  169   train loss:  0.036085449159145355  val loss:  0.024859344586730003\n",
      "epoch:  10   step:  170   train loss:  0.028542742133140564  val loss:  0.024496424943208694\n",
      "epoch:  10   step:  171   train loss:  0.04341299086809158  val loss:  0.02386939525604248\n",
      "epoch:  10   step:  172   train loss:  0.03740769252181053  val loss:  0.02392696402966976\n",
      "epoch:  10   step:  173   train loss:  0.03228766471147537  val loss:  0.024134662002325058\n",
      "epoch:  10   step:  174   train loss:  0.04573914781212807  val loss:  0.024691851809620857\n",
      "epoch:  10   step:  175   train loss:  0.05581146478652954  val loss:  0.025256270542740822\n",
      "epoch:  10   step:  176   train loss:  0.028096918016672134  val loss:  0.025882137939333916\n",
      "epoch:  10   step:  177   train loss:  0.02977772243320942  val loss:  0.02456819638609886\n",
      "epoch:  10   step:  178   train loss:  0.03300094231963158  val loss:  0.024402616545557976\n",
      "epoch:  10   step:  179   train loss:  0.030971258878707886  val loss:  0.024559421464800835\n",
      "epoch:  10   step:  180   train loss:  0.04887628182768822  val loss:  0.02415373921394348\n",
      "epoch:  10   step:  181   train loss:  0.030044754967093468  val loss:  0.02418963611125946\n",
      "epoch:  10   step:  182   train loss:  0.01621275208890438  val loss:  0.024387270212173462\n",
      "epoch:  10   step:  183   train loss:  0.03392254561185837  val loss:  0.024950338527560234\n",
      "epoch:  10   step:  184   train loss:  0.024010324850678444  val loss:  0.02480856142938137\n",
      "epoch:  10   step:  185   train loss:  0.02406902238726616  val loss:  0.025071097537875175\n",
      "epoch:  10   step:  186   train loss:  0.025309812277555466  val loss:  0.024871239438652992\n",
      "epoch:  10   step:  187   train loss:  0.038361843675374985  val loss:  0.024005288258194923\n",
      "epoch:  10   step:  188   train loss:  0.02553430385887623  val loss:  0.024129385128617287\n",
      "epoch:  10   step:  189   train loss:  0.06664352864027023  val loss:  0.02354431338608265\n",
      "epoch:  10   step:  190   train loss:  0.037131816148757935  val loss:  0.02353348582983017\n",
      "epoch:  10   step:  191   train loss:  0.02005927264690399  val loss:  0.02311795763671398\n",
      "epoch:  10   step:  192   train loss:  0.03222090005874634  val loss:  0.023061083629727364\n",
      "epoch:  10   step:  193   train loss:  0.03556444123387337  val loss:  0.022976431995630264\n",
      "epoch:  10   step:  194   train loss:  0.041159749031066895  val loss:  0.02338859997689724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  10   step:  195   train loss:  0.03174612671136856  val loss:  0.02325458638370037\n",
      "epoch:  10   step:  196   train loss:  0.02287382259964943  val loss:  0.02331342175602913\n",
      "epoch:  10   step:  197   train loss:  0.05119228735566139  val loss:  0.023444773629307747\n",
      "epoch:  10   step:  198   train loss:  0.03075476735830307  val loss:  0.023091575130820274\n",
      "epoch:  10   step:  199   train loss:  0.02248276025056839  val loss:  0.022791989147663116\n",
      "epoch:  10   step:  200   train loss:  0.04621143639087677  val loss:  0.02313118986785412\n",
      "epoch:  10   step:  201   train loss:  0.024610096588730812  val loss:  0.023229358717799187\n",
      "epoch:  10   step:  202   train loss:  0.024892959743738174  val loss:  0.023433830589056015\n",
      "epoch:  10   step:  203   train loss:  0.046591099351644516  val loss:  0.023531561717391014\n",
      "epoch:  10   step:  204   train loss:  0.03918423131108284  val loss:  0.023676201701164246\n",
      "epoch:  10   step:  205   train loss:  0.031492944806814194  val loss:  0.024010952562093735\n",
      "epoch:  10   step:  206   train loss:  0.027057074010372162  val loss:  0.024039361625909805\n",
      "epoch:  10   step:  207   train loss:  0.06067095324397087  val loss:  0.02424221858382225\n",
      "epoch:  10   step:  208   train loss:  0.04015231877565384  val loss:  0.024250490590929985\n",
      "epoch:  10   step:  209   train loss:  0.029888926073908806  val loss:  0.024309281259775162\n",
      "epoch:  10   step:  210   train loss:  0.0422317236661911  val loss:  0.02419423870742321\n",
      "epoch:  10   step:  211   train loss:  0.03485596179962158  val loss:  0.024269718676805496\n",
      "epoch:  10   step:  212   train loss:  0.03169149532914162  val loss:  0.024883389472961426\n",
      "epoch:  10   step:  213   train loss:  0.0409073531627655  val loss:  0.025185588747262955\n",
      "epoch:  10   step:  214   train loss:  0.052184946835041046  val loss:  0.025802969932556152\n",
      "epoch:  10   step:  215   train loss:  0.03546244278550148  val loss:  0.025540143251419067\n",
      "epoch:  10   step:  216   train loss:  0.042271871119737625  val loss:  0.026078976690769196\n",
      "epoch:  10   step:  217   train loss:  0.031993716955184937  val loss:  0.025862157344818115\n",
      "epoch:  10   step:  218   train loss:  0.02940634824335575  val loss:  0.0257789995521307\n",
      "epoch:  10   step:  219   train loss:  0.02159150131046772  val loss:  0.02534572035074234\n",
      "epoch:  10   step:  220   train loss:  0.03475383296608925  val loss:  0.025193868204951286\n",
      "epoch:  10   step:  221   train loss:  0.024321705102920532  val loss:  0.02475116029381752\n",
      "epoch:  10   step:  222   train loss:  0.029232002794742584  val loss:  0.02523934468626976\n",
      "epoch:  10   step:  223   train loss:  0.03685504570603371  val loss:  0.025424249470233917\n",
      "epoch:  10   step:  224   train loss:  0.0222670529037714  val loss:  0.025208858773112297\n",
      "epoch:  10   step:  225   train loss:  0.02915993705391884  val loss:  0.025528477504849434\n",
      "epoch:  10   step:  226   train loss:  0.03221708908677101  val loss:  0.024689441546797752\n",
      "epoch:  10   step:  227   train loss:  0.02860053814947605  val loss:  0.025004984810948372\n",
      "epoch:  10   step:  228   train loss:  0.02462686225771904  val loss:  0.025240328162908554\n",
      "epoch:  10   step:  229   train loss:  0.02955840714275837  val loss:  0.024802973493933678\n",
      "epoch:  10   step:  230   train loss:  0.02531503699719906  val loss:  0.024481773376464844\n",
      "epoch:  10   step:  231   train loss:  0.026878826320171356  val loss:  0.024157442152500153\n",
      "epoch:  10   step:  232   train loss:  0.04497848451137543  val loss:  0.024371355772018433\n",
      "epoch:  10   step:  233   train loss:  0.04839996621012688  val loss:  0.024693889543414116\n",
      "epoch:  10   step:  234   train loss:  0.043576713651418686  val loss:  0.024232452735304832\n",
      "epoch:  10   step:  235   train loss:  0.048405736684799194  val loss:  0.024531351402401924\n",
      "epoch:  10   step:  236   train loss:  0.01729564741253853  val loss:  0.02402075193822384\n",
      "epoch:  10   step:  237   train loss:  0.019791392609477043  val loss:  0.024730954319238663\n",
      "epoch:  10   step:  238   train loss:  0.04446879029273987  val loss:  0.024691317230463028\n",
      "epoch:  10   step:  239   train loss:  0.034547459334135056  val loss:  0.02438005618751049\n",
      "epoch:  10   step:  240   train loss:  0.03086179867386818  val loss:  0.024844110012054443\n",
      "epoch:  10   step:  241   train loss:  0.051806170493364334  val loss:  0.0249283816665411\n",
      "epoch:  10   step:  242   train loss:  0.040311314165592194  val loss:  0.02466478757560253\n",
      "epoch:  10   step:  243   train loss:  0.03875507041811943  val loss:  0.02463299036026001\n",
      "epoch:  10   step:  244   train loss:  0.03305324167013168  val loss:  0.02426440268754959\n",
      "epoch:  10   step:  245   train loss:  0.048302069306373596  val loss:  0.02467973344027996\n",
      "epoch:  10   step:  246   train loss:  0.02680462971329689  val loss:  0.02439187467098236\n",
      "epoch:  10   step:  247   train loss:  0.04927992820739746  val loss:  0.024781515821814537\n",
      "epoch:  10   step:  248   train loss:  0.03714629262685776  val loss:  0.024700533598661423\n",
      "epoch:  10   step:  249   train loss:  0.03208845481276512  val loss:  0.024908393621444702\n",
      "epoch:  10   step:  250   train loss:  0.024207845330238342  val loss:  0.02540282905101776\n",
      "epoch:  10   step:  251   train loss:  0.019398361444473267  val loss:  0.025064442306756973\n",
      "epoch:  10   step:  252   train loss:  0.04158129170536995  val loss:  0.02542761154472828\n",
      "epoch:  10   step:  253   train loss:  0.028381066396832466  val loss:  0.025589440017938614\n",
      "epoch:  10   step:  254   train loss:  0.033615536987781525  val loss:  0.02558792382478714\n",
      "epoch:  10   step:  255   train loss:  0.013532389886677265  val loss:  0.02632788009941578\n",
      "epoch:  10   step:  256   train loss:  0.04044599458575249  val loss:  0.02590581774711609\n",
      "epoch:  10   step:  257   train loss:  0.03837408870458603  val loss:  0.025952544063329697\n",
      "epoch:  10   step:  258   train loss:  0.04849310964345932  val loss:  0.026062695309519768\n",
      "epoch:  10   step:  259   train loss:  0.021040746942162514  val loss:  0.02605414018034935\n",
      "epoch:  10   step:  260   train loss:  0.03550669178366661  val loss:  0.026058092713356018\n",
      "epoch:  10   step:  261   train loss:  0.03130620718002319  val loss:  0.02612452767789364\n",
      "epoch:  10   step:  262   train loss:  0.02050687000155449  val loss:  0.02600705996155739\n",
      "epoch:  10   step:  263   train loss:  0.0320008285343647  val loss:  0.0260002538561821\n",
      "epoch:  10   step:  264   train loss:  0.024849748238921165  val loss:  0.0260345209389925\n",
      "epoch:  10   step:  265   train loss:  0.03136823698878288  val loss:  0.026050910353660583\n",
      "epoch:  10   step:  266   train loss:  0.05375555530190468  val loss:  0.025739464908838272\n",
      "epoch:  10   step:  267   train loss:  0.04458281025290489  val loss:  0.025818675756454468\n",
      "epoch:  10   step:  268   train loss:  0.02765030972659588  val loss:  0.025909719988703728\n",
      "epoch:  10   step:  269   train loss:  0.02255578152835369  val loss:  0.02575007639825344\n",
      "epoch:  10   step:  270   train loss:  0.03707338497042656  val loss:  0.025169093161821365\n",
      "epoch:  10   step:  271   train loss:  0.029188400134444237  val loss:  0.024673329666256905\n",
      "epoch:  10   step:  272   train loss:  0.02449703775346279  val loss:  0.024599242955446243\n",
      "epoch:  10   step:  273   train loss:  0.027379076927900314  val loss:  0.02452818863093853\n",
      "epoch:  10   step:  274   train loss:  0.03395877778530121  val loss:  0.024714291095733643\n",
      "epoch:  10   step:  275   train loss:  0.03234412521123886  val loss:  0.02427915297448635\n",
      "epoch:  10   step:  276   train loss:  0.04287072643637657  val loss:  0.024078918620944023\n",
      "epoch:  10   step:  277   train loss:  0.05732373520731926  val loss:  0.024363873526453972\n",
      "epoch:  10   step:  278   train loss:  0.05597120150923729  val loss:  0.024621987715363503\n",
      "epoch:  10   step:  279   train loss:  0.023383859544992447  val loss:  0.02496417984366417\n",
      "epoch:  10   step:  280   train loss:  0.03646830469369888  val loss:  0.024960286915302277\n",
      "epoch:  10   step:  281   train loss:  0.02743620052933693  val loss:  0.02510371245443821\n",
      "epoch:  10   step:  282   train loss:  0.028237057849764824  val loss:  0.025309091433882713\n",
      "epoch:  10   step:  283   train loss:  0.025106215849518776  val loss:  0.02542261779308319\n",
      "epoch:  10   step:  284   train loss:  0.032099105417728424  val loss:  0.02549857832491398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  10   step:  285   train loss:  0.018489092588424683  val loss:  0.02547123283147812\n",
      "epoch:  10   step:  286   train loss:  0.045223310589790344  val loss:  0.025445520877838135\n",
      "epoch:  10   step:  287   train loss:  0.02473880909383297  val loss:  0.025679033249616623\n",
      "epoch:  10   step:  288   train loss:  0.0528084896504879  val loss:  0.02605101652443409\n",
      "epoch:  10   step:  289   train loss:  0.03945041447877884  val loss:  0.026266852393746376\n",
      "epoch:  10   step:  290   train loss:  0.03140954673290253  val loss:  0.02626304142177105\n",
      "epoch:  10   step:  291   train loss:  0.04028209298849106  val loss:  0.02596110850572586\n",
      "epoch:  10   step:  292   train loss:  0.027905195951461792  val loss:  0.02658240683376789\n",
      "epoch:  10   step:  293   train loss:  0.026538707315921783  val loss:  0.026567187160253525\n",
      "epoch:  10   step:  294   train loss:  0.040005624294281006  val loss:  0.026559099555015564\n",
      "epoch:  10   step:  295   train loss:  0.023749906569719315  val loss:  0.026269521564245224\n",
      "epoch:  10   step:  296   train loss:  0.022816302254796028  val loss:  0.02613760344684124\n",
      "epoch:  10   step:  297   train loss:  0.02503623440861702  val loss:  0.02642950974404812\n",
      "epoch:  10   step:  298   train loss:  0.0330926887691021  val loss:  0.026705529540777206\n",
      "epoch:  10   step:  299   train loss:  0.02312328852713108  val loss:  0.02678186632692814\n",
      "epoch:  10   step:  300   train loss:  0.02240161783993244  val loss:  0.026404058560729027\n",
      "epoch:  10   step:  301   train loss:  0.026428278535604477  val loss:  0.026587162166833878\n",
      "epoch:  10   step:  302   train loss:  0.04051097854971886  val loss:  0.026790503412485123\n",
      "epoch:  10   step:  303   train loss:  0.030044035986065865  val loss:  0.02703149989247322\n",
      "epoch:  10   step:  304   train loss:  0.030849875882267952  val loss:  0.02714303880929947\n",
      "epoch:  10   step:  305   train loss:  0.035043925046920776  val loss:  0.02740476280450821\n",
      "epoch:  10   step:  306   train loss:  0.04826578125357628  val loss:  0.027647487819194794\n",
      "epoch:  10   step:  307   train loss:  0.05877538397908211  val loss:  0.027741795405745506\n",
      "epoch:  10   step:  308   train loss:  0.03256514295935631  val loss:  0.02735065296292305\n",
      "epoch:  10   step:  309   train loss:  0.015775125473737717  val loss:  0.02761455439031124\n",
      "epoch:  10   step:  310   train loss:  0.03217456117272377  val loss:  0.027163682505488396\n",
      "epoch:  10   step:  311   train loss:  0.0275863129645586  val loss:  0.026884734630584717\n",
      "epoch:  10   step:  312   train loss:  0.026580186560750008  val loss:  0.027126943692564964\n",
      "epoch:  10   step:  313   train loss:  0.0353279672563076  val loss:  0.027119852602481842\n",
      "epoch:  10   step:  314   train loss:  0.031695783138275146  val loss:  0.026417585089802742\n",
      "epoch:  10   step:  315   train loss:  0.030871054157614708  val loss:  0.02631833404302597\n",
      "epoch:  10   step:  316   train loss:  0.04000111669301987  val loss:  0.02621810883283615\n",
      "epoch:  10   step:  317   train loss:  0.03125610947608948  val loss:  0.025917716324329376\n",
      "epoch:  10   step:  318   train loss:  0.03673891723155975  val loss:  0.025540277361869812\n",
      "epoch:  10   step:  319   train loss:  0.027685876935720444  val loss:  0.02555174194276333\n",
      "epoch:  10   step:  320   train loss:  0.02613636665046215  val loss:  0.025375785306096077\n",
      "epoch:  10   step:  321   train loss:  0.022608494386076927  val loss:  0.02459815889596939\n",
      "epoch:  10   step:  322   train loss:  0.04869600012898445  val loss:  0.024498913437128067\n",
      "epoch:  10   step:  323   train loss:  0.03579747676849365  val loss:  0.024438997730612755\n",
      "epoch:  10   step:  324   train loss:  0.03839774429798126  val loss:  0.025076450780034065\n",
      "epoch:  10   step:  325   train loss:  0.029591280966997147  val loss:  0.02540242113173008\n",
      "epoch:  10   step:  326   train loss:  0.054260965436697006  val loss:  0.02513006329536438\n",
      "epoch:  10   step:  327   train loss:  0.02711886540055275  val loss:  0.025178389623761177\n",
      "epoch:  10   step:  328   train loss:  0.019472720101475716  val loss:  0.024617142975330353\n",
      "epoch:  10   step:  329   train loss:  0.04840533062815666  val loss:  0.024931959807872772\n",
      "epoch:  10   step:  330   train loss:  0.021267935633659363  val loss:  0.024717040359973907\n",
      "epoch:  10   step:  331   train loss:  0.026325765997171402  val loss:  0.024639446288347244\n",
      "epoch:  10   step:  332   train loss:  0.02804938703775406  val loss:  0.024735240265727043\n",
      "epoch:  10   step:  333   train loss:  0.037512991577386856  val loss:  0.024615958333015442\n",
      "epoch:  10   step:  334   train loss:  0.034743934869766235  val loss:  0.02479933388531208\n",
      "epoch:  10   step:  335   train loss:  0.034989338368177414  val loss:  0.024950550869107246\n",
      "epoch:  10   step:  336   train loss:  0.04875386133790016  val loss:  0.02511466108262539\n",
      "epoch:  10   step:  337   train loss:  0.06390954554080963  val loss:  0.02537604793906212\n",
      "epoch:  10   step:  338   train loss:  0.07610838860273361  val loss:  0.026137227192521095\n",
      "epoch:  10   step:  339   train loss:  0.042187903076410294  val loss:  0.026353975757956505\n",
      "epoch:  10   step:  340   train loss:  0.025769416242837906  val loss:  0.026182325556874275\n",
      "epoch:  10   step:  341   train loss:  0.016810506582260132  val loss:  0.02631630375981331\n",
      "epoch:  10   step:  342   train loss:  0.02186615765094757  val loss:  0.026121048256754875\n",
      "epoch:  10   step:  343   train loss:  0.018257364630699158  val loss:  0.02562500536441803\n",
      "epoch:  11   step:  0   train loss:  0.044342879205942154  val loss:  0.02559051476418972\n",
      "epoch:  11   step:  1   train loss:  0.035433005541563034  val loss:  0.026273518800735474\n",
      "epoch:  11   step:  2   train loss:  0.03063822165131569  val loss:  0.02612585946917534\n",
      "epoch:  11   step:  3   train loss:  0.035469602793455124  val loss:  0.02641548588871956\n",
      "epoch:  11   step:  4   train loss:  0.026657015085220337  val loss:  0.026176344603300095\n",
      "epoch:  11   step:  5   train loss:  0.027805261313915253  val loss:  0.026236001402139664\n",
      "epoch:  11   step:  6   train loss:  0.030168883502483368  val loss:  0.02632300928235054\n",
      "epoch:  11   step:  7   train loss:  0.03294327110052109  val loss:  0.026593726128339767\n",
      "epoch:  11   step:  8   train loss:  0.024939684197306633  val loss:  0.02711092308163643\n",
      "epoch:  11   step:  9   train loss:  0.028753612190485  val loss:  0.026865381747484207\n",
      "epoch:  11   step:  10   train loss:  0.026637114584445953  val loss:  0.027473827823996544\n",
      "epoch:  11   step:  11   train loss:  0.0337885245680809  val loss:  0.027066253125667572\n",
      "epoch:  11   step:  12   train loss:  0.019344046711921692  val loss:  0.027012765407562256\n",
      "epoch:  11   step:  13   train loss:  0.020421508699655533  val loss:  0.02667797915637493\n",
      "epoch:  11   step:  14   train loss:  0.08531749993562698  val loss:  0.026303913444280624\n",
      "epoch:  11   step:  15   train loss:  0.028082335367798805  val loss:  0.02577081322669983\n",
      "epoch:  11   step:  16   train loss:  0.04315463453531265  val loss:  0.025526704266667366\n",
      "epoch:  11   step:  17   train loss:  0.026327837258577347  val loss:  0.02530531957745552\n",
      "epoch:  11   step:  18   train loss:  0.03033445216715336  val loss:  0.025186877697706223\n",
      "epoch:  11   step:  19   train loss:  0.02815670520067215  val loss:  0.025323165580630302\n",
      "epoch:  11   step:  20   train loss:  0.024683447554707527  val loss:  0.0253569595515728\n",
      "epoch:  11   step:  21   train loss:  0.03134048357605934  val loss:  0.0255754254758358\n",
      "epoch:  11   step:  22   train loss:  0.020964954048395157  val loss:  0.025597739964723587\n",
      "epoch:  11   step:  23   train loss:  0.031736113131046295  val loss:  0.025675611570477486\n",
      "epoch:  11   step:  24   train loss:  0.033364083617925644  val loss:  0.02558790147304535\n",
      "epoch:  11   step:  25   train loss:  0.018980324268341064  val loss:  0.025457045063376427\n",
      "epoch:  11   step:  26   train loss:  0.03398735448718071  val loss:  0.02533046342432499\n",
      "epoch:  11   step:  27   train loss:  0.02342839539051056  val loss:  0.024877287447452545\n",
      "epoch:  11   step:  28   train loss:  0.033114202320575714  val loss:  0.024489032104611397\n",
      "epoch:  11   step:  29   train loss:  0.053952910006046295  val loss:  0.02435704879462719\n",
      "epoch:  11   step:  30   train loss:  0.047522373497486115  val loss:  0.023949118331074715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  11   step:  31   train loss:  0.038281459361314774  val loss:  0.023951051756739616\n",
      "epoch:  11   step:  32   train loss:  0.032339975237846375  val loss:  0.0240014661103487\n",
      "epoch:  11   step:  33   train loss:  0.021745197474956512  val loss:  0.02391292154788971\n",
      "epoch:  11   step:  34   train loss:  0.06917168200016022  val loss:  0.024555517360568047\n",
      "epoch:  11   step:  35   train loss:  0.020878875628113747  val loss:  0.024281267076730728\n",
      "epoch:  11   step:  36   train loss:  0.02652488648891449  val loss:  0.02494005300104618\n",
      "epoch:  11   step:  37   train loss:  0.032688915729522705  val loss:  0.024766698479652405\n",
      "epoch:  11   step:  38   train loss:  0.03192880004644394  val loss:  0.025111878290772438\n",
      "epoch:  11   step:  39   train loss:  0.030431609600782394  val loss:  0.02517431229352951\n",
      "epoch:  11   step:  40   train loss:  0.03706961125135422  val loss:  0.025106927379965782\n",
      "epoch:  11   step:  41   train loss:  0.025887418538331985  val loss:  0.025063006207346916\n",
      "epoch:  11   step:  42   train loss:  0.020484978333115578  val loss:  0.025366516783833504\n",
      "epoch:  11   step:  43   train loss:  0.03982819244265556  val loss:  0.02537306770682335\n",
      "epoch:  11   step:  44   train loss:  0.026768065989017487  val loss:  0.024699870496988297\n",
      "epoch:  11   step:  45   train loss:  0.03500727191567421  val loss:  0.024786246940493584\n",
      "epoch:  11   step:  46   train loss:  0.022031622007489204  val loss:  0.024849457666277885\n",
      "epoch:  11   step:  47   train loss:  0.024928182363510132  val loss:  0.024726253002882004\n",
      "epoch:  11   step:  48   train loss:  0.04742021858692169  val loss:  0.02504820004105568\n",
      "epoch:  11   step:  49   train loss:  0.02614649571478367  val loss:  0.024749519303441048\n",
      "epoch:  11   step:  50   train loss:  0.032451774924993515  val loss:  0.025173554196953773\n",
      "epoch:  11   step:  51   train loss:  0.03840620070695877  val loss:  0.026011453941464424\n",
      "epoch:  11   step:  52   train loss:  0.028550922870635986  val loss:  0.02542741224169731\n",
      "epoch:  11   step:  53   train loss:  0.03963744640350342  val loss:  0.025554832071065903\n",
      "epoch:  11   step:  54   train loss:  0.034757740795612335  val loss:  0.02606314793229103\n",
      "epoch:  11   step:  55   train loss:  0.02491665445268154  val loss:  0.025986121967434883\n",
      "epoch:  11   step:  56   train loss:  0.023203397169709206  val loss:  0.025715110823512077\n",
      "epoch:  11   step:  57   train loss:  0.022668564692139626  val loss:  0.02560400404036045\n",
      "epoch:  11   step:  58   train loss:  0.02253847010433674  val loss:  0.02558041550219059\n",
      "epoch:  11   step:  59   train loss:  0.01790146343410015  val loss:  0.024608440697193146\n",
      "epoch:  11   step:  60   train loss:  0.04386680945754051  val loss:  0.024384109303355217\n",
      "epoch:  11   step:  61   train loss:  0.04647384583950043  val loss:  0.024231988936662674\n",
      "epoch:  11   step:  62   train loss:  0.03549804538488388  val loss:  0.02378278598189354\n",
      "epoch:  11   step:  63   train loss:  0.05784625560045242  val loss:  0.024165082722902298\n",
      "epoch:  11   step:  64   train loss:  0.018205495551228523  val loss:  0.024429911747574806\n",
      "epoch:  11   step:  65   train loss:  0.026878030970692635  val loss:  0.024550722911953926\n",
      "epoch:  11   step:  66   train loss:  0.018459921702742577  val loss:  0.02416401542723179\n",
      "epoch:  11   step:  67   train loss:  0.028727589175105095  val loss:  0.02443510852754116\n",
      "epoch:  11   step:  68   train loss:  0.030147098004817963  val loss:  0.0241409819573164\n",
      "epoch:  11   step:  69   train loss:  0.02423548325896263  val loss:  0.024056468158960342\n",
      "epoch:  11   step:  70   train loss:  0.022756502032279968  val loss:  0.024329369887709618\n",
      "epoch:  11   step:  71   train loss:  0.057724449783563614  val loss:  0.024636009708046913\n",
      "epoch:  11   step:  72   train loss:  0.0390365831553936  val loss:  0.024435384199023247\n",
      "epoch:  11   step:  73   train loss:  0.0340283177793026  val loss:  0.024631036445498466\n",
      "epoch:  11   step:  74   train loss:  0.023932484909892082  val loss:  0.024768192321062088\n",
      "epoch:  11   step:  75   train loss:  0.029028067365288734  val loss:  0.024145929142832756\n",
      "epoch:  11   step:  76   train loss:  0.04410739988088608  val loss:  0.024327095597982407\n",
      "epoch:  11   step:  77   train loss:  0.01884591206908226  val loss:  0.024331197142601013\n",
      "epoch:  11   step:  78   train loss:  0.022003451362252235  val loss:  0.024250395596027374\n",
      "epoch:  11   step:  79   train loss:  0.017048487439751625  val loss:  0.02417094074189663\n",
      "epoch:  11   step:  80   train loss:  0.03278074041008949  val loss:  0.02430148795247078\n",
      "epoch:  11   step:  81   train loss:  0.023315586149692535  val loss:  0.024007050320506096\n",
      "epoch:  11   step:  82   train loss:  0.02865220047533512  val loss:  0.02397742122411728\n",
      "epoch:  11   step:  83   train loss:  0.03160363808274269  val loss:  0.024017829447984695\n",
      "epoch:  11   step:  84   train loss:  0.030788779258728027  val loss:  0.023990383371710777\n",
      "epoch:  11   step:  85   train loss:  0.03239169716835022  val loss:  0.023887500166893005\n",
      "epoch:  11   step:  86   train loss:  0.02356933429837227  val loss:  0.023835044354200363\n",
      "epoch:  11   step:  87   train loss:  0.0386643186211586  val loss:  0.02462949976325035\n",
      "epoch:  11   step:  88   train loss:  0.0274906437844038  val loss:  0.024620186537504196\n",
      "epoch:  11   step:  89   train loss:  0.032365087419748306  val loss:  0.02417781390249729\n",
      "epoch:  11   step:  90   train loss:  0.021490992978215218  val loss:  0.024173235520720482\n",
      "epoch:  11   step:  91   train loss:  0.04964439198374748  val loss:  0.023875143378973007\n",
      "epoch:  11   step:  92   train loss:  0.032238855957984924  val loss:  0.023719467222690582\n",
      "epoch:  11   step:  93   train loss:  0.030632534995675087  val loss:  0.02380799502134323\n",
      "epoch:  11   step:  94   train loss:  0.01814388856291771  val loss:  0.024008333683013916\n",
      "epoch:  11   step:  95   train loss:  0.03714992105960846  val loss:  0.02398047409951687\n",
      "epoch:  11   step:  96   train loss:  0.02838911861181259  val loss:  0.024446748197078705\n",
      "epoch:  11   step:  97   train loss:  0.05358247086405754  val loss:  0.024063706398010254\n",
      "epoch:  11   step:  98   train loss:  0.036134131252765656  val loss:  0.024918369948863983\n",
      "epoch:  11   step:  99   train loss:  0.018737969920039177  val loss:  0.024978747591376305\n",
      "epoch:  11   step:  100   train loss:  0.05762578174471855  val loss:  0.02439199946820736\n",
      "epoch:  11   step:  101   train loss:  0.02399185299873352  val loss:  0.02459128201007843\n",
      "epoch:  11   step:  102   train loss:  0.044698137789964676  val loss:  0.024881670251488686\n",
      "epoch:  11   step:  103   train loss:  0.0347868949174881  val loss:  0.02562832273542881\n",
      "epoch:  11   step:  104   train loss:  0.025005720555782318  val loss:  0.025188013911247253\n",
      "epoch:  11   step:  105   train loss:  0.02919779345393181  val loss:  0.025299811735749245\n",
      "epoch:  11   step:  106   train loss:  0.03459148854017258  val loss:  0.025196494534611702\n",
      "epoch:  11   step:  107   train loss:  0.03229823336005211  val loss:  0.024949612095952034\n",
      "epoch:  11   step:  108   train loss:  0.025591911748051643  val loss:  0.024850958958268166\n",
      "epoch:  11   step:  109   train loss:  0.05489879474043846  val loss:  0.025563359260559082\n",
      "epoch:  11   step:  110   train loss:  0.02977871149778366  val loss:  0.026039866730570793\n",
      "epoch:  11   step:  111   train loss:  0.024449527263641357  val loss:  0.025953223928809166\n",
      "epoch:  11   step:  112   train loss:  0.03705844655632973  val loss:  0.025164902210235596\n",
      "epoch:  11   step:  113   train loss:  0.0478924997150898  val loss:  0.025117775425314903\n",
      "epoch:  11   step:  114   train loss:  0.03995291888713837  val loss:  0.02531915158033371\n",
      "epoch:  11   step:  115   train loss:  0.030360978096723557  val loss:  0.02476201392710209\n",
      "epoch:  11   step:  116   train loss:  0.05922186002135277  val loss:  0.024813776835799217\n",
      "epoch:  11   step:  117   train loss:  0.024334844201803207  val loss:  0.02445267327129841\n",
      "epoch:  11   step:  118   train loss:  0.022487258538603783  val loss:  0.024692477658391\n",
      "epoch:  11   step:  119   train loss:  0.023082083091139793  val loss:  0.025075189769268036\n",
      "epoch:  11   step:  120   train loss:  0.038592252880334854  val loss:  0.02521766722202301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  11   step:  121   train loss:  0.018788736313581467  val loss:  0.025268426164984703\n",
      "epoch:  11   step:  122   train loss:  0.029805665835738182  val loss:  0.025307396426796913\n",
      "epoch:  11   step:  123   train loss:  0.017628787085413933  val loss:  0.024887247011065483\n",
      "epoch:  11   step:  124   train loss:  0.03004794754087925  val loss:  0.024534355849027634\n",
      "epoch:  11   step:  125   train loss:  0.026466021314263344  val loss:  0.024814337491989136\n",
      "epoch:  11   step:  126   train loss:  0.019995959475636482  val loss:  0.0249574426561594\n",
      "epoch:  11   step:  127   train loss:  0.047875698655843735  val loss:  0.024640528485178947\n",
      "epoch:  11   step:  128   train loss:  0.03316499665379524  val loss:  0.024498330429196358\n",
      "epoch:  11   step:  129   train loss:  0.026355870068073273  val loss:  0.02425181306898594\n",
      "epoch:  11   step:  130   train loss:  0.033193256705999374  val loss:  0.02445191703736782\n",
      "epoch:  11   step:  131   train loss:  0.025005675852298737  val loss:  0.024372825399041176\n",
      "epoch:  11   step:  132   train loss:  0.02474784478545189  val loss:  0.024272745475172997\n",
      "epoch:  11   step:  133   train loss:  0.045122768729925156  val loss:  0.024171022698283195\n",
      "epoch:  11   step:  134   train loss:  0.029098602011799812  val loss:  0.024335797876119614\n",
      "epoch:  11   step:  135   train loss:  0.02737128548324108  val loss:  0.023475538939237595\n",
      "epoch:  11   step:  136   train loss:  0.03246035799384117  val loss:  0.023698801174759865\n",
      "epoch:  11   step:  137   train loss:  0.02371072582900524  val loss:  0.023500053212046623\n",
      "epoch:  11   step:  138   train loss:  0.0216717179864645  val loss:  0.023670976981520653\n",
      "epoch:  11   step:  139   train loss:  0.03805872052907944  val loss:  0.02296442911028862\n",
      "epoch:  11   step:  140   train loss:  0.015057068318128586  val loss:  0.023014377802610397\n",
      "epoch:  11   step:  141   train loss:  0.016266539692878723  val loss:  0.02247508242726326\n",
      "epoch:  11   step:  142   train loss:  0.04755797237157822  val loss:  0.021939458325505257\n",
      "min_val_loss_print 0.021939458325505257\n",
      "epoch:  11   step:  143   train loss:  0.027475114911794662  val loss:  0.020859263837337494\n",
      "min_val_loss_print 0.020859263837337494\n",
      "epoch:  11   step:  144   train loss:  0.030703091993927956  val loss:  0.02103124000132084\n",
      "epoch:  11   step:  145   train loss:  0.03229658305644989  val loss:  0.02099948190152645\n",
      "epoch:  11   step:  146   train loss:  0.03290114924311638  val loss:  0.020739605650305748\n",
      "min_val_loss_print 0.020739605650305748\n",
      "epoch:  11   step:  147   train loss:  0.023272207006812096  val loss:  0.020881161093711853\n",
      "epoch:  11   step:  148   train loss:  0.044263117015361786  val loss:  0.020830439403653145\n",
      "epoch:  11   step:  149   train loss:  0.024701561778783798  val loss:  0.02079867199063301\n",
      "epoch:  11   step:  150   train loss:  0.015879252925515175  val loss:  0.0203436017036438\n",
      "min_val_loss_print 0.0203436017036438\n",
      "epoch:  11   step:  151   train loss:  0.02832358330488205  val loss:  0.020455149933695793\n",
      "epoch:  11   step:  152   train loss:  0.027075495570898056  val loss:  0.020725639536976814\n",
      "epoch:  11   step:  153   train loss:  0.031866490840911865  val loss:  0.020820286124944687\n",
      "epoch:  11   step:  154   train loss:  0.04468037933111191  val loss:  0.020795000717043877\n",
      "epoch:  11   step:  155   train loss:  0.04058147966861725  val loss:  0.02054431103169918\n",
      "epoch:  11   step:  156   train loss:  0.027192698791623116  val loss:  0.02076246403157711\n",
      "epoch:  11   step:  157   train loss:  0.030453525483608246  val loss:  0.020987484604120255\n",
      "epoch:  11   step:  158   train loss:  0.023723864927887917  val loss:  0.02093975804746151\n",
      "epoch:  11   step:  159   train loss:  0.03835209831595421  val loss:  0.02096031792461872\n",
      "epoch:  11   step:  160   train loss:  0.018297431990504265  val loss:  0.021234136074781418\n",
      "epoch:  11   step:  161   train loss:  0.04206039384007454  val loss:  0.022089382633566856\n",
      "epoch:  11   step:  162   train loss:  0.02879791334271431  val loss:  0.022603249177336693\n",
      "epoch:  11   step:  163   train loss:  0.03883671015501022  val loss:  0.023397428914904594\n",
      "epoch:  11   step:  164   train loss:  0.038934655487537384  val loss:  0.023240434005856514\n",
      "epoch:  11   step:  165   train loss:  0.02579725906252861  val loss:  0.023316990584135056\n",
      "epoch:  11   step:  166   train loss:  0.033508144319057465  val loss:  0.02328977733850479\n",
      "epoch:  11   step:  167   train loss:  0.033295877277851105  val loss:  0.02336309850215912\n",
      "epoch:  11   step:  168   train loss:  0.02259162999689579  val loss:  0.023504750803112984\n",
      "epoch:  11   step:  169   train loss:  0.05051004886627197  val loss:  0.023213470354676247\n",
      "epoch:  11   step:  170   train loss:  0.031434040516614914  val loss:  0.023520098999142647\n",
      "epoch:  11   step:  171   train loss:  0.019378285855054855  val loss:  0.02379991114139557\n",
      "epoch:  11   step:  172   train loss:  0.043218061327934265  val loss:  0.023532021790742874\n",
      "epoch:  11   step:  173   train loss:  0.035703592002391815  val loss:  0.023470569401979446\n",
      "epoch:  11   step:  174   train loss:  0.021866170689463615  val loss:  0.0227811336517334\n",
      "epoch:  11   step:  175   train loss:  0.02731233648955822  val loss:  0.023366475477814674\n",
      "epoch:  11   step:  176   train loss:  0.024589406326413155  val loss:  0.02362939342856407\n",
      "epoch:  11   step:  177   train loss:  0.02053503692150116  val loss:  0.023549770936369896\n",
      "epoch:  11   step:  178   train loss:  0.02711634151637554  val loss:  0.023495158180594444\n",
      "epoch:  11   step:  179   train loss:  0.03540552034974098  val loss:  0.023500561714172363\n",
      "epoch:  11   step:  180   train loss:  0.024465864524245262  val loss:  0.023768454790115356\n",
      "epoch:  11   step:  181   train loss:  0.017251182347536087  val loss:  0.023654326796531677\n",
      "epoch:  11   step:  182   train loss:  0.023524556308984756  val loss:  0.02376522310078144\n",
      "epoch:  11   step:  183   train loss:  0.0222073495388031  val loss:  0.02394244261085987\n",
      "epoch:  11   step:  184   train loss:  0.011956494301557541  val loss:  0.024217022582888603\n",
      "epoch:  11   step:  185   train loss:  0.03873971477150917  val loss:  0.024693943560123444\n",
      "epoch:  11   step:  186   train loss:  0.023366106674075127  val loss:  0.024692067876458168\n",
      "epoch:  11   step:  187   train loss:  0.03461965173482895  val loss:  0.02486756071448326\n",
      "epoch:  11   step:  188   train loss:  0.024409720674157143  val loss:  0.02463523857295513\n",
      "epoch:  11   step:  189   train loss:  0.02014368213713169  val loss:  0.024172669276595116\n",
      "epoch:  11   step:  190   train loss:  0.030947651714086533  val loss:  0.025206172838807106\n",
      "epoch:  11   step:  191   train loss:  0.047571539878845215  val loss:  0.02468279004096985\n",
      "epoch:  11   step:  192   train loss:  0.02442225068807602  val loss:  0.024362042546272278\n",
      "epoch:  11   step:  193   train loss:  0.035386063158512115  val loss:  0.024797173216938972\n",
      "epoch:  11   step:  194   train loss:  0.036394551396369934  val loss:  0.0253589004278183\n",
      "epoch:  11   step:  195   train loss:  0.05646137148141861  val loss:  0.02438167668879032\n",
      "epoch:  11   step:  196   train loss:  0.02721787989139557  val loss:  0.02487615868449211\n",
      "epoch:  11   step:  197   train loss:  0.026156660169363022  val loss:  0.02528340183198452\n",
      "epoch:  11   step:  198   train loss:  0.02160823531448841  val loss:  0.025275351479649544\n",
      "epoch:  11   step:  199   train loss:  0.02925691194832325  val loss:  0.02539827674627304\n",
      "epoch:  11   step:  200   train loss:  0.02963075414299965  val loss:  0.025302428752183914\n",
      "epoch:  11   step:  201   train loss:  0.01883123256266117  val loss:  0.02579362876713276\n",
      "epoch:  11   step:  202   train loss:  0.026733411476016045  val loss:  0.02564098685979843\n",
      "epoch:  11   step:  203   train loss:  0.024365929886698723  val loss:  0.025619186460971832\n",
      "epoch:  11   step:  204   train loss:  0.019499188289046288  val loss:  0.025515615940093994\n",
      "epoch:  11   step:  205   train loss:  0.02405691333115101  val loss:  0.02487558126449585\n",
      "epoch:  11   step:  206   train loss:  0.023963695392012596  val loss:  0.02482655644416809\n",
      "epoch:  11   step:  207   train loss:  0.014719358645379543  val loss:  0.024971701204776764\n",
      "epoch:  11   step:  208   train loss:  0.03927384316921234  val loss:  0.024970045313239098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  11   step:  209   train loss:  0.01747453771531582  val loss:  0.025003544986248016\n",
      "epoch:  11   step:  210   train loss:  0.03221697732806206  val loss:  0.024639694020152092\n",
      "epoch:  11   step:  211   train loss:  0.03637168928980827  val loss:  0.02476847916841507\n",
      "epoch:  11   step:  212   train loss:  0.030884351581335068  val loss:  0.024653227999806404\n",
      "epoch:  11   step:  213   train loss:  0.027871888130903244  val loss:  0.024786144495010376\n",
      "epoch:  11   step:  214   train loss:  0.021450983360409737  val loss:  0.024418191984295845\n",
      "epoch:  11   step:  215   train loss:  0.04249684140086174  val loss:  0.02483142539858818\n",
      "epoch:  11   step:  216   train loss:  0.023652439936995506  val loss:  0.025131922215223312\n",
      "epoch:  11   step:  217   train loss:  0.04762302711606026  val loss:  0.025074360892176628\n",
      "epoch:  11   step:  218   train loss:  0.033555418252944946  val loss:  0.02503737434744835\n",
      "epoch:  11   step:  219   train loss:  0.03392397612333298  val loss:  0.02485276199877262\n",
      "epoch:  11   step:  220   train loss:  0.041852038353681564  val loss:  0.024746403098106384\n",
      "epoch:  11   step:  221   train loss:  0.03609521687030792  val loss:  0.02413024567067623\n",
      "epoch:  11   step:  222   train loss:  0.02967129647731781  val loss:  0.02365824766457081\n",
      "epoch:  11   step:  223   train loss:  0.03538034111261368  val loss:  0.0240740068256855\n",
      "epoch:  11   step:  224   train loss:  0.03805847838521004  val loss:  0.024387981742620468\n",
      "epoch:  11   step:  225   train loss:  0.017689108848571777  val loss:  0.02421516552567482\n",
      "epoch:  11   step:  226   train loss:  0.02985743060708046  val loss:  0.023562265560030937\n",
      "epoch:  11   step:  227   train loss:  0.020113244652748108  val loss:  0.024114912375807762\n",
      "epoch:  11   step:  228   train loss:  0.023343561217188835  val loss:  0.023982489481568336\n",
      "epoch:  11   step:  229   train loss:  0.03018161840736866  val loss:  0.02394789084792137\n",
      "epoch:  11   step:  230   train loss:  0.05230611562728882  val loss:  0.023599568754434586\n",
      "epoch:  11   step:  231   train loss:  0.016238808631896973  val loss:  0.023773785680532455\n",
      "epoch:  11   step:  232   train loss:  0.030883757397532463  val loss:  0.02303520031273365\n",
      "epoch:  11   step:  233   train loss:  0.033197496086359024  val loss:  0.023067070171236992\n",
      "epoch:  11   step:  234   train loss:  0.03135644644498825  val loss:  0.023023612797260284\n",
      "epoch:  11   step:  235   train loss:  0.034938886761665344  val loss:  0.022630512714385986\n",
      "epoch:  11   step:  236   train loss:  0.03263348713517189  val loss:  0.022424638271331787\n",
      "epoch:  11   step:  237   train loss:  0.0470808781683445  val loss:  0.022120466455817223\n",
      "epoch:  11   step:  238   train loss:  0.03898613154888153  val loss:  0.021652473136782646\n",
      "epoch:  11   step:  239   train loss:  0.049917735159397125  val loss:  0.02123585343360901\n",
      "epoch:  11   step:  240   train loss:  0.047353893518447876  val loss:  0.021367724984884262\n",
      "epoch:  11   step:  241   train loss:  0.020153669640421867  val loss:  0.021224569529294968\n",
      "epoch:  11   step:  242   train loss:  0.023238863795995712  val loss:  0.021002888679504395\n",
      "epoch:  11   step:  243   train loss:  0.025362735614180565  val loss:  0.02076762542128563\n",
      "epoch:  11   step:  244   train loss:  0.017737865447998047  val loss:  0.020649734884500504\n",
      "epoch:  11   step:  245   train loss:  0.04485020041465759  val loss:  0.020644502714276314\n",
      "epoch:  11   step:  246   train loss:  0.027876116335392  val loss:  0.020494936034083366\n",
      "epoch:  11   step:  247   train loss:  0.0451403371989727  val loss:  0.020601361989974976\n",
      "epoch:  11   step:  248   train loss:  0.02539633959531784  val loss:  0.020417004823684692\n",
      "epoch:  11   step:  249   train loss:  0.023158814758062363  val loss:  0.020531442016363144\n",
      "epoch:  11   step:  250   train loss:  0.030264025554060936  val loss:  0.021139051765203476\n",
      "epoch:  11   step:  251   train loss:  0.042454611510038376  val loss:  0.02135833539068699\n",
      "epoch:  11   step:  252   train loss:  0.025620397180318832  val loss:  0.02156231179833412\n",
      "epoch:  11   step:  253   train loss:  0.01326964795589447  val loss:  0.0215380247682333\n",
      "epoch:  11   step:  254   train loss:  0.035279661417007446  val loss:  0.02151503786444664\n",
      "epoch:  11   step:  255   train loss:  0.019835252314805984  val loss:  0.021279750391840935\n",
      "epoch:  11   step:  256   train loss:  0.02877633646130562  val loss:  0.021836161613464355\n",
      "epoch:  11   step:  257   train loss:  0.018784262239933014  val loss:  0.02199404500424862\n",
      "epoch:  11   step:  258   train loss:  0.026277074590325356  val loss:  0.022117285057902336\n",
      "epoch:  11   step:  259   train loss:  0.02286900021135807  val loss:  0.021868305280804634\n",
      "epoch:  11   step:  260   train loss:  0.01675671897828579  val loss:  0.021727602928876877\n",
      "epoch:  11   step:  261   train loss:  0.038795020431280136  val loss:  0.02131309174001217\n",
      "epoch:  11   step:  262   train loss:  0.02869998663663864  val loss:  0.02156759425997734\n",
      "epoch:  11   step:  263   train loss:  0.021280759945511818  val loss:  0.021508634090423584\n",
      "epoch:  11   step:  264   train loss:  0.03168386593461037  val loss:  0.02232499234378338\n",
      "epoch:  11   step:  265   train loss:  0.030362695455551147  val loss:  0.02180388942360878\n",
      "epoch:  11   step:  266   train loss:  0.04163498803973198  val loss:  0.021708404645323753\n",
      "epoch:  11   step:  267   train loss:  0.029388099908828735  val loss:  0.02145685814321041\n",
      "epoch:  11   step:  268   train loss:  0.026418635621666908  val loss:  0.02196180634200573\n",
      "epoch:  11   step:  269   train loss:  0.04432934522628784  val loss:  0.02231636829674244\n",
      "epoch:  11   step:  270   train loss:  0.029013311490416527  val loss:  0.02298870123922825\n",
      "epoch:  11   step:  271   train loss:  0.03489037603139877  val loss:  0.02330985851585865\n",
      "epoch:  11   step:  272   train loss:  0.024004532024264336  val loss:  0.023926977068185806\n",
      "epoch:  11   step:  273   train loss:  0.030504152178764343  val loss:  0.02410188689827919\n",
      "epoch:  11   step:  274   train loss:  0.02095917798578739  val loss:  0.024121932685375214\n",
      "epoch:  11   step:  275   train loss:  0.028963899239897728  val loss:  0.023791272193193436\n",
      "epoch:  11   step:  276   train loss:  0.029678678140044212  val loss:  0.022986261174082756\n",
      "epoch:  11   step:  277   train loss:  0.05079478770494461  val loss:  0.023442676290869713\n",
      "epoch:  11   step:  278   train loss:  0.02149396575987339  val loss:  0.024002213031053543\n",
      "epoch:  11   step:  279   train loss:  0.026310404762625694  val loss:  0.023891299962997437\n",
      "epoch:  11   step:  280   train loss:  0.021557655185461044  val loss:  0.02330803871154785\n",
      "epoch:  11   step:  281   train loss:  0.04434363543987274  val loss:  0.02249087765812874\n",
      "epoch:  11   step:  282   train loss:  0.02601305954158306  val loss:  0.022700686007738113\n",
      "epoch:  11   step:  283   train loss:  0.036342471837997437  val loss:  0.02329881861805916\n",
      "epoch:  11   step:  284   train loss:  0.024518180638551712  val loss:  0.022989770397543907\n",
      "epoch:  11   step:  285   train loss:  0.02184985764324665  val loss:  0.022879494354128838\n",
      "epoch:  11   step:  286   train loss:  0.04194933548569679  val loss:  0.02246631681919098\n",
      "epoch:  11   step:  287   train loss:  0.02624945528805256  val loss:  0.022098956629633904\n",
      "epoch:  11   step:  288   train loss:  0.028468674048781395  val loss:  0.022554663941264153\n",
      "epoch:  11   step:  289   train loss:  0.02565424144268036  val loss:  0.022676236927509308\n",
      "epoch:  11   step:  290   train loss:  0.016066353768110275  val loss:  0.022691750898957253\n",
      "epoch:  11   step:  291   train loss:  0.025969460606575012  val loss:  0.02260831743478775\n",
      "epoch:  11   step:  292   train loss:  0.019897213205695152  val loss:  0.022989921271800995\n",
      "epoch:  11   step:  293   train loss:  0.028076978400349617  val loss:  0.023043673485517502\n",
      "epoch:  11   step:  294   train loss:  0.032326839864254  val loss:  0.023304328322410583\n",
      "epoch:  11   step:  295   train loss:  0.01793239451944828  val loss:  0.023540539667010307\n",
      "epoch:  11   step:  296   train loss:  0.028472553938627243  val loss:  0.023794015869498253\n",
      "epoch:  11   step:  297   train loss:  0.03314520791172981  val loss:  0.023927556350827217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  11   step:  298   train loss:  0.023914452642202377  val loss:  0.023659722879529\n",
      "epoch:  11   step:  299   train loss:  0.02154788188636303  val loss:  0.0237573254853487\n",
      "epoch:  11   step:  300   train loss:  0.02271459437906742  val loss:  0.023920772597193718\n",
      "epoch:  11   step:  301   train loss:  0.028969375416636467  val loss:  0.023854415863752365\n",
      "epoch:  11   step:  302   train loss:  0.03518976643681526  val loss:  0.023335246369242668\n",
      "epoch:  11   step:  303   train loss:  0.0219552181661129  val loss:  0.023678326979279518\n",
      "epoch:  11   step:  304   train loss:  0.0365639291703701  val loss:  0.02346430905163288\n",
      "epoch:  11   step:  305   train loss:  0.030203135684132576  val loss:  0.023022593930363655\n",
      "epoch:  11   step:  306   train loss:  0.028399502858519554  val loss:  0.023218650370836258\n",
      "epoch:  11   step:  307   train loss:  0.03316231071949005  val loss:  0.023171132430434227\n",
      "epoch:  11   step:  308   train loss:  0.029699617996811867  val loss:  0.023013312369585037\n",
      "epoch:  11   step:  309   train loss:  0.028826314955949783  val loss:  0.023126021027565002\n",
      "epoch:  11   step:  310   train loss:  0.02902633138000965  val loss:  0.02370963618159294\n",
      "epoch:  11   step:  311   train loss:  0.022018736228346825  val loss:  0.023666594177484512\n",
      "epoch:  11   step:  312   train loss:  0.02937113307416439  val loss:  0.02361377328634262\n",
      "epoch:  11   step:  313   train loss:  0.020349202677607536  val loss:  0.0235251747071743\n",
      "epoch:  11   step:  314   train loss:  0.027380263432860374  val loss:  0.023823363706469536\n",
      "epoch:  11   step:  315   train loss:  0.02575002610683441  val loss:  0.023946363478899002\n",
      "epoch:  11   step:  316   train loss:  0.04481993988156319  val loss:  0.023273039609193802\n",
      "epoch:  11   step:  317   train loss:  0.029935043305158615  val loss:  0.02312510460615158\n",
      "epoch:  11   step:  318   train loss:  0.04627932608127594  val loss:  0.023234311491250992\n",
      "epoch:  11   step:  319   train loss:  0.03556569293141365  val loss:  0.023593949154019356\n",
      "epoch:  11   step:  320   train loss:  0.026716221123933792  val loss:  0.02310005947947502\n",
      "epoch:  11   step:  321   train loss:  0.026943646371364594  val loss:  0.023182470351457596\n",
      "epoch:  11   step:  322   train loss:  0.04033369943499565  val loss:  0.023393940180540085\n",
      "epoch:  11   step:  323   train loss:  0.01918722689151764  val loss:  0.022608682513237\n",
      "epoch:  11   step:  324   train loss:  0.041730932891368866  val loss:  0.022595608606934547\n",
      "epoch:  11   step:  325   train loss:  0.033212799578905106  val loss:  0.02264454960823059\n",
      "epoch:  11   step:  326   train loss:  0.055439453572034836  val loss:  0.022894365713000298\n",
      "epoch:  11   step:  327   train loss:  0.03219342604279518  val loss:  0.023022379726171494\n",
      "epoch:  11   step:  328   train loss:  0.024033693596720695  val loss:  0.022548941895365715\n",
      "epoch:  11   step:  329   train loss:  0.02016206458210945  val loss:  0.022479118779301643\n",
      "epoch:  11   step:  330   train loss:  0.03650040924549103  val loss:  0.02261168695986271\n",
      "epoch:  11   step:  331   train loss:  0.04034058749675751  val loss:  0.022688357159495354\n",
      "epoch:  11   step:  332   train loss:  0.04762254282832146  val loss:  0.022303825244307518\n",
      "epoch:  11   step:  333   train loss:  0.020382290706038475  val loss:  0.022416088730096817\n",
      "epoch:  11   step:  334   train loss:  0.022777710109949112  val loss:  0.021704498678445816\n",
      "epoch:  11   step:  335   train loss:  0.04774128645658493  val loss:  0.021721551194787025\n",
      "epoch:  11   step:  336   train loss:  0.043679866939783096  val loss:  0.021310700103640556\n",
      "epoch:  11   step:  337   train loss:  0.025964343920350075  val loss:  0.021555528044700623\n",
      "epoch:  11   step:  338   train loss:  0.0411740280687809  val loss:  0.021948376670479774\n",
      "epoch:  11   step:  339   train loss:  0.013026234693825245  val loss:  0.02197841927409172\n",
      "epoch:  11   step:  340   train loss:  0.026090119034051895  val loss:  0.021976977586746216\n",
      "epoch:  11   step:  341   train loss:  0.018541667610406876  val loss:  0.021915053948760033\n",
      "epoch:  11   step:  342   train loss:  0.02862084098160267  val loss:  0.02205990068614483\n",
      "epoch:  11   step:  343   train loss:  0.02271137572824955  val loss:  0.021487515419721603\n",
      "epoch:  12   step:  0   train loss:  0.021936217322945595  val loss:  0.021680744364857674\n",
      "epoch:  12   step:  1   train loss:  0.022391950711607933  val loss:  0.022144746035337448\n",
      "epoch:  12   step:  2   train loss:  0.043253909796476364  val loss:  0.022732704877853394\n",
      "epoch:  12   step:  3   train loss:  0.05378060042858124  val loss:  0.02262602560222149\n",
      "epoch:  12   step:  4   train loss:  0.026431690901517868  val loss:  0.022486720234155655\n",
      "epoch:  12   step:  5   train loss:  0.0258194413036108  val loss:  0.022284314036369324\n",
      "epoch:  12   step:  6   train loss:  0.02054089680314064  val loss:  0.02208402194082737\n",
      "epoch:  12   step:  7   train loss:  0.03247363492846489  val loss:  0.021909348666667938\n",
      "epoch:  12   step:  8   train loss:  0.012614107690751553  val loss:  0.021854832768440247\n",
      "epoch:  12   step:  9   train loss:  0.03480694442987442  val loss:  0.022321142256259918\n",
      "epoch:  12   step:  10   train loss:  0.022082192823290825  val loss:  0.021970335394144058\n",
      "epoch:  12   step:  11   train loss:  0.037893492728471756  val loss:  0.02202426642179489\n",
      "epoch:  12   step:  12   train loss:  0.030491897836327553  val loss:  0.022105418145656586\n",
      "epoch:  12   step:  13   train loss:  0.027873598039150238  val loss:  0.02214675396680832\n",
      "epoch:  12   step:  14   train loss:  0.032115064561367035  val loss:  0.02256261371076107\n",
      "epoch:  12   step:  15   train loss:  0.039054255932569504  val loss:  0.02224036678671837\n",
      "epoch:  12   step:  16   train loss:  0.030344760045409203  val loss:  0.022663012146949768\n",
      "epoch:  12   step:  17   train loss:  0.027279548346996307  val loss:  0.02152315340936184\n",
      "epoch:  12   step:  18   train loss:  0.03276825323700905  val loss:  0.022528590634465218\n",
      "epoch:  12   step:  19   train loss:  0.040583640336990356  val loss:  0.02250329591333866\n",
      "epoch:  12   step:  20   train loss:  0.023227509111166  val loss:  0.023068608716130257\n",
      "epoch:  12   step:  21   train loss:  0.04175209254026413  val loss:  0.022927971556782722\n",
      "epoch:  12   step:  22   train loss:  0.019785569980740547  val loss:  0.022596746683120728\n",
      "epoch:  12   step:  23   train loss:  0.02194800414144993  val loss:  0.022200724110007286\n",
      "epoch:  12   step:  24   train loss:  0.03216894716024399  val loss:  0.02177424542605877\n",
      "epoch:  12   step:  25   train loss:  0.01767769083380699  val loss:  0.02102578617632389\n",
      "epoch:  12   step:  26   train loss:  0.02991163544356823  val loss:  0.02130826748907566\n",
      "epoch:  12   step:  27   train loss:  0.02941386215388775  val loss:  0.02125745266675949\n",
      "epoch:  12   step:  28   train loss:  0.02676672860980034  val loss:  0.021310046315193176\n",
      "epoch:  12   step:  29   train loss:  0.015857595950365067  val loss:  0.021274855360388756\n",
      "epoch:  12   step:  30   train loss:  0.03620187193155289  val loss:  0.02145400643348694\n",
      "epoch:  12   step:  31   train loss:  0.020447611808776855  val loss:  0.021661605685949326\n",
      "epoch:  12   step:  32   train loss:  0.019952857866883278  val loss:  0.02172579988837242\n",
      "epoch:  12   step:  33   train loss:  0.024678414687514305  val loss:  0.021779464557766914\n",
      "epoch:  12   step:  34   train loss:  0.026077641174197197  val loss:  0.02118377387523651\n",
      "epoch:  12   step:  35   train loss:  0.01630314812064171  val loss:  0.02118227444589138\n",
      "epoch:  12   step:  36   train loss:  0.037479232996702194  val loss:  0.021771561354398727\n",
      "epoch:  12   step:  37   train loss:  0.0225327517837286  val loss:  0.021300161257386208\n",
      "epoch:  12   step:  38   train loss:  0.017980774864554405  val loss:  0.021009238436818123\n",
      "epoch:  12   step:  39   train loss:  0.015910539776086807  val loss:  0.021057892590761185\n",
      "epoch:  12   step:  40   train loss:  0.03460829332470894  val loss:  0.020902587100863457\n",
      "epoch:  12   step:  41   train loss:  0.02235274575650692  val loss:  0.020710503682494164\n",
      "epoch:  12   step:  42   train loss:  0.027816995978355408  val loss:  0.020541099831461906\n",
      "epoch:  12   step:  43   train loss:  0.03849540278315544  val loss:  0.02046467363834381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  12   step:  44   train loss:  0.03290576860308647  val loss:  0.0207801666110754\n",
      "epoch:  12   step:  45   train loss:  0.028703974559903145  val loss:  0.020726341754198074\n",
      "epoch:  12   step:  46   train loss:  0.027196763083338737  val loss:  0.020867271348834038\n",
      "epoch:  12   step:  47   train loss:  0.035359013825654984  val loss:  0.020834866911172867\n",
      "epoch:  12   step:  48   train loss:  0.019567659124732018  val loss:  0.02101735956966877\n",
      "epoch:  12   step:  49   train loss:  0.033197708427906036  val loss:  0.021350940689444542\n",
      "epoch:  12   step:  50   train loss:  0.0395374670624733  val loss:  0.021637465804815292\n",
      "epoch:  12   step:  51   train loss:  0.025741884484887123  val loss:  0.021876836195588112\n",
      "epoch:  12   step:  52   train loss:  0.019717995077371597  val loss:  0.02161036804318428\n",
      "epoch:  12   step:  53   train loss:  0.014712871983647346  val loss:  0.02154482714831829\n",
      "epoch:  12   step:  54   train loss:  0.022766543552279472  val loss:  0.021486742421984673\n",
      "epoch:  12   step:  55   train loss:  0.018514560535550117  val loss:  0.02151952125132084\n",
      "epoch:  12   step:  56   train loss:  0.040375348180532455  val loss:  0.021840937435626984\n",
      "epoch:  12   step:  57   train loss:  0.018088433891534805  val loss:  0.02217438444495201\n",
      "epoch:  12   step:  58   train loss:  0.02501101978123188  val loss:  0.021999794989824295\n",
      "epoch:  12   step:  59   train loss:  0.033811841160058975  val loss:  0.021702397614717484\n",
      "epoch:  12   step:  60   train loss:  0.04276302456855774  val loss:  0.02162080444395542\n",
      "epoch:  12   step:  61   train loss:  0.05784331634640694  val loss:  0.027453361079096794\n",
      "epoch:  12   step:  62   train loss:  0.013791163451969624  val loss:  0.027736112475395203\n",
      "epoch:  12   step:  63   train loss:  0.020531129091978073  val loss:  0.027657190337777138\n",
      "epoch:  12   step:  64   train loss:  0.039501678198575974  val loss:  0.027622409164905548\n",
      "epoch:  12   step:  65   train loss:  0.027321146801114082  val loss:  0.028217241168022156\n",
      "epoch:  12   step:  66   train loss:  0.020914459601044655  val loss:  0.02864023484289646\n",
      "epoch:  12   step:  67   train loss:  0.03002743050456047  val loss:  0.028043407946825027\n",
      "epoch:  12   step:  68   train loss:  0.03956088796257973  val loss:  0.028378261253237724\n",
      "epoch:  12   step:  69   train loss:  0.022208524867892265  val loss:  0.02873798832297325\n",
      "epoch:  12   step:  70   train loss:  0.029568243771791458  val loss:  0.028622614219784737\n",
      "epoch:  12   step:  71   train loss:  0.01930484175682068  val loss:  0.028788350522518158\n",
      "epoch:  12   step:  72   train loss:  0.037172503769397736  val loss:  0.028333015739917755\n",
      "epoch:  12   step:  73   train loss:  0.05420217663049698  val loss:  0.02836974896490574\n",
      "epoch:  12   step:  74   train loss:  0.02691321261227131  val loss:  0.029106877744197845\n",
      "epoch:  12   step:  75   train loss:  0.023763421922922134  val loss:  0.030579041689634323\n",
      "epoch:  12   step:  76   train loss:  0.020912455394864082  val loss:  0.03011390008032322\n",
      "epoch:  12   step:  77   train loss:  0.02130819484591484  val loss:  0.030528731644153595\n",
      "epoch:  12   step:  78   train loss:  0.03008008375763893  val loss:  0.02969341352581978\n",
      "epoch:  12   step:  79   train loss:  0.02225009724497795  val loss:  0.028830235823988914\n",
      "epoch:  12   step:  80   train loss:  0.03217494860291481  val loss:  0.029277974739670753\n",
      "epoch:  12   step:  81   train loss:  0.033982206135988235  val loss:  0.028701523318886757\n",
      "epoch:  12   step:  82   train loss:  0.02090725302696228  val loss:  0.029602937400341034\n",
      "epoch:  12   step:  83   train loss:  0.02068483643233776  val loss:  0.030461272224783897\n",
      "epoch:  12   step:  84   train loss:  0.023754872381687164  val loss:  0.030432531610131264\n",
      "epoch:  12   step:  85   train loss:  0.02920549362897873  val loss:  0.031248440966010094\n",
      "epoch:  12   step:  86   train loss:  0.046990953385829926  val loss:  0.031604643911123276\n",
      "epoch:  12   step:  87   train loss:  0.01757478155195713  val loss:  0.032236140221357346\n",
      "epoch:  12   step:  88   train loss:  0.020000940188765526  val loss:  0.03176981955766678\n",
      "epoch:  12   step:  89   train loss:  0.017277728766202927  val loss:  0.032307256013154984\n",
      "epoch:  12   step:  90   train loss:  0.03224973380565643  val loss:  0.03204817697405815\n",
      "epoch:  12   step:  91   train loss:  0.019671836867928505  val loss:  0.03126821666955948\n",
      "epoch:  12   step:  92   train loss:  0.02850325033068657  val loss:  0.03114999644458294\n",
      "epoch:  12   step:  93   train loss:  0.028848862275481224  val loss:  0.030295226722955704\n",
      "epoch:  12   step:  94   train loss:  0.02551214024424553  val loss:  0.03054426610469818\n",
      "epoch:  12   step:  95   train loss:  0.03729726001620293  val loss:  0.030376680195331573\n",
      "epoch:  12   step:  96   train loss:  0.02711925469338894  val loss:  0.03100474365055561\n",
      "epoch:  12   step:  97   train loss:  0.02230232022702694  val loss:  0.030973270535469055\n",
      "epoch:  12   step:  98   train loss:  0.024188918992877007  val loss:  0.030941132456064224\n",
      "epoch:  12   step:  99   train loss:  0.014525060541927814  val loss:  0.030274400487542152\n",
      "epoch:  12   step:  100   train loss:  0.018392525613307953  val loss:  0.03088163584470749\n",
      "epoch:  12   step:  101   train loss:  0.027310431003570557  val loss:  0.030210448428988457\n",
      "epoch:  12   step:  102   train loss:  0.020774122327566147  val loss:  0.029396697878837585\n",
      "epoch:  12   step:  103   train loss:  0.025610221549868584  val loss:  0.029145224019885063\n",
      "epoch:  12   step:  104   train loss:  0.03805999830365181  val loss:  0.028961457312107086\n",
      "epoch:  12   step:  105   train loss:  0.02697032131254673  val loss:  0.028996193781495094\n",
      "epoch:  12   step:  106   train loss:  0.049935661256313324  val loss:  0.030091729015111923\n",
      "epoch:  12   step:  107   train loss:  0.03166557848453522  val loss:  0.029991034418344498\n",
      "epoch:  12   step:  108   train loss:  0.03962765634059906  val loss:  0.02944207936525345\n",
      "epoch:  12   step:  109   train loss:  0.02083209529519081  val loss:  0.029509855434298515\n",
      "epoch:  12   step:  110   train loss:  0.029768148437142372  val loss:  0.03010581061244011\n",
      "epoch:  12   step:  111   train loss:  0.0352628193795681  val loss:  0.03046805039048195\n",
      "epoch:  12   step:  112   train loss:  0.02047782391309738  val loss:  0.029395781457424164\n",
      "epoch:  12   step:  113   train loss:  0.021781234070658684  val loss:  0.028812190517783165\n",
      "epoch:  12   step:  114   train loss:  0.030676137655973434  val loss:  0.028158236294984818\n",
      "epoch:  12   step:  115   train loss:  0.028326595202088356  val loss:  0.02806713618338108\n",
      "epoch:  12   step:  116   train loss:  0.055959735065698624  val loss:  0.02858409844338894\n",
      "epoch:  12   step:  117   train loss:  0.0416877344250679  val loss:  0.02817770652472973\n",
      "epoch:  12   step:  118   train loss:  0.030342595651745796  val loss:  0.028111813589930534\n",
      "epoch:  12   step:  119   train loss:  0.032170992344617844  val loss:  0.028136316686868668\n",
      "epoch:  12   step:  120   train loss:  0.030919482931494713  val loss:  0.028645571321249008\n",
      "epoch:  12   step:  121   train loss:  0.020943008363246918  val loss:  0.02825242653489113\n",
      "epoch:  12   step:  122   train loss:  0.026878947392106056  val loss:  0.028594186529517174\n",
      "epoch:  12   step:  123   train loss:  0.025931265205144882  val loss:  0.028983596712350845\n",
      "epoch:  12   step:  124   train loss:  0.03859645500779152  val loss:  0.0289987213909626\n",
      "epoch:  12   step:  125   train loss:  0.02248416841030121  val loss:  0.0287228561937809\n",
      "epoch:  12   step:  126   train loss:  0.027903888374567032  val loss:  0.027933016419410706\n",
      "epoch:  12   step:  127   train loss:  0.04293374717235565  val loss:  0.027274979278445244\n",
      "epoch:  12   step:  128   train loss:  0.02598288469016552  val loss:  0.026617970317602158\n",
      "epoch:  12   step:  129   train loss:  0.03582290932536125  val loss:  0.026904165744781494\n",
      "epoch:  12   step:  130   train loss:  0.06759046018123627  val loss:  0.026065994054079056\n",
      "epoch:  12   step:  131   train loss:  0.030173610895872116  val loss:  0.026557696983218193\n",
      "epoch:  12   step:  132   train loss:  0.020247677341103554  val loss:  0.026489321142435074\n",
      "epoch:  12   step:  133   train loss:  0.030686218291521072  val loss:  0.02699260041117668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  12   step:  134   train loss:  0.02509867586195469  val loss:  0.028058839961886406\n",
      "epoch:  12   step:  135   train loss:  0.017394840717315674  val loss:  0.028782321140170097\n",
      "epoch:  12   step:  136   train loss:  0.03654904663562775  val loss:  0.02917887084186077\n",
      "epoch:  12   step:  137   train loss:  0.020733360201120377  val loss:  0.02948453463613987\n",
      "epoch:  12   step:  138   train loss:  0.024255296215415  val loss:  0.029947221279144287\n",
      "epoch:  12   step:  139   train loss:  0.041273292154073715  val loss:  0.029397878795862198\n",
      "epoch:  12   step:  140   train loss:  0.024691879749298096  val loss:  0.029864782467484474\n",
      "epoch:  12   step:  141   train loss:  0.018653564155101776  val loss:  0.030069692060351372\n",
      "epoch:  12   step:  142   train loss:  0.02189352735877037  val loss:  0.029746191576123238\n",
      "epoch:  12   step:  143   train loss:  0.018752625212073326  val loss:  0.030013876035809517\n",
      "epoch:  12   step:  144   train loss:  0.026176633313298225  val loss:  0.030330952256917953\n",
      "epoch:  12   step:  145   train loss:  0.03150789067149162  val loss:  0.030539000406861305\n",
      "epoch:  12   step:  146   train loss:  0.01731313206255436  val loss:  0.031246243044734\n",
      "epoch:  12   step:  147   train loss:  0.015081499703228474  val loss:  0.03030073083937168\n",
      "epoch:  12   step:  148   train loss:  0.02326285094022751  val loss:  0.029840636998414993\n",
      "epoch:  12   step:  149   train loss:  0.023071331903338432  val loss:  0.02975836955010891\n",
      "epoch:  12   step:  150   train loss:  0.036401066929101944  val loss:  0.02928273007273674\n",
      "epoch:  12   step:  151   train loss:  0.025753449648618698  val loss:  0.02857193350791931\n",
      "epoch:  12   step:  152   train loss:  0.035949572920799255  val loss:  0.029770756140351295\n",
      "epoch:  12   step:  153   train loss:  0.02864740788936615  val loss:  0.028831517323851585\n",
      "epoch:  12   step:  154   train loss:  0.02531191147863865  val loss:  0.029220785945653915\n",
      "epoch:  12   step:  155   train loss:  0.04281265661120415  val loss:  0.029111171141266823\n",
      "epoch:  12   step:  156   train loss:  0.023866333067417145  val loss:  0.029048841446638107\n",
      "epoch:  12   step:  157   train loss:  0.014284591190516949  val loss:  0.02872411161661148\n",
      "epoch:  12   step:  158   train loss:  0.017724448814988136  val loss:  0.028706179931759834\n",
      "epoch:  12   step:  159   train loss:  0.021479036659002304  val loss:  0.02897021174430847\n",
      "epoch:  12   step:  160   train loss:  0.022337656468153  val loss:  0.02978912927210331\n",
      "epoch:  12   step:  161   train loss:  0.02276395633816719  val loss:  0.03013261966407299\n",
      "epoch:  12   step:  162   train loss:  0.033795371651649475  val loss:  0.030594605952501297\n",
      "epoch:  12   step:  163   train loss:  0.029905227944254875  val loss:  0.030206821858882904\n",
      "epoch:  12   step:  164   train loss:  0.026278836652636528  val loss:  0.030842460691928864\n",
      "epoch:  12   step:  165   train loss:  0.017592214047908783  val loss:  0.03160447999835014\n",
      "epoch:  12   step:  166   train loss:  0.028100211173295975  val loss:  0.032066620886325836\n",
      "epoch:  12   step:  167   train loss:  0.01693803071975708  val loss:  0.03147955238819122\n",
      "epoch:  12   step:  168   train loss:  0.030240792781114578  val loss:  0.029739664867520332\n",
      "epoch:  12   step:  169   train loss:  0.03747521713376045  val loss:  0.030344979837536812\n",
      "epoch:  12   step:  170   train loss:  0.044044144451618195  val loss:  0.029663700610399246\n",
      "epoch:  12   step:  171   train loss:  0.01892969384789467  val loss:  0.029095804318785667\n",
      "epoch:  12   step:  172   train loss:  0.031844258308410645  val loss:  0.030238935723900795\n",
      "epoch:  12   step:  173   train loss:  0.029582960531115532  val loss:  0.029889507219195366\n",
      "epoch:  12   step:  174   train loss:  0.038070887327194214  val loss:  0.029588371515274048\n",
      "epoch:  12   step:  175   train loss:  0.025927221402525902  val loss:  0.028821533545851707\n",
      "epoch:  12   step:  176   train loss:  0.0311847273260355  val loss:  0.027758290991187096\n",
      "epoch:  12   step:  177   train loss:  0.025468159466981888  val loss:  0.028509845957159996\n",
      "epoch:  12   step:  178   train loss:  0.025562824681401253  val loss:  0.029404684901237488\n",
      "epoch:  12   step:  179   train loss:  0.04092962667346001  val loss:  0.028507160022854805\n",
      "epoch:  12   step:  180   train loss:  0.019921589642763138  val loss:  0.027346068993210793\n",
      "epoch:  12   step:  181   train loss:  0.027128567919135094  val loss:  0.0281301811337471\n",
      "epoch:  12   step:  182   train loss:  0.019889947026968002  val loss:  0.026540741324424744\n",
      "epoch:  12   step:  183   train loss:  0.015182613395154476  val loss:  0.026516862213611603\n",
      "epoch:  12   step:  184   train loss:  0.019747179001569748  val loss:  0.02690030261874199\n",
      "epoch:  12   step:  185   train loss:  0.026919987052679062  val loss:  0.02630767971277237\n",
      "epoch:  12   step:  186   train loss:  0.03178631141781807  val loss:  0.026323162019252777\n",
      "epoch:  12   step:  187   train loss:  0.02867225743830204  val loss:  0.026313815265893936\n",
      "epoch:  12   step:  188   train loss:  0.03158418461680412  val loss:  0.026387732475996017\n",
      "epoch:  12   step:  189   train loss:  0.030073653906583786  val loss:  0.02756780944764614\n",
      "epoch:  12   step:  190   train loss:  0.02328498847782612  val loss:  0.02769806981086731\n",
      "epoch:  12   step:  191   train loss:  0.032093968242406845  val loss:  0.028270862996578217\n",
      "epoch:  12   step:  192   train loss:  0.029812876135110855  val loss:  0.028550265356898308\n",
      "epoch:  12   step:  193   train loss:  0.019052818417549133  val loss:  0.027737654745578766\n",
      "epoch:  12   step:  194   train loss:  0.020610831677913666  val loss:  0.028490466997027397\n",
      "epoch:  12   step:  195   train loss:  0.015937376767396927  val loss:  0.027588633820414543\n",
      "epoch:  12   step:  196   train loss:  0.02284666895866394  val loss:  0.028491659089922905\n",
      "epoch:  12   step:  197   train loss:  0.020460408180952072  val loss:  0.028404347598552704\n",
      "epoch:  12   step:  198   train loss:  0.027137525379657745  val loss:  0.028661495074629784\n",
      "epoch:  12   step:  199   train loss:  0.04974543675780296  val loss:  0.028644509613513947\n",
      "epoch:  12   step:  200   train loss:  0.024400370195508003  val loss:  0.029837187379598618\n",
      "epoch:  12   step:  201   train loss:  0.032459806650877  val loss:  0.0301594790071249\n",
      "epoch:  12   step:  202   train loss:  0.03175807744264603  val loss:  0.03065844625234604\n",
      "epoch:  12   step:  203   train loss:  0.022095829248428345  val loss:  0.031038055196404457\n",
      "epoch:  12   step:  204   train loss:  0.05010240152478218  val loss:  0.03198320046067238\n",
      "epoch:  12   step:  205   train loss:  0.025997774675488472  val loss:  0.032244279980659485\n",
      "epoch:  12   step:  206   train loss:  0.024915756657719612  val loss:  0.03236325830221176\n",
      "epoch:  12   step:  207   train loss:  0.02664961665868759  val loss:  0.032228983938694\n",
      "epoch:  12   step:  208   train loss:  0.029494788497686386  val loss:  0.032043345272541046\n",
      "epoch:  12   step:  209   train loss:  0.03344829007983208  val loss:  0.03180494159460068\n",
      "epoch:  12   step:  210   train loss:  0.03243403509259224  val loss:  0.03136349469423294\n",
      "epoch:  12   step:  211   train loss:  0.029010869562625885  val loss:  0.030445698648691177\n",
      "epoch:  12   step:  212   train loss:  0.0217019971460104  val loss:  0.030713306739926338\n",
      "epoch:  12   step:  213   train loss:  0.031346023082733154  val loss:  0.03122970648109913\n",
      "epoch:  12   step:  214   train loss:  0.026721129193902016  val loss:  0.03141726180911064\n",
      "epoch:  12   step:  215   train loss:  0.03431128337979317  val loss:  0.03203393518924713\n",
      "epoch:  12   step:  216   train loss:  0.025616826489567757  val loss:  0.03215646743774414\n",
      "epoch:  12   step:  217   train loss:  0.018355006352066994  val loss:  0.03141092136502266\n",
      "epoch:  12   step:  218   train loss:  0.040797360241413116  val loss:  0.031447261571884155\n",
      "epoch:  12   step:  219   train loss:  0.030994435772299767  val loss:  0.029609430581331253\n",
      "epoch:  12   step:  220   train loss:  0.017221108078956604  val loss:  0.029006175696849823\n",
      "epoch:  12   step:  221   train loss:  0.037436243146657944  val loss:  0.029350122436881065\n",
      "epoch:  12   step:  222   train loss:  0.019976608455181122  val loss:  0.028711946681141853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  12   step:  223   train loss:  0.024872466921806335  val loss:  0.02915174886584282\n",
      "epoch:  12   step:  224   train loss:  0.01987786591053009  val loss:  0.029452841728925705\n",
      "epoch:  12   step:  225   train loss:  0.018992891535162926  val loss:  0.029540428891777992\n",
      "epoch:  12   step:  226   train loss:  0.02170330286026001  val loss:  0.029439685866236687\n",
      "epoch:  12   step:  227   train loss:  0.04164576530456543  val loss:  0.029244525358080864\n",
      "epoch:  12   step:  228   train loss:  0.022601187229156494  val loss:  0.02874506637454033\n",
      "epoch:  12   step:  229   train loss:  0.03669235110282898  val loss:  0.027362514287233353\n",
      "epoch:  12   step:  230   train loss:  0.017719300463795662  val loss:  0.027018090710043907\n",
      "epoch:  12   step:  231   train loss:  0.03247735649347305  val loss:  0.027430115267634392\n",
      "epoch:  12   step:  232   train loss:  0.044291045516729355  val loss:  0.02901167795062065\n",
      "epoch:  12   step:  233   train loss:  0.04584747552871704  val loss:  0.028688350692391396\n",
      "epoch:  12   step:  234   train loss:  0.021994484588503838  val loss:  0.027993962168693542\n",
      "epoch:  12   step:  235   train loss:  0.026187177747488022  val loss:  0.027417676523327827\n",
      "epoch:  12   step:  236   train loss:  0.02247895672917366  val loss:  0.027466433122754097\n",
      "epoch:  12   step:  237   train loss:  0.028377197682857513  val loss:  0.028723761439323425\n",
      "epoch:  12   step:  238   train loss:  0.04248344525694847  val loss:  0.02943354845046997\n",
      "epoch:  12   step:  239   train loss:  0.023948848247528076  val loss:  0.029024479910731316\n",
      "epoch:  12   step:  240   train loss:  0.022738195955753326  val loss:  0.029214227572083473\n",
      "epoch:  12   step:  241   train loss:  0.056197624653577805  val loss:  0.028865430504083633\n",
      "epoch:  12   step:  242   train loss:  0.021476920694112778  val loss:  0.02883010171353817\n",
      "epoch:  12   step:  243   train loss:  0.0445849746465683  val loss:  0.028993884101510048\n",
      "epoch:  12   step:  244   train loss:  0.024607442319393158  val loss:  0.028853559866547585\n",
      "epoch:  12   step:  245   train loss:  0.0290251262485981  val loss:  0.029072707518935204\n",
      "epoch:  12   step:  246   train loss:  0.022290097549557686  val loss:  0.02924765832722187\n",
      "epoch:  12   step:  247   train loss:  0.01887911558151245  val loss:  0.02962885983288288\n",
      "epoch:  12   step:  248   train loss:  0.03817804902791977  val loss:  0.029294509440660477\n",
      "epoch:  12   step:  249   train loss:  0.026810402050614357  val loss:  0.029182976111769676\n",
      "epoch:  12   step:  250   train loss:  0.029122838750481606  val loss:  0.03050120361149311\n",
      "epoch:  12   step:  251   train loss:  0.022119976580142975  val loss:  0.03030623123049736\n",
      "epoch:  12   step:  252   train loss:  0.016389207914471626  val loss:  0.02965312823653221\n",
      "epoch:  12   step:  253   train loss:  0.01677938736975193  val loss:  0.029734034091234207\n",
      "epoch:  12   step:  254   train loss:  0.02450554631650448  val loss:  0.02886774204671383\n",
      "epoch:  12   step:  255   train loss:  0.021921973675489426  val loss:  0.028615977615118027\n",
      "epoch:  12   step:  256   train loss:  0.03768158331513405  val loss:  0.029161550104618073\n",
      "epoch:  12   step:  257   train loss:  0.03648678585886955  val loss:  0.02956468053162098\n",
      "epoch:  12   step:  258   train loss:  0.03285117447376251  val loss:  0.02963308058679104\n",
      "epoch:  12   step:  259   train loss:  0.024308446794748306  val loss:  0.029082968831062317\n",
      "epoch:  12   step:  260   train loss:  0.03206740692257881  val loss:  0.02915184013545513\n",
      "epoch:  12   step:  261   train loss:  0.016327522695064545  val loss:  0.028819579631090164\n",
      "epoch:  12   step:  262   train loss:  0.03295603021979332  val loss:  0.02852165326476097\n",
      "epoch:  12   step:  263   train loss:  0.02579616755247116  val loss:  0.029521314427256584\n",
      "epoch:  12   step:  264   train loss:  0.03552383556962013  val loss:  0.029518693685531616\n",
      "epoch:  12   step:  265   train loss:  0.021162675693631172  val loss:  0.029540596529841423\n",
      "epoch:  12   step:  266   train loss:  0.024654919281601906  val loss:  0.029630335047841072\n",
      "epoch:  12   step:  267   train loss:  0.02602728083729744  val loss:  0.029861966148018837\n",
      "epoch:  12   step:  268   train loss:  0.020478932186961174  val loss:  0.030353033915162086\n",
      "epoch:  12   step:  269   train loss:  0.016594991087913513  val loss:  0.03036637417972088\n",
      "epoch:  12   step:  270   train loss:  0.034243009984493256  val loss:  0.03025886043906212\n",
      "epoch:  12   step:  271   train loss:  0.016230639070272446  val loss:  0.029391491785645485\n",
      "epoch:  12   step:  272   train loss:  0.027894528582692146  val loss:  0.029963064938783646\n",
      "epoch:  12   step:  273   train loss:  0.01606120355427265  val loss:  0.02905908226966858\n",
      "epoch:  12   step:  274   train loss:  0.02102701924741268  val loss:  0.02955750934779644\n",
      "epoch:  12   step:  275   train loss:  0.044110871851444244  val loss:  0.02961878664791584\n",
      "epoch:  12   step:  276   train loss:  0.016712944954633713  val loss:  0.030248479917645454\n",
      "epoch:  12   step:  277   train loss:  0.01647956669330597  val loss:  0.02954234927892685\n",
      "epoch:  12   step:  278   train loss:  0.02687167376279831  val loss:  0.029645105823874474\n",
      "epoch:  12   step:  279   train loss:  0.04505559056997299  val loss:  0.030214130878448486\n",
      "epoch:  12   step:  280   train loss:  0.02948145382106304  val loss:  0.02974414825439453\n",
      "epoch:  12   step:  281   train loss:  0.02233867533504963  val loss:  0.029102962464094162\n",
      "epoch:  12   step:  282   train loss:  0.01873025856912136  val loss:  0.02926548197865486\n",
      "epoch:  12   step:  283   train loss:  0.0139153515920043  val loss:  0.030293866991996765\n",
      "epoch:  12   step:  284   train loss:  0.0436064638197422  val loss:  0.03039476089179516\n",
      "epoch:  12   step:  285   train loss:  0.020375119522213936  val loss:  0.030764160677790642\n",
      "epoch:  12   step:  286   train loss:  0.025721464306116104  val loss:  0.03133615106344223\n",
      "epoch:  12   step:  287   train loss:  0.027743373066186905  val loss:  0.03199980407953262\n",
      "epoch:  12   step:  288   train loss:  0.017993072047829628  val loss:  0.03234570100903511\n",
      "epoch:  12   step:  289   train loss:  0.04673109948635101  val loss:  0.03235496208071709\n",
      "epoch:  12   step:  290   train loss:  0.025983495637774467  val loss:  0.03170807659626007\n",
      "epoch:  12   step:  291   train loss:  0.023071635514497757  val loss:  0.03263506293296814\n",
      "epoch:  12   step:  292   train loss:  0.014322048984467983  val loss:  0.03255242109298706\n",
      "epoch:  12   step:  293   train loss:  0.02554178796708584  val loss:  0.03210107237100601\n",
      "epoch:  12   step:  294   train loss:  0.020951295271515846  val loss:  0.03187684342265129\n",
      "epoch:  12   step:  295   train loss:  0.028097061440348625  val loss:  0.031472403556108475\n",
      "epoch:  12   step:  296   train loss:  0.02168135717511177  val loss:  0.030060430988669395\n",
      "epoch:  12   step:  297   train loss:  0.02143208123743534  val loss:  0.03094993159174919\n",
      "epoch:  12   step:  298   train loss:  0.027256175875663757  val loss:  0.03155675157904625\n",
      "epoch:  12   step:  299   train loss:  0.024435987696051598  val loss:  0.031760428100824356\n",
      "epoch:  12   step:  300   train loss:  0.028124922886490822  val loss:  0.031078921630978584\n",
      "epoch:  12   step:  301   train loss:  0.01985247991979122  val loss:  0.031060978770256042\n",
      "epoch:  12   step:  302   train loss:  0.034574590623378754  val loss:  0.030529363080859184\n",
      "epoch:  12   step:  303   train loss:  0.048826005309820175  val loss:  0.030084842815995216\n",
      "epoch:  12   step:  304   train loss:  0.021277569234371185  val loss:  0.029964163899421692\n",
      "epoch:  12   step:  305   train loss:  0.01870313659310341  val loss:  0.02953520603477955\n",
      "epoch:  12   step:  306   train loss:  0.02576649934053421  val loss:  0.029565773904323578\n",
      "epoch:  12   step:  307   train loss:  0.023399807512760162  val loss:  0.02948499657213688\n",
      "epoch:  12   step:  308   train loss:  0.039874911308288574  val loss:  0.02919805608689785\n",
      "epoch:  12   step:  309   train loss:  0.02928580529987812  val loss:  0.029352586716413498\n",
      "epoch:  12   step:  310   train loss:  0.022845111787319183  val loss:  0.029254376888275146\n",
      "epoch:  12   step:  311   train loss:  0.021312475204467773  val loss:  0.02968813106417656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  12   step:  312   train loss:  0.011484595015645027  val loss:  0.029445171356201172\n",
      "epoch:  12   step:  313   train loss:  0.024734610691666603  val loss:  0.03030155785381794\n",
      "epoch:  12   step:  314   train loss:  0.0389336533844471  val loss:  0.030171768739819527\n",
      "epoch:  12   step:  315   train loss:  0.023389142006635666  val loss:  0.03048050031065941\n",
      "epoch:  12   step:  316   train loss:  0.022109324112534523  val loss:  0.030566811561584473\n",
      "epoch:  12   step:  317   train loss:  0.03915831074118614  val loss:  0.03078591823577881\n",
      "epoch:  12   step:  318   train loss:  0.02589023858308792  val loss:  0.030848143622279167\n",
      "epoch:  12   step:  319   train loss:  0.03338796645402908  val loss:  0.030776213854551315\n",
      "epoch:  12   step:  320   train loss:  0.025843048468232155  val loss:  0.0301055908203125\n",
      "epoch:  12   step:  321   train loss:  0.04577164724469185  val loss:  0.03047819808125496\n",
      "epoch:  12   step:  322   train loss:  0.037444744259119034  val loss:  0.030459988862276077\n",
      "epoch:  12   step:  323   train loss:  0.01276615634560585  val loss:  0.029864713549613953\n",
      "epoch:  12   step:  324   train loss:  0.024561528116464615  val loss:  0.029842479154467583\n",
      "epoch:  12   step:  325   train loss:  0.014245245605707169  val loss:  0.029101775959134102\n",
      "epoch:  12   step:  326   train loss:  0.03452795743942261  val loss:  0.02985304594039917\n",
      "epoch:  12   step:  327   train loss:  0.032053470611572266  val loss:  0.030597954988479614\n",
      "epoch:  12   step:  328   train loss:  0.050970062613487244  val loss:  0.030949337407946587\n",
      "epoch:  12   step:  329   train loss:  0.016099682077765465  val loss:  0.031327344477176666\n",
      "epoch:  12   step:  330   train loss:  0.013573898933827877  val loss:  0.03128358721733093\n",
      "epoch:  12   step:  331   train loss:  0.030082199722528458  val loss:  0.03120153583586216\n",
      "epoch:  12   step:  332   train loss:  0.02053222991526127  val loss:  0.031095795333385468\n",
      "epoch:  12   step:  333   train loss:  0.019100792706012726  val loss:  0.031044291332364082\n",
      "epoch:  12   step:  334   train loss:  0.0287049300968647  val loss:  0.0298155564814806\n",
      "epoch:  12   step:  335   train loss:  0.032851818948984146  val loss:  0.02979598380625248\n",
      "epoch:  12   step:  336   train loss:  0.021838761866092682  val loss:  0.030035734176635742\n",
      "epoch:  12   step:  337   train loss:  0.02404758334159851  val loss:  0.03078235685825348\n",
      "epoch:  12   step:  338   train loss:  0.03138415142893791  val loss:  0.03139680624008179\n",
      "epoch:  12   step:  339   train loss:  0.021114308387041092  val loss:  0.0316227562725544\n",
      "epoch:  12   step:  340   train loss:  0.03020324371755123  val loss:  0.03190179541707039\n",
      "epoch:  12   step:  341   train loss:  0.02950630523264408  val loss:  0.03264694660902023\n",
      "epoch:  12   step:  342   train loss:  0.037404440343379974  val loss:  0.032923948019742966\n",
      "epoch:  12   step:  343   train loss:  0.06716621667146683  val loss:  0.032410215586423874\n",
      "epoch:  13   step:  0   train loss:  0.027356890961527824  val loss:  0.03267604112625122\n",
      "epoch:  13   step:  1   train loss:  0.023688560351729393  val loss:  0.03212333098053932\n",
      "epoch:  13   step:  2   train loss:  0.032728683203458786  val loss:  0.03216545656323433\n",
      "epoch:  13   step:  3   train loss:  0.022688128054142  val loss:  0.03145503252744675\n",
      "epoch:  13   step:  4   train loss:  0.02450370602309704  val loss:  0.031423263251781464\n",
      "epoch:  13   step:  5   train loss:  0.01983226276934147  val loss:  0.030915625393390656\n",
      "epoch:  13   step:  6   train loss:  0.018126316368579865  val loss:  0.030665116384625435\n",
      "epoch:  13   step:  7   train loss:  0.035731710493564606  val loss:  0.031214188784360886\n",
      "epoch:  13   step:  8   train loss:  0.03268354758620262  val loss:  0.030453471466898918\n",
      "epoch:  13   step:  9   train loss:  0.037270691245794296  val loss:  0.03134777396917343\n",
      "epoch:  13   step:  10   train loss:  0.027957387268543243  val loss:  0.03071037493646145\n",
      "epoch:  13   step:  11   train loss:  0.019548913463950157  val loss:  0.03063027746975422\n",
      "epoch:  13   step:  12   train loss:  0.03159554675221443  val loss:  0.03005964308977127\n",
      "epoch:  13   step:  13   train loss:  0.03706439957022667  val loss:  0.029811324551701546\n",
      "epoch:  13   step:  14   train loss:  0.025789774954319  val loss:  0.0294575784355402\n",
      "epoch:  13   step:  15   train loss:  0.01393794734030962  val loss:  0.029134303331375122\n",
      "epoch:  13   step:  16   train loss:  0.02510579116642475  val loss:  0.0291343592107296\n",
      "epoch:  13   step:  17   train loss:  0.019867341965436935  val loss:  0.0290127731859684\n",
      "epoch:  13   step:  18   train loss:  0.01670856587588787  val loss:  0.028858531266450882\n",
      "epoch:  13   step:  19   train loss:  0.024101734161376953  val loss:  0.028883378952741623\n",
      "epoch:  13   step:  20   train loss:  0.01388776209205389  val loss:  0.029331546276807785\n",
      "epoch:  13   step:  21   train loss:  0.03117653913795948  val loss:  0.029641883447766304\n",
      "epoch:  13   step:  22   train loss:  0.014496545307338238  val loss:  0.029871487990021706\n",
      "epoch:  13   step:  23   train loss:  0.030193209648132324  val loss:  0.02924811653792858\n",
      "epoch:  13   step:  24   train loss:  0.017983270809054375  val loss:  0.028570428490638733\n",
      "epoch:  13   step:  25   train loss:  0.01436655130237341  val loss:  0.02848733402788639\n",
      "epoch:  13   step:  26   train loss:  0.024000609293580055  val loss:  0.028738703578710556\n",
      "epoch:  13   step:  27   train loss:  0.01659892499446869  val loss:  0.028782201930880547\n",
      "epoch:  13   step:  28   train loss:  0.031113626435399055  val loss:  0.028761764988303185\n",
      "epoch:  13   step:  29   train loss:  0.025860842317342758  val loss:  0.029359353706240654\n",
      "epoch:  13   step:  30   train loss:  0.021078549325466156  val loss:  0.029511701315641403\n",
      "epoch:  13   step:  31   train loss:  0.04021811857819557  val loss:  0.029900524765253067\n",
      "epoch:  13   step:  32   train loss:  0.022987334057688713  val loss:  0.029898550361394882\n",
      "epoch:  13   step:  33   train loss:  0.021547356620430946  val loss:  0.029341721907258034\n",
      "epoch:  13   step:  34   train loss:  0.017971212044358253  val loss:  0.02956170216202736\n",
      "epoch:  13   step:  35   train loss:  0.030361400917172432  val loss:  0.030176006257534027\n",
      "epoch:  13   step:  36   train loss:  0.02268499694764614  val loss:  0.030217831954360008\n",
      "epoch:  13   step:  37   train loss:  0.016788505017757416  val loss:  0.03011324256658554\n",
      "epoch:  13   step:  38   train loss:  0.029484398663043976  val loss:  0.030320750549435616\n",
      "epoch:  13   step:  39   train loss:  0.01898348517715931  val loss:  0.02973192371428013\n",
      "epoch:  13   step:  40   train loss:  0.026334725320339203  val loss:  0.029496004804968834\n",
      "epoch:  13   step:  41   train loss:  0.06008381396532059  val loss:  0.029277734458446503\n",
      "epoch:  13   step:  42   train loss:  0.026009665802121162  val loss:  0.029986344277858734\n",
      "epoch:  13   step:  43   train loss:  0.024723799899220467  val loss:  0.030177179723978043\n",
      "epoch:  13   step:  44   train loss:  0.018676282837986946  val loss:  0.030044790357351303\n",
      "epoch:  13   step:  45   train loss:  0.02143496833741665  val loss:  0.03055754117667675\n",
      "epoch:  13   step:  46   train loss:  0.01945354789495468  val loss:  0.03070991113781929\n",
      "epoch:  13   step:  47   train loss:  0.02732657454907894  val loss:  0.031175369396805763\n",
      "epoch:  13   step:  48   train loss:  0.02815922163426876  val loss:  0.031064746901392937\n",
      "epoch:  13   step:  49   train loss:  0.024486787617206573  val loss:  0.030704936012625694\n",
      "epoch:  13   step:  50   train loss:  0.08131632953882217  val loss:  0.02910606376826763\n",
      "epoch:  13   step:  51   train loss:  0.027569521218538284  val loss:  0.02965410426259041\n",
      "epoch:  13   step:  52   train loss:  0.031847432255744934  val loss:  0.030136311426758766\n",
      "epoch:  13   step:  53   train loss:  0.030366957187652588  val loss:  0.03017544187605381\n",
      "epoch:  13   step:  54   train loss:  0.022650115191936493  val loss:  0.028661062940955162\n",
      "epoch:  13   step:  55   train loss:  0.027698013931512833  val loss:  0.028383243829011917\n",
      "epoch:  13   step:  56   train loss:  0.015253092162311077  val loss:  0.02728099189698696\n",
      "epoch:  13   step:  57   train loss:  0.023484457284212112  val loss:  0.027554651722311974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  13   step:  58   train loss:  0.03277406841516495  val loss:  0.02718736045062542\n",
      "epoch:  13   step:  59   train loss:  0.015542629174888134  val loss:  0.027188483625650406\n",
      "epoch:  13   step:  60   train loss:  0.03346072509884834  val loss:  0.027569010853767395\n",
      "epoch:  13   step:  61   train loss:  0.018128909170627594  val loss:  0.027227619662880898\n",
      "epoch:  13   step:  62   train loss:  0.0346185527741909  val loss:  0.026693159714341164\n",
      "epoch:  13   step:  63   train loss:  0.020919103175401688  val loss:  0.025981634855270386\n",
      "epoch:  13   step:  64   train loss:  0.06507854163646698  val loss:  0.025592556223273277\n",
      "epoch:  13   step:  65   train loss:  0.026443084701895714  val loss:  0.025912700220942497\n",
      "epoch:  13   step:  66   train loss:  0.02170599438250065  val loss:  0.02680838294327259\n",
      "epoch:  13   step:  67   train loss:  0.028792599216103554  val loss:  0.027594532817602158\n",
      "epoch:  13   step:  68   train loss:  0.035197436809539795  val loss:  0.027276666834950447\n",
      "epoch:  13   step:  69   train loss:  0.028669768944382668  val loss:  0.027568235993385315\n",
      "epoch:  13   step:  70   train loss:  0.03327147290110588  val loss:  0.028092479333281517\n",
      "epoch:  13   step:  71   train loss:  0.04538671299815178  val loss:  0.02834310010075569\n",
      "epoch:  13   step:  72   train loss:  0.017823800444602966  val loss:  0.027660418301820755\n",
      "epoch:  13   step:  73   train loss:  0.017993854358792305  val loss:  0.027378132566809654\n",
      "epoch:  13   step:  74   train loss:  0.023912161588668823  val loss:  0.027291176840662956\n",
      "epoch:  13   step:  75   train loss:  0.031691644340753555  val loss:  0.026564357802271843\n",
      "epoch:  13   step:  76   train loss:  0.03369362652301788  val loss:  0.026159945875406265\n",
      "epoch:  13   step:  77   train loss:  0.02707061357796192  val loss:  0.025531122460961342\n",
      "epoch:  13   step:  78   train loss:  0.02373531088232994  val loss:  0.025124995037913322\n",
      "epoch:  13   step:  79   train loss:  0.029083020985126495  val loss:  0.024600479751825333\n",
      "epoch:  13   step:  80   train loss:  0.048276808112859726  val loss:  0.024800240993499756\n",
      "epoch:  13   step:  81   train loss:  0.0200054794549942  val loss:  0.025830423459410667\n",
      "epoch:  13   step:  82   train loss:  0.035687148571014404  val loss:  0.025362636893987656\n",
      "epoch:  13   step:  83   train loss:  0.022524230182170868  val loss:  0.0261257141828537\n",
      "epoch:  13   step:  84   train loss:  0.030675971880555153  val loss:  0.0265260711312294\n",
      "epoch:  13   step:  85   train loss:  0.02934899926185608  val loss:  0.027577286586165428\n",
      "epoch:  13   step:  86   train loss:  0.01701469160616398  val loss:  0.028065698221325874\n",
      "epoch:  13   step:  87   train loss:  0.0264787245541811  val loss:  0.027224775403738022\n",
      "epoch:  13   step:  88   train loss:  0.01604103483259678  val loss:  0.027080880478024483\n",
      "epoch:  13   step:  89   train loss:  0.03488530218601227  val loss:  0.02774466946721077\n",
      "epoch:  13   step:  90   train loss:  0.03160851076245308  val loss:  0.028453964740037918\n",
      "epoch:  13   step:  91   train loss:  0.03217627480626106  val loss:  0.028510814532637596\n",
      "epoch:  13   step:  92   train loss:  0.018958814442157745  val loss:  0.028166769072413445\n",
      "epoch:  13   step:  93   train loss:  0.023107752203941345  val loss:  0.028815066441893578\n",
      "epoch:  13   step:  94   train loss:  0.042162224650382996  val loss:  0.028826892375946045\n",
      "epoch:  13   step:  95   train loss:  0.02390037477016449  val loss:  0.02847939357161522\n",
      "epoch:  13   step:  96   train loss:  0.033640410751104355  val loss:  0.028510546311736107\n",
      "epoch:  13   step:  97   train loss:  0.017812343314290047  val loss:  0.028332140296697617\n",
      "epoch:  13   step:  98   train loss:  0.03561476618051529  val loss:  0.028206655755639076\n",
      "epoch:  13   step:  99   train loss:  0.03787470608949661  val loss:  0.02809000015258789\n",
      "epoch:  13   step:  100   train loss:  0.015132532455027103  val loss:  0.027694858610630035\n",
      "epoch:  13   step:  101   train loss:  0.014334115199744701  val loss:  0.02844720147550106\n",
      "epoch:  13   step:  102   train loss:  0.025160135701298714  val loss:  0.028734244406223297\n",
      "epoch:  13   step:  103   train loss:  0.027577504515647888  val loss:  0.029855947941541672\n",
      "epoch:  13   step:  104   train loss:  0.02021671086549759  val loss:  0.030061116442084312\n",
      "epoch:  13   step:  105   train loss:  0.02378280833363533  val loss:  0.0310283862054348\n",
      "epoch:  13   step:  106   train loss:  0.0178680382668972  val loss:  0.03137415647506714\n",
      "epoch:  13   step:  107   train loss:  0.025729108601808548  val loss:  0.03057905100286007\n",
      "epoch:  13   step:  108   train loss:  0.02260562963783741  val loss:  0.03079409711062908\n",
      "epoch:  13   step:  109   train loss:  0.02062809467315674  val loss:  0.0313325859606266\n",
      "epoch:  13   step:  110   train loss:  0.03214295580983162  val loss:  0.03136209025979042\n",
      "epoch:  13   step:  111   train loss:  0.026466643437743187  val loss:  0.031171314418315887\n",
      "epoch:  13   step:  112   train loss:  0.02407175488770008  val loss:  0.031086595728993416\n",
      "epoch:  13   step:  113   train loss:  0.03393128514289856  val loss:  0.030741970986127853\n",
      "epoch:  13   step:  114   train loss:  0.022266900166869164  val loss:  0.0306612066924572\n",
      "epoch:  13   step:  115   train loss:  0.02881757728755474  val loss:  0.03065154142677784\n",
      "epoch:  13   step:  116   train loss:  0.021441133692860603  val loss:  0.03048846870660782\n",
      "epoch:  13   step:  117   train loss:  0.018556823953986168  val loss:  0.029766390100121498\n",
      "epoch:  13   step:  118   train loss:  0.017011115327477455  val loss:  0.028868919238448143\n",
      "epoch:  13   step:  119   train loss:  0.015474631451070309  val loss:  0.02816750667989254\n",
      "epoch:  13   step:  120   train loss:  0.02511453442275524  val loss:  0.02871161699295044\n",
      "epoch:  13   step:  121   train loss:  0.018327102065086365  val loss:  0.02895650453865528\n",
      "epoch:  13   step:  122   train loss:  0.027026519179344177  val loss:  0.030048351734876633\n",
      "epoch:  13   step:  123   train loss:  0.023527346551418304  val loss:  0.029103901237249374\n",
      "epoch:  13   step:  124   train loss:  0.022954817861318588  val loss:  0.02937372215092182\n",
      "epoch:  13   step:  125   train loss:  0.02416086383163929  val loss:  0.02944560907781124\n",
      "epoch:  13   step:  126   train loss:  0.02926981821656227  val loss:  0.029623698443174362\n",
      "epoch:  13   step:  127   train loss:  0.036268092691898346  val loss:  0.030317896977066994\n",
      "epoch:  13   step:  128   train loss:  0.025894969701766968  val loss:  0.029846716672182083\n",
      "epoch:  13   step:  129   train loss:  0.01661979965865612  val loss:  0.029868587851524353\n",
      "epoch:  13   step:  130   train loss:  0.015039930120110512  val loss:  0.029645415022969246\n",
      "epoch:  13   step:  131   train loss:  0.029421936720609665  val loss:  0.028721585869789124\n",
      "epoch:  13   step:  132   train loss:  0.03645789623260498  val loss:  0.029168151319026947\n",
      "epoch:  13   step:  133   train loss:  0.014063075184822083  val loss:  0.027704467996954918\n",
      "epoch:  13   step:  134   train loss:  0.022505639120936394  val loss:  0.028701329603791237\n",
      "epoch:  13   step:  135   train loss:  0.028917428106069565  val loss:  0.027781540527939796\n",
      "epoch:  13   step:  136   train loss:  0.021865878254175186  val loss:  0.027524763718247414\n",
      "epoch:  13   step:  137   train loss:  0.025089703500270844  val loss:  0.027650883421301842\n",
      "epoch:  13   step:  138   train loss:  0.03638635203242302  val loss:  0.027220385149121284\n",
      "epoch:  13   step:  139   train loss:  0.02040960267186165  val loss:  0.027622418478131294\n",
      "epoch:  13   step:  140   train loss:  0.03246985375881195  val loss:  0.028291910886764526\n",
      "epoch:  13   step:  141   train loss:  0.01887984201312065  val loss:  0.027698392048478127\n",
      "epoch:  13   step:  142   train loss:  0.02633635513484478  val loss:  0.028191404417157173\n",
      "epoch:  13   step:  143   train loss:  0.023178867995738983  val loss:  0.02910390868782997\n",
      "epoch:  13   step:  144   train loss:  0.032004911452531815  val loss:  0.028978068381547928\n",
      "epoch:  13   step:  145   train loss:  0.017692023888230324  val loss:  0.029008440673351288\n",
      "epoch:  13   step:  146   train loss:  0.019818516448140144  val loss:  0.029023120179772377\n",
      "epoch:  13   step:  147   train loss:  0.019608814269304276  val loss:  0.029266178607940674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  13   step:  148   train loss:  0.02719808556139469  val loss:  0.029469875618815422\n",
      "epoch:  13   step:  149   train loss:  0.0258046742528677  val loss:  0.030056729912757874\n",
      "epoch:  13   step:  150   train loss:  0.03871145844459534  val loss:  0.029495395720005035\n",
      "epoch:  13   step:  151   train loss:  0.029254725202918053  val loss:  0.02862645499408245\n",
      "epoch:  13   step:  152   train loss:  0.03233083710074425  val loss:  0.02884840779006481\n",
      "epoch:  13   step:  153   train loss:  0.016418373212218285  val loss:  0.02867114543914795\n",
      "epoch:  13   step:  154   train loss:  0.018076850101351738  val loss:  0.028148161247372627\n",
      "epoch:  13   step:  155   train loss:  0.011179724708199501  val loss:  0.028570465743541718\n",
      "epoch:  13   step:  156   train loss:  0.021082734689116478  val loss:  0.02971516363322735\n",
      "epoch:  13   step:  157   train loss:  0.021824944764375687  val loss:  0.030253034085035324\n",
      "epoch:  13   step:  158   train loss:  0.026822196319699287  val loss:  0.030661387369036674\n",
      "epoch:  13   step:  159   train loss:  0.026894941926002502  val loss:  0.031359702348709106\n",
      "epoch:  13   step:  160   train loss:  0.03710413724184036  val loss:  0.030808134004473686\n",
      "epoch:  13   step:  161   train loss:  0.028893768787384033  val loss:  0.03061196580529213\n",
      "epoch:  13   step:  162   train loss:  0.021999316290020943  val loss:  0.03033791482448578\n",
      "epoch:  13   step:  163   train loss:  0.036605313420295715  val loss:  0.030098332092165947\n",
      "epoch:  13   step:  164   train loss:  0.033858951181173325  val loss:  0.03006082773208618\n",
      "epoch:  13   step:  165   train loss:  0.02487829513847828  val loss:  0.02987065725028515\n",
      "epoch:  13   step:  166   train loss:  0.034396857023239136  val loss:  0.02877298928797245\n",
      "epoch:  13   step:  167   train loss:  0.02255534753203392  val loss:  0.02799067087471485\n",
      "epoch:  13   step:  168   train loss:  0.03046264871954918  val loss:  0.02843867614865303\n",
      "epoch:  13   step:  169   train loss:  0.027267633005976677  val loss:  0.027690235525369644\n",
      "epoch:  13   step:  170   train loss:  0.015890853479504585  val loss:  0.027575168758630753\n",
      "epoch:  13   step:  171   train loss:  0.030435042455792427  val loss:  0.02749357931315899\n",
      "epoch:  13   step:  172   train loss:  0.013466023840010166  val loss:  0.02746742218732834\n",
      "epoch:  13   step:  173   train loss:  0.018969139084219933  val loss:  0.027610164135694504\n",
      "epoch:  13   step:  174   train loss:  0.029726997017860413  val loss:  0.027349798008799553\n",
      "epoch:  13   step:  175   train loss:  0.020921090617775917  val loss:  0.027124712243676186\n",
      "epoch:  13   step:  176   train loss:  0.021889183670282364  val loss:  0.027079297229647636\n",
      "epoch:  13   step:  177   train loss:  0.01732887700200081  val loss:  0.027093829587101936\n",
      "epoch:  13   step:  178   train loss:  0.011101977899670601  val loss:  0.027393441647291183\n",
      "epoch:  13   step:  179   train loss:  0.023088863119482994  val loss:  0.027530234307050705\n",
      "epoch:  13   step:  180   train loss:  0.018611496314406395  val loss:  0.027462201192975044\n",
      "epoch:  13   step:  181   train loss:  0.017347630113363266  val loss:  0.026878798380494118\n",
      "epoch:  13   step:  182   train loss:  0.026398666203022003  val loss:  0.02739689312875271\n",
      "epoch:  13   step:  183   train loss:  0.028666142374277115  val loss:  0.026928972452878952\n",
      "epoch:  13   step:  184   train loss:  0.053490471094846725  val loss:  0.026832548901438713\n",
      "epoch:  13   step:  185   train loss:  0.025400415062904358  val loss:  0.026876965537667274\n",
      "epoch:  13   step:  186   train loss:  0.021028662100434303  val loss:  0.02680213935673237\n",
      "epoch:  13   step:  187   train loss:  0.015927625820040703  val loss:  0.02694711834192276\n",
      "epoch:  13   step:  188   train loss:  0.021915283054113388  val loss:  0.027287878096103668\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5c6fc606598b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_data_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mval_loss_print\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;31m#print('epoch: ', epoch, '  step: ', step, '  train loss: ', train_loss_print, ' val loss: ', val_loss_print)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'  step: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'  train loss: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_print\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' val loss: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_print\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py\u001b[0m in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \"\"\"\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_emptyCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(params = net.parameters(), lr=1e-4, momentum=0.9, weight_decay=2e-5)\n",
    "#optimizer = torch.optim.Adam(params = net.parameters(), lr=1e-4)\n",
    "#optimizer = torch.optim.SGD(params = net.parameters(), lr=1e-4)\n",
    "loss_func = torch.nn.CrossEntropyLoss(ignore_index = 0, weight=class_weights).cuda()  # the target label is NOT an one-hotted\n",
    "\n",
    "val_data_input = val_data_input.cuda().float()\n",
    "val_data_label = val_data_label.cuda()\n",
    "\n",
    "min_val_loss_print=float('inf')\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    for step, (batch_data, batch_label) in enumerate(loader):\n",
    "        batch_data = batch_data.cuda().float()\n",
    "        batch_label = batch_label.cuda()\n",
    "        net.train()\n",
    "        output  = net(batch_data)\n",
    "        train_loss = loss_func(output,batch_label)\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        #nn.utils.clip_grad_norm_(net.parameters(), max_norm=20, norm_type=2)\n",
    "        optimizer.step()\n",
    "        train_loss_print = train_loss.data.item()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            val_output  = net(val_data_input)\n",
    "            val_loss = loss_func(val_output,val_data_label)\n",
    "            val_loss_print = val_loss.data.item()\n",
    "            torch.cuda.empty_cache()\n",
    "        #print('epoch: ', epoch, '  step: ', step, '  train loss: ', train_loss_print, ' val loss: ', val_loss_print)\n",
    "        print('epoch: ', epoch, '  step: ', step, '  train loss: ', train_loss_print, ' val loss: ', val_loss_print)\n",
    "        if val_loss_print < min_val_loss_print:\n",
    "            torch.save(net.state_dict(), 'net_params_20210503/net_params_20210503_01/epoch_'+str(epoch)+'.pkl') \n",
    "            min_val_loss_print = val_loss_print\n",
    "            print('min_val_loss_print', min_val_loss_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "0.33143209636728294\n",
      "0.4555787749628391\n",
      "0.7182799161622191\n",
      "0.8427627835338143\n",
      "0.8974464518032462\n",
      "0.9333851373805397\n",
      "0.9495371299032743\n",
      "0.9607305421792682\n",
      "0.9692911568526904\n",
      "0.9729224114938483\n",
      "0.977344438587109\n",
      "0.9781540547738128\n",
      "0.9798619332282431\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter   \n",
    "import math\n",
    "\n",
    "writer = SummaryWriter('./net_params_20210503/net_params_20210503_1')\n",
    "\n",
    "acc = [1.1043323278427124, 0.7861866354942322, 0.33089593052864075, 0.17106975615024567, 0.10820182412862778, 0.06893736869096756, 0.05178064480423927, 0.04006130248308182, 0.031190240755677223, 0.027450941503047943, 0.022916141897439957, 0.022088101133704185, 0.0203436017036438]\n",
    "\n",
    "print(len(acc))\n",
    "\n",
    "for epoch in range(len(acc)):\n",
    "    acc_e = math.exp(-acc[epoch])\n",
    "    print(acc_e)\n",
    "    writer.add_scalar('accuracy', acc_e, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
